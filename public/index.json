[
{
    
    "uri": "/development/dotnet.html",
    "title": ".Net",
    "tags": [],
    "description": "Dévelopement .Net",
    "content": "\n (505) HTTP version not supported en .Net     Accès au WS Sécurisé en .Net     Web service erreur CS0029, CS0030    ", 
    "breadcrumb": " > development > dotnet.html"
},
{
    
    "uri": "/linux/administration.html",
    "title": "Administration",
    "tags": [],
    "description": "",
    "content": "\n Activer SUDO sur Debian     Ajouter de la swap sans toucher au partition     Analyze systemd boot speed     Change Active Directory password on Linux     Changer le nom d’une machine     Comment installer une alternative     Configuration Swappiness     Demarrage de JBoss en service     Desactiver l’interface graphique     Installer backup sur jail BSD     Installer une imprimante IPP     Last Firefox Version in testing     Lister le materiel présent sur la machine     MTA est long à démarrer     Missing LSB tags and overrides     Monter une image ISO     Nettoyer les fichiers de configuration     Nettoyer sa Debian     Probleme de politique de securite X11     apt-get Encountered a section with no Package header    ", 
    "breadcrumb": " > linux > administration.html"
},
{
    
    "uri": "/outils/ansible.html",
    "title": "Ansible",
    "tags": [],
    "description": "",
    "content": "\n Template error while templating string: Missing end of comment tag    ", 
    "breadcrumb": " > outils > ansible.html"
},
{
    
    "uri": "/cuisine.html",
    "title": "Cuisine",
    "tags": [],
    "description": "Quelques recettes de cuisine",
    "content": "\n Tataki de thon    ", 
    "breadcrumb": " > cuisine.html"
},
{
    
    "uri": "/linux/divers.html",
    "title": "Divers",
    "tags": [],
    "description": "",
    "content": "\n Autoriser XHost à Docker     Clé USB en lecture seule     Comparaisons avec diff     Connexion VPN Rackspace depuis Linux     Creer une clé USB d’install Debian     Créer un conteneur chiffré     Créer une partition chiffré sur une clé USB     Retrouver la cle OEM Windows     Squid StoreId rewrite     Squid refresh pattern    ", 
    "breadcrumb": " > linux > divers.html"
},
{
    
    "uri": "/misc.html",
    "title": "Divers",
    "tags": [],
    "description": "Un peu tout et n’importe quoi",
    "content": "\n Désactiver le fixup Mozilla     Mute et CEC sur OpenELEC     Resynchroniser des CPL Devolo     Upload file to Nexus Repository    ", 
    "breadcrumb": " > misc.html"
},
{
    
    "uri": "/outils/docker.html",
    "title": "Docker",
    "tags": [],
    "description": "",
    "content": "\n Accès Docker non root     Autoriser l’acces au DISPLAY pour un container specifique     Changer le répertoire des images docker     Docker failed to create image rootfs     Export Importer une image Docker     InetAddress does’nt resolve ip on alpine docker container     Registry Docker derrière un Nginx     Remove ophans from docker registry     Supprimer les images non taggé     Supprimer les vieux conteneurs     System error on docker run     Update toutes les images     Weekly cleaner    ", 
    "breadcrumb": " > outils > docker.html"
},
{
    
    "uri": "/development.html",
    "title": "Développement",
    "tags": [],
    "description": "Tout pour le dev, Java, javascript, ...",
    "content": "\n .Net    (505) HTTP version not supported en .Net   Accès au WS Sécurisé en .Net   Web service erreur CS0029, CS0030    GWT    IllegalStateException: SimplePanel   Module may need to be (re)compiled   Problème de sérialisation    Java    Accès JNDI de JBoss   Accéder un membre classe private   Ajouter un certificat pour les connexions SSL Java   Ajouter une Police de caractère à Java   Appeler un Web-Service au travers d’un proxy   Concepts intéressants en Java   Configuration Log4j   Connexion LDAP Exchange   Convertir un date entre Timezone avec Jodatime   Installer Java Sun JDK sous Debian   Lancer Neo4j Impermanent Database \u0026#43; REST Server sur un port aléatoire   Maven Release Plugin   Maven: Exemple de settings.xml   Mode debug sur Tomcat   Optimisation des statements Batch MySQL   Où télécharger les JDK ?   Remote JMX Console   Requête Oracle avec Variables Liées   Répéter un test x fois avec jUnit   StringIndexOutOfBoundsException dans Ivy   Tester des membres private   Transformer une URL java en chemin complet   Télécharger Java en une ligne   Visitor vs. InstanceOf   Xmx \u0026amp; Xms par defaut    Javascript    Classes Javascript propres   Tester un numérique en JavaScript   Utiliser Firebug (Lite) avec IE   Utiliser Firebug sans planter IE7\u0026amp;8    Spring    Connection securisé et reverse proxy    Hack CSS    ", 
    "breadcrumb": " > development.html"
},
{
    
    "uri": "/outils/eclipse.html",
    "title": "Eclipse",
    "tags": [],
    "description": "L’EDI Java, rien à voir avec la lune",
    "content": "\n Ajouter la tache SCP a Ant sous Eclipse     Crash d’Eclipse     Erreur Cannot connect to VM lors dun debug     Locking is not possible in the directory ...    ", 
    "breadcrumb": " > outils > eclipse.html"
},
{
    
    "uri": "/development/gwt.html",
    "title": "GWT",
    "tags": [],
    "description": "Google Web Toolkit",
    "content": "\n IllegalStateException: SimplePanel     Module may need to be (re)compiled     Problème de sérialisation    ", 
    "breadcrumb": " > development > gwt.html"
},
{
    
    "uri": "/outils/git.html",
    "title": "Git",
    "tags": [],
    "description": "Si vous ne devez maitriser qu’un seul des outils c’est celui là",
    "content": "\n Accélérer le clonage d’un repo Git     Changelog depuis milestone gitlab     Convention de message de commit     Git en ligne de commande     Git rerere     Git un modèle de branches efficace     Introduction à GIT     Push sur plusieurs repo git     Regrouper des repos git sans perdre l’historique     gitflow-breakdown     non fast forward updates were rejected    ", 
    "breadcrumb": " > outils > git.html"
},
{
    
    "uri": "/linux/gnomeshell.html",
    "title": "GnomeShell",
    "tags": [],
    "description": "",
    "content": "\n Afficher la date du jour dans le bandeau     Documents recents dans gnome-shell     Fusionner les icones d’application dans le dock     Screencast avec gnome-shell     gnome-shell segfault at 84 ip libcogl.so.12.1.1    ", 
    "breadcrumb": " > linux > gnomeshell.html"
},
{
    
    "uri": "/serveurs/http.html",
    "title": "HTTP",
    "tags": [],
    "description": "",
    "content": "\n 1s de cache contre le downtime     HTTPS Securisé avec Nginx     Hôtes virtuels SSL multiples     Nginx WebDAV et problème de crochets     Optimiser les performances    ", 
    "breadcrumb": " > serveurs > http.html"
},
{
    
    "uri": "/outils/intellij.html",
    "title": "IntelliJ",
    "tags": [],
    "description": "Marre d’Eclipde ?",
    "content": "\n Ajouter le dictionnaire français     Meilleures options de JVM pour IntelliJ    ", 
    "breadcrumb": " > outils > intellij.html"
},
{
    
    "uri": "/serveurs/jboss.html",
    "title": "JBoss",
    "tags": [],
    "description": "",
    "content": "\n Connexion JBoss SQL-Server     Gérer l’ordre de déploiement dans JBoss    ", 
    "breadcrumb": " > serveurs > jboss.html"
},
{
    
    "uri": "/development/java.html",
    "title": "Java",
    "tags": [],
    "description": "",
    "content": "\n Accès JNDI de JBoss     Accéder un membre classe private     Ajouter un certificat pour les connexions SSL Java     Ajouter une Police de caractère à Java     Appeler un Web-Service au travers d’un proxy     Concepts intéressants en Java     Configuration Log4j     Connexion LDAP Exchange     Convertir un date entre Timezone avec Jodatime     Installer Java Sun JDK sous Debian     Lancer Neo4j Impermanent Database \u0026#43; REST Server sur un port aléatoire     Maven Release Plugin     Maven: Exemple de settings.xml     Mode debug sur Tomcat     Optimisation des statements Batch MySQL     Où télécharger les JDK ?     Remote JMX Console     Requête Oracle avec Variables Liées     Répéter un test x fois avec jUnit     StringIndexOutOfBoundsException dans Ivy     Tester des membres private     Transformer une URL java en chemin complet     Télécharger Java en une ligne     Visitor vs. InstanceOf     Xmx \u0026amp; Xms par defaut    ", 
    "breadcrumb": " > development > java.html"
},
{
    
    "uri": "/development/javascript.html",
    "title": "Javascript",
    "tags": [],
    "description": "",
    "content": "\n Classes Javascript propres     Tester un numérique en JavaScript     Utiliser Firebug (Lite) avec IE     Utiliser Firebug sans planter IE7\u0026amp;8    ", 
    "breadcrumb": " > development > javascript.html"
},
{
    
    "uri": "/outils/latex.html",
    "title": "LaTeX",
    "tags": [],
    "description": "C’est _on mais _’est _haut",
    "content": "\n Dollar en latex     Mettre à jour un package LaTeX     Optimizer la taille d’un PDF     Visualisation instantané de modifications Latex    ", 
    "breadcrumb": " > outils > latex.html"
},
{
    
    "uri": "/linux.html",
    "title": "Linux",
    "tags": [],
    "description": "Le seul le vrai l’unique système",
    "content": "\n Administration    Activer SUDO sur Debian   Ajouter de la swap sans toucher au partition   Analyze systemd boot speed   Change Active Directory password on Linux   Changer le nom d’une machine   Comment installer une alternative   Configuration Swappiness   Demarrage de JBoss en service   Desactiver l’interface graphique   Installer backup sur jail BSD   Installer une imprimante IPP   Last Firefox Version in testing   Lister le materiel présent sur la machine   MTA est long à démarrer   Missing LSB tags and overrides   Monter une image ISO   Nettoyer les fichiers de configuration   Nettoyer sa Debian   Probleme de politique de securite X11   apt-get Encountered a section with no Package header    Divers    Autoriser XHost à Docker   Clé USB en lecture seule   Comparaisons avec diff   Connexion VPN Rackspace depuis Linux   Creer une clé USB d’install Debian   Créer un conteneur chiffré   Créer une partition chiffré sur une clé USB   Retrouver la cle OEM Windows   Squid StoreId rewrite   Squid refresh pattern    GnomeShell    Afficher la date du jour dans le bandeau   Documents recents dans gnome-shell   Fusionner les icones d’application dans le dock   Screencast avec gnome-shell   gnome-shell segfault at 84 ip libcogl.so.12.1.1    Réseau    Authentification SSH par certificat   Avoir plusieurs adresses IP sur la meme interface   Configuration firewall IPTables   Configuration réseau sur VM Linux   DNS Securise   Domaines de recherche pour NetworkManager   Download fichier via SSH avec reprise   Débloquer le wifi   Firefox CommandLine Options   Lancer TCPDump en non root   NFS au travers de SSH   SSH Vérification de la clé hôte échoué   SSH: Reprendre la main sur un session perdue   Scanner les ports sur une IP   WGET avec un proxy   ssh, control de tunnel via socket    Shell    Crypter tar.gz avec mot de passe   Découper et rattacher un gros fichier   Effacer définitivement un disque dur   Envoyer POST avec cURL   Formater simplement un XML   Lecture d’arguments en bash   Lister les fichiers d’une arborescence   Mettre un processus en pause   Modifier une fonction shell   Multi-thread avec xargs   Raccourcis Terminal   Rechercher dans les fichiers   Recuperer le chemin d’un script bash   Rendre exécutable ce qui peut l’être dans une arborescence   Renommer des fichiers sous Linux   Suppression de fichiers en masse   Suppression de fichiers par Inode   Supprimer un type de fichier dans une arborescence   Synchronisation Rsync   Tmux cheatcheet   Trouver les UUID de mes partitions   Trouver les gros fichiers   Télécharger un répertoire entier via FTP   Unifier des PDFs   Using MPC   Web-server en une ligne de commande   ", 
    "breadcrumb": " > linux.html"
},
{
    
    "uri": "/serveurs/mysql.html",
    "title": "MySQL",
    "tags": [],
    "description": "",
    "content": "\n Erreur de read-only status avec JDBC     Exporter/Importer une base d’un dump     Timezoner MySQL     UUID Most Significant Bits    ", 
    "breadcrumb": " > serveurs > mysql.html"
},
{
    
    "uri": "/serveurs/neo4j.html",
    "title": "Neo4J",
    "tags": [],
    "description": "",
    "content": "\n Cypher: Nodes sans Label     Cypher: Paths sans path intermédiaire     Recovery required from position LogPosition    ", 
    "breadcrumb": " > serveurs > neo4j.html"
},
{
    
    "uri": "/serveurs/oracle.html",
    "title": "Oracle",
    "tags": [],
    "description": "",
    "content": "\n Changer le nom d’une machine sans péter XE     Changer le port de l’interface d’admin de XE     Connaître l’encodage de sa base     Démarrage/Arrêt automatique d’Oracle sous Linux     Démarrer/Arrêter une base en ligne de commande Windows     Désinstaller Oracle XE sous Linux     Exporter/importer un schéma de base     Fusionner les espaces libre contigus     Gestion des dates Oracle     Lister les locks sur Oracle     Tracer une requête Oracle     Utiliser DataPump en ligne de commande    ", 
    "breadcrumb": " > serveurs > oracle.html"
},
{
    
    "uri": "/outils.html",
    "title": "Outils",
    "tags": [],
    "description": "Les outils pour le dev, Git, Ansible, Docker, ...",
    "content": "\n Ansible    Template error while templating string: Missing end of comment tag    Docker    Accès Docker non root   Autoriser l’acces au DISPLAY pour un container specifique   Changer le répertoire des images docker   Docker failed to create image rootfs   Export Importer une image Docker   InetAddress does’nt resolve ip on alpine docker container   Registry Docker derrière un Nginx   Remove ophans from docker registry   Supprimer les images non taggé   Supprimer les vieux conteneurs   System error on docker run   Update toutes les images   Weekly cleaner    Eclipse    Ajouter la tache SCP a Ant sous Eclipse   Crash d’Eclipse   Erreur Cannot connect to VM lors dun debug   Locking is not possible in the directory ...    Git    Accélérer le clonage d’un repo Git   Changelog depuis milestone gitlab   Convention de message de commit   Git en ligne de commande   Git rerere   Git un modèle de branches efficace   Introduction à GIT   Push sur plusieurs repo git   Regrouper des repos git sans perdre l’historique   gitflow-breakdown   non fast forward updates were rejected    IntelliJ    Ajouter le dictionnaire français   Meilleures options de JVM pour IntelliJ    LaTeX    Dollar en latex   Mettre à jour un package LaTeX   Optimizer la taille d’un PDF   Visualisation instantané de modifications Latex    VMWare    ESXi 4.1 avec VMPlayer   Geler l’heure d une VM   Installer VMWare Tools sur un guest Debian   Recompilation VMPlayer sous Linux   Redirection de ports avec VMWare Server   Retrouver la configuration réseau VMWare   Stopper une VM sur le serveur en ligne de commande   VMTools en ligne de commande    Gollum at startup     Jekyll     Logstach config for Metrics     Optimisation de PNG     Rafraichir un repo git dans Redmine     Trucs et Astuce SoapUI    ", 
    "breadcrumb": " > outils.html"
},
{
    
    "uri": "/linux/network.html",
    "title": "Réseau",
    "tags": [],
    "description": "",
    "content": "\n Authentification SSH par certificat     Avoir plusieurs adresses IP sur la meme interface     Configuration firewall IPTables     Configuration réseau sur VM Linux     DNS Securise     Domaines de recherche pour NetworkManager     Download fichier via SSH avec reprise     Débloquer le wifi     Firefox CommandLine Options     Lancer TCPDump en non root     NFS au travers de SSH     SSH Vérification de la clé hôte échoué     SSH: Reprendre la main sur un session perdue     Scanner les ports sur une IP     WGET avec un proxy     ssh, control de tunnel via socket    ", 
    "breadcrumb": " > linux > network.html"
},
{
    
    "uri": "/serveurs.html",
    "title": "Serveur",
    "tags": [],
    "description": "Les serveurs dans l’ensemble, HTTP, Base de données, ...",
    "content": "\n HTTP    1s de cache contre le downtime   HTTPS Securisé avec Nginx   Hôtes virtuels SSL multiples   Nginx WebDAV et problème de crochets   Optimiser les performances    JBoss    Connexion JBoss SQL-Server   Gérer l’ordre de déploiement dans JBoss    MySQL    Erreur de read-only status avec JDBC   Exporter/Importer une base d’un dump   Timezoner MySQL   UUID Most Significant Bits    Neo4J    Cypher: Nodes sans Label   Cypher: Paths sans path intermédiaire   Recovery required from position LogPosition    Oracle    Changer le nom d’une machine sans péter XE   Changer le port de l’interface d’admin de XE   Connaître l’encodage de sa base   Démarrage/Arrêt automatique d’Oracle sous Linux   Démarrer/Arrêter une base en ligne de commande Windows   Désinstaller Oracle XE sous Linux   Exporter/importer un schéma de base   Fusionner les espaces libre contigus   Gestion des dates Oracle   Lister les locks sur Oracle   Tracer une requête Oracle   Utiliser DataPump en ligne de commande    TeraData    Logger les requêtes Teradata   [Error 1178] [SQLState HY000] The Teradata Database did not provide the expected response   ", 
    "breadcrumb": " > serveurs.html"
},
{
    
    "uri": "/linux/shell.html",
    "title": "Shell",
    "tags": [],
    "description": "",
    "content": "\n Crypter tar.gz avec mot de passe     Découper et rattacher un gros fichier     Effacer définitivement un disque dur     Envoyer POST avec cURL     Formater simplement un XML     Lecture d’arguments en bash     Lister les fichiers d’une arborescence     Mettre un processus en pause     Modifier une fonction shell     Multi-thread avec xargs     Raccourcis Terminal     Rechercher dans les fichiers     Recuperer le chemin d’un script bash     Rendre exécutable ce qui peut l’être dans une arborescence     Renommer des fichiers sous Linux     Suppression de fichiers en masse     Suppression de fichiers par Inode     Supprimer un type de fichier dans une arborescence     Synchronisation Rsync     Tmux cheatcheet     Trouver les UUID de mes partitions     Trouver les gros fichiers     Télécharger un répertoire entier via FTP     Unifier des PDFs     Using MPC     Web-server en une ligne de commande    ", 
    "breadcrumb": " > linux > shell.html"
},
{
    
    "uri": "/development/spring.html",
    "title": "Spring",
    "tags": [],
    "description": "",
    "content": "\n Connection securisé et reverse proxy    ", 
    "breadcrumb": " > development > spring.html"
},
{
    
    "uri": "/serveurs/teradata.html",
    "title": "TeraData",
    "tags": [],
    "description": "",
    "content": "\n Logger les requêtes Teradata     [Error 1178] [SQLState HY000] The Teradata Database did not provide the expected response    ", 
    "breadcrumb": " > serveurs > teradata.html"
},
{
    
    "uri": "/outils/vmware.html",
    "title": "VMWare",
    "tags": [],
    "description": "",
    "content": "\n ESXi 4.1 avec VMPlayer     Geler l’heure d une VM     Installer VMWare Tools sur un guest Debian     Recompilation VMPlayer sous Linux     Redirection de ports avec VMWare Server     Retrouver la configuration réseau VMWare     Stopper une VM sur le serveur en ligne de commande     VMTools en ligne de commande    ", 
    "breadcrumb": " > outils > vmware.html"
},
{
    
    "uri": "/serveurs/teradata/logger-les-requetes-teradata.html",
    "title": " Logger les requêtes Teradata",
    "tags": ["server", "teradata", "logs"],
    "description": "",
    "content": "Dans un console bteq connecté en dbc :\nPour un seul utilisateur. Mais ça a pas trop marché pour moi\nbegin query logging with explain,objects, sql on all account=(\u0026#39;myaccount\u0026#39;) end query logging with explain,objects, sql on all account=(\u0026#39;myaccount\u0026#39;) Pour tous les utilisateurs et là ça a fonctionné\nbegin query logging with objects, sql on all end query logging with objects, sql on all Et pour lister les règles\nselect * from dbc.dbqlrules Enfin, pour voir les requêtes enregistrées\nSELECT * FROM DBC.QryLogSQL;", 
    "breadcrumb": " > serveurs > teradata > logger-les-requetes-teradata.html"
},
{
    
    "uri": "/development/dotnet/505-http-version-not-supported-en-.net.html",
    "title": "(505) HTTP version not supported en .Net",
    "tags": ["development", "dotnet", ".net", "ws"],
    "description": "",
    "content": "Lors de l\u0026rsquo;appel d\u0026rsquo;un WS Cameleon depuis .Net on peut tomber sur ce genre de message super explicite ! Pour résoudre le soucis, il faut ajouter les lignes suivante dans le fichier de configuration du WS client :\n\u0026lt;system.net\u0026gt; \u0026lt;settings\u0026gt; \u0026lt;servicePointManager expect100Continue=\u0026#34;false\u0026#34;/\u0026gt; \u0026lt;/settings\u0026gt; \u0026lt;/system.net\u0026gt; ", 
    "breadcrumb": " > development > dotnet > 505-http-version-not-supported-en-.net.html"
},
{
    
    "uri": "/tags/.net.html",
    "title": ".Net",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > .net.html"
},
{
    
    "uri": "/serveurs/http/1s-de-cache-nginx.html",
    "title": "1s de cache contre le downtime",
    "tags": ["server", "http", "nginx", "cache"],
    "description": "",
    "content": " Copie depuis https://lord.re/posts/60-cache-proxy-nginx/ de lord@lord.re\nDepuis que mon serveur ne me sert plus de routeur il m’arrive de le couper de temps à autres. Et pourtant mon site reste accessible. J’ai en fait, sur mon routeur, installé un container avec un nginx qui tourne et qui proxy. Les connexions se font donc via le nginx du routeur qui sert de cache quand le vrai serveur ne répond pas. Mais histoire de ne pas avoir de contenu pas à jour mais toujours d’une fraîcheur exemplaire je me contente d’un cache de maximum 1seconde.\n1s de cache Oui oui une seconde suffit. Bon dans mon cas c’est overkill car mon site est statique mais pour des sites dynamiques à fort trafic c’est clairement valable. Fournir un contenu vieu d’un seconde n’est généralement pas gênant. Par contre la différence de perf est assez énormissime.\nMais au delà de ça, si on rajoute deux trois options de configuration, vous pourrez vous prémunir des downtime (ce que je recherchais surtout).\nBon on va définir le_cache qui va être l’endroit où seront stockées nos données en cache : proxy_cache_path /var/www/cache keys_zone=le_cache:1m max_size=20m inactive=10d use_temp_path=off;\nBon sur la machine qui va vous servir de proxy vous allez dans la conf du bel nginx /etc/conf/nginx/nginx.conf vous ajoutez la conf du vhost:\nserver { listen 80; listen 443 ssl http2; server_name www.lordtoniok.com lordtoniok.com bender.lordtoniok.com www.lord.re lord.re bender.lord.re _; include ssl.conf; ssl_certificate /etc/ssl/acme/lord.re/fullchain.pem; ssl_certificate_key /etc/ssl/acme/private/lord.re/privkey.pem; location /.well-known/acme-challenge { alias /var/www/acme; } location / { proxy_cache le_cache; proxy_pass http://10.0.0.1; proxy_cache_lock on; proxy_cache_use_stale updating error timeout http_502 http_503; proxy_cache_valid 200 1s; add_header X-Cache-Status $upstream_cache_status; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # proxy_buffering off;  } } La magie se trouve dans le proxy_cache_use_stale qui fait en sorte d’envoyer le cache si le serveur upstream ne répond pas.\nDésormais je peux couper le serveur sans que ça ne se voit. Ça peut permettre de mettre à jour l’esprit libre. Sur un site dynamique ça peut énormément booster les perfs sans trop de détriments (surtout avec juste 1s de cache).\nLe proxy_buffering off; n’est peut-être pas adapté à votre cas mais si je ne le met pas, lorsqu’un bienveillant internaute tente de récupérer un fichier un poil gros (plus de quelques Mo) bha ça déconne de partout car la machine a peu de ram, donc ça rentre pas en ram, donc nginx tente de fouttre ça dans le cache, mais comme j’ai restreint le cache à 20Mo… bha si ça rentre pas dedans ça merdoie et ça n’envoi plus les données. Voilà voilà. Donc là je l’ai mis en commentaire car cet exemple n’est qu’un morceau de ma conf complète mais je voulais quand même vous présenter cette option.\n", 
    "breadcrumb": " > serveurs > http > 1s-de-cache-nginx.html"
},
{
    
    "uri": "/outils/docker/acces-docker-non-root.html",
    "title": "Accès Docker non root",
    "tags": ["development", "docker", "linux"],
    "description": "",
    "content": "Normalement tous les accès docker se font en root avec sudo ou autre. Pour utiliser docker en non-root :\nsudo usermod -a -G docker netflow", 
    "breadcrumb": " > outils > docker > acces-docker-non-root.html"
},
{
    
    "uri": "/development/java/acces-jndi-de-jboss.html",
    "title": "Accès JNDI de JBoss",
    "tags": ["development", "java", "jndi", "jboss"],
    "description": "",
    "content": "Pour accéder au composants EJB déployé sous JBoss, voici un exemple de code. Il récupère le ~CorbasManager mais ça marche avec tout les EJB déployé.\n// Get initial context of JNDI tree Hashtable\u0026lt;String, String\u0026gt; w_param = new Hashtable\u0026lt;String, String\u0026gt;(); w_param.put(javax.naming.Context.INITIAL_CONTEXT_FACTORY,\u0026#34;org.jnp.interfaces.NamingContextFactory\u0026#34;); w_param.put(javax.naming.Context.PROVIDER_URL, \u0026#34;jnp://192.168.168.128:1099/\u0026#34;); javax.naming.Context ctx = new javax.naming.InitialContext(w_param); // Get advanced pricing EJB home Object obj = ctx.lookup(\u0026#34;cic.CICCorbaManagerEJBHome\u0026#34;); cic.CICCorbaManagerEJBHome w_corbasHome = (cic.CICCorbaManagerEJBHome)javax.rmi.PortableRemoteObject.narrow(obj,cic.CICCorbaManagerEJBHome.class); // Create a new advanced pricing session cic.CICCorbaManagerEJB w_corbasManager = w_corbasHome.create(\u0026#34;CORBAS\u0026#34;); System.out.println(\u0026#34;Cleanup Status : \u0026#34;+w_corbasManager.getCleanupStatus());\tw_corbasManager.clearCache(); System.out.println(\u0026#34;Cleanup Status : \u0026#34;+w_corbasManager.getCleanupStatus()); Pour lister les EJB de l’annuaire on peut faire comme ça :\nEnumeration\u0026lt;NameClassPair\u0026gt; w_list = ctx.list(\u0026#34;\u0026#34;); while (w_list.hasMoreElements()) { NameClassPair t_class = w_list.nextElement(); System.out.println(t_class.getName()); } ATTENTION Penser à faire un remove sur l’EJB à la fin pour ne pas générer un aspirateur de connexion, le Corbas étant limité !\n", 
    "breadcrumb": " > development > java > acces-jndi-de-jboss.html"
},
{
    
    "uri": "/development/dotnet/acces-au-ws-securise-en-.net.html",
    "title": "Accès au WS Sécurisé en .Net",
    "tags": ["development", "dotnet", ".net", "security", "ws"],
    "description": "",
    "content": "Depuis la version 8 (je sais plus quel fix), les services Cameleon sont sécurisé. La difficulté est donc maintenant de les accéder en .Net, c\u0026rsquo;est moins facile qu\u0026rsquo;en Java !\nAvec chaque WS client .Net, il y a un fichier de configuration associé dans lequel on met par exemple le endpoint et ce genre de truc. C\u0026rsquo;est dans ce fichier qu\u0026rsquo;il faut configurer l\u0026rsquo;authentification du WS en ajoutant les lignes :\n\u0026lt;security mode=\u0026#34;TransportCredentialOnly\u0026#34;\u0026gt; \u0026lt;transport clientCredentialType=\u0026#34;Basic\u0026#34; proxyCredentialType=\u0026#34;None\u0026#34; realm=\u0026#34;Cameleon Ws\u0026#34;/\u0026gt; \u0026lt;message clientCredentialType=\u0026#34;UserName\u0026#34; algorithmSuite=\u0026#34;Default\u0026#34;/\u0026gt; \u0026lt;/security\u0026gt; Il faudra ensuite bien sur, dans le code qui consomme le WS, ajouter les lignes de code pour passer le user et le password :\ncart.ClientCredentials.UserName.UserName = \u0026#34;cameleon\u0026#34;; cart.ClientCredentials.UserName.Password = \u0026#34;leon\u0026#34;;", 
    "breadcrumb": " > development > dotnet > acces-au-ws-securise-en-.net.html"
},
{
    
    "uri": "/development/java/acceder-un-membre-classe-private.html",
    "title": "Accéder un membre classe private",
    "tags": ["development", "java", "reflection"],
    "description": "",
    "content": "Pour des raison de test, on peut avoir besoin d\u0026rsquo;accéder des membres de classe privé pour tester leur contenu. Il est possible de faire ça sans forcément ajouter des accesseurs \u0026ldquo;juste pour les tests\u0026rdquo; sur le classe testé.\npublic static \u0026lt;T\u0026gt; Object getPrivateMember(T testObject, String fieldName) { try { Field field = testObject.getClass().getDeclaredField(fieldName); field.setAccessible(true); return field.get(testObject); } catch (Exception e) { throw new RuntimeException(e); } } Et à l’usage :\nList\u0026lt;TableDaoBuffer\u0026gt; bufferDAO = (List\u0026lt;TableDaoBuffer\u0026gt;) getPrivateMember(vpnResponseTimePacketPush, \u0026#34;bufferDAO\u0026#34;); Où bufferDAO est le nom de la variable private.\nRemarque : Il est aussi possible de les setter.\n", 
    "breadcrumb": " > development > java > acceder-un-membre-classe-private.html"
},
{
    
    "uri": "/outils/git/speedup-git-clone.html",
    "title": "Accélérer le clonage d’un repo Git",
    "tags": ["outils", "git", "script", "gitlab"],
    "description": "",
    "content": "git clone --depth 1 git@gitlab.i-run.fr:irun/.git cd irun-core git fetch --depth 1 origin mabranch:mabranch git diff --name-only master.. | awk -F / \u0026#39;{print $1}\u0026#39; | uniq ", 
    "breadcrumb": " > outils > git > speedup-git-clone.html"
},
{
    
    "uri": "/linux/administration/activer-sudo-sur-debian.html",
    "title": "Activer SUDO sur Debian",
    "tags": ["linux", "sysadmin", "debian", "sudo"],
    "description": "",
    "content": " Pour des raisons pratique et de sécurité, il peut être intéressant d\u0026rsquo;activer SUDO. Sur les Ubuntu c\u0026rsquo;est le cas par défaut, pas sur les Debian. Voilà la marche à suivre pour le faire.\nActivation de SUDO Se connecter en administrateur $ su - Installer le paquet # apt-get install sudo Ajouter l\u0026rsquo;utilisateur administrateur aux sudoers Sudo autorise des utilisateurs normaux, comme celui qui est créé lors de l\u0026rsquo;installation, à se connecter en tant que super utilisateur. Pour avoir ce privilège, un utilisateur standard doit être placé dans le group sudo.\n# adduser administrateur sudo Ajout de l\u0026#39;utilisateur « administrateur » au groupe « sudo »... Ajout de l\u0026#39;utilisateur administrateur au groupe sudo Fait. Il faut se déconnecter de l\u0026rsquo;utilisateur administrateur pour que la modification soit effective. A la reconnexion testé avec la commande :\n$ sudo -i Il vous est alors demandé de saisir un mot de passe. C\u0026rsquo;est votre mot de passe, celui de administrateur et pas celui de root qu\u0026rsquo;il faut saisir !\nDésactiver l\u0026rsquo;utilisateur root Il n\u0026rsquo;est pas possible de supprimer l\u0026rsquo;utilisateur root mais il est possible de le désactiver :\n# passwd -l root Il est maintenant impossible de se connecter en root autrement que via sudo.\n## Éviter la saisie du mot de passe Maintenant, quand vous faites un sudo il vous est demandé le mot de passe. Il est possible d\u0026rsquo;éviter d\u0026rsquo;avoir à saisir le mot de passe en procédant comme suis. C\u0026rsquo;est pas très sécurisé mais sur des machines qui ne risque pas grand chose, comme les VMs client, ça permet de gagner du temps.\nEn tant que root :\n# visudo Puis à la fin du fichier, ajouter la ligne :\nusername ALL=(ALL) NOPASSWD: ALL  en remplaçant username par le nom de l\u0026rsquo;utilisateur qui n\u0026rsquo;aura plus à renseigner le password lors de la commande sudo (exemple: administrateur).\n", 
    "breadcrumb": " > linux > administration > activer-sudo-sur-debian.html"
},
{
    
    "uri": "/linux/gnomeshell/afficher-la-date-du-jour-dans-le-bandeau.html",
    "title": "Afficher la date du jour dans le bandeau",
    "tags": ["linux", "gnome", "shell", "debian"],
    "description": "",
    "content": "gsettings set org.gnome.shell.clock show-date true", 
    "breadcrumb": " > linux > gnomeshell > afficher-la-date-du-jour-dans-le-bandeau.html"
},
{
    
    "uri": "/linux/administration/ajouter-de-la-swap-sans-toucher-au-partition.html",
    "title": "Ajouter de la swap sans toucher au partition",
    "tags": ["linux", "sysadmin", "swap", "memory"],
    "description": "",
    "content": "Ouais, il faut un minimum de swap pour installer Oracle XE (ou autre chose) et on a pas toujours ce que l’installeur demande. Par exemple si on a pas choisi de partitionner à la main !\nMais comme Linux c’est quand même super bien branlé, on va pouvoir créer un fichier de swap supplémentaire qui va permettre d’atteindre le minimum demandé sans avoir a retoucher les partoches.\nDéjà, pour savoir combien on a :\n# free Puis on crée un fichier de swap de la taille de l’espace qu’il manque :\n# dd if=/dev/zero of=/swapfile bs=1M count=100 Le count représente l’espace manquant en Mo.\nEnsuite il faut créer le fichier :\n# mkswap /swapfile Puis l’activer :\n# swapon /swapfile Retestez la commande free vous devriez constater l’augmentation de la swap. Cette astuce est temporaire, au prochain démarrage, la swap sera revenue à son niveau précédent, il vous faudra supprimer le fichier /swapfile pour récupérer l’espace.\n", 
    "breadcrumb": " > linux > administration > ajouter-de-la-swap-sans-toucher-au-partition.html"
},
{
    
    "uri": "/outils/eclipse/ajouter-la-tache-scp-a-ant-sous-eclipse.html",
    "title": "Ajouter la tache SCP a Ant sous Eclipse",
    "tags": ["outils", "eclipse", "scp", "ant"],
    "description": "",
    "content": "Aller à cette adresse pour télécharger le jar : http://www.jcraft.com/jsch/index.html\nPuis coller le jar dans ~/.ant/lib pour linux ou C:\\Documents and Settings\\\u0026lt;user\u0026gt;\\.ant\\lib pour Windows\nRelancer Eclipse\n", 
    "breadcrumb": " > outils > eclipse > ajouter-la-tache-scp-a-ant-sous-eclipse.html"
},
{
    
    "uri": "/outils/intellij/ajouter-le-dictionnaire-francais.html",
    "title": "Ajouter le dictionnaire français",
    "tags": ["outils", "intellij"],
    "description": "",
    "content": "Oui les commentaires dans le code ça devrait toujours être en anglais\u0026hellip; Mais parfois, on fait aussi de la documentation ou des choses du genre qui peuvent être traduite et dans ce cas on aimerait bien ne pas avoir tout les mots souligné et pourquoi pas un peu de correction. Donc voilà comment ajouter un dictionnaire à IntelliJ en utilisant Aspell :\naspell --lang fr dump master | aspell --lang fr expand | tr \u0026#39; \u0026#39; \u0026#39;\\n\u0026#39; \u0026gt; french.dic Et ça fonctionne pareil pour les autres langues.\nEnsuite dans IntelliJ, on va dans File -\u0026gt; Settings... -\u0026gt; Editor -\u0026gt; Spelling et on ajoute le répertoire de dictionnaire custom dans lequel on a mis le fichier dictionnaire.\n", 
    "breadcrumb": " > outils > intellij > ajouter-le-dictionnaire-francais.html"
},
{
    
    "uri": "/development/java/ajouter-un-certificat-pour-les-connexions-ssl-java.html",
    "title": "Ajouter un certificat pour les connexions SSL Java",
    "tags": ["development", "java", "ssl", "crypto", "security"],
    "description": "",
    "content": " Symptôme Dans le cas par exemple d\u0026rsquo;une connexion en LDAPS (LDAP via SSL) on a de bonne chance de se prendre ce genre d’erreur :\njavax.net.ssl.SSLHandshakeException: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target at com.sun.net.ssl.internal.ssl.Alerts.getSSLException(Alerts.java:150) at com.sun.net.ssl.internal.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1584) at com.sun.net.ssl.internal.ssl.Handshaker.fatalSE(Handshaker.java:174) at com.sun.net.ssl.internal.ssl.Handshaker.fatalSE(Handshaker.java:168) at com.sun.net.ssl.internal.ssl.ClientHandshaker.serverCertificate(ClientHandshaker.java:848) at com.sun.net.ssl.internal.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:106) at com.sun.net.ssl.internal.ssl.Handshaker.processLoop(Handshaker.java:495) at com.sun.net.ssl.internal.ssl.Handshaker.process_record(Handshaker.java:433) ...  Note que c’est la même pour les accès à des Web-Service en HTTPS via Axis\nExplication Cette erreur vient du fait que le client n’accepte pas le certificat du serveur. Les histoires des certificats sont manifestement un poil compliqué mais bon, grosso modo, il faut ajouter le certif du serveur à la politique sécurité du client.\nSachant qu\u0026rsquo;on parle du JRE utilisé par le serveur JBoss !\nSolution 1 Solution \u0026ldquo;manuelle\u0026rdquo;, testé chez La Poste et GMC. Les exemples donner sont sous Linux mais ça fonctionne de la même façon sous Windows (j\u0026rsquo;ai testé). La commande keytool est dans les binaire de java. Sous Windows elle est peut-être pas dans le classpath.\nEn tant que \u0026ldquo;root\u0026rdquo;, exécuter la commande :\nkeytool -importcert -keystore \u0026#34;/usr/lib/jvm/java-6-sun/jre/lib/security/jssecacerts\u0026#34; -trustcacerts -alias \u0026#34;nom.dusitequipublielecertif.fr\u0026#34; -file mon-certificat.cer Attention le chemin de le JRE est pas toujours le même ! Sous linux pour le connaître facilement taper:\nupdate-alternatives --list java\u0026lt;/code\u0026gt; C\u0026rsquo;est évident mais on sait jamais, s\u0026rsquo;il y a plusieurs JRE sur le serveur, il faut exécuter sur celle qu\u0026rsquo;utilise JBoss !!\nLa commande keytool devrait donner un truc du genre, le mot de passe est changeit\nTapez le mot de passe du Keystore : Ressaisissez le nouveau mot de passe : Propriétaire : CN=A10.blabla.test, L=PARIS, C=FR Émetteur : CN=A10.blabla.test, L=PARIS, C=FR Numéro de série : 9397247ec5cf9e4f Valide du : Tue Jul 27 01:23:59 CEST 2010 au : Thu Jul 26 01:23:59 CEST 2012 Empreintes du certificat : MD5 : 27:73:2E:FF:8F:AB:3A:2D:1F:47:42:A5:97:49:CF:74 SHA1 : 31:83:A1:FB:AE:69:91:F3:14:BF:5C:A8:67:2A:FD:CA:83:A6:5B:9A Nom de l'algorithme de signature : MD5withRSA Version : 4 Faire confiance à ce certificat ? [non] : oui Certificat ajouté au Keystore  Après les appels https sur le serveur dont vous venez d\u0026rsquo;enregistrer le certificat devrait passer tout seul :p\nSolution 2 Donc le truc à faire c\u0026rsquo;est d\u0026rsquo;aller récupérer la classe InstallCert.java de la compiler puis de l\u0026rsquo;exécuter avec en paramètre le serveur que vous voulez accéder en SSL :\njavac -g InstallCert.java java InstallCert your-ldap-server.somewhere.edu:636 cp jssecacerts $JAVA_HOME/lib/security En gros, on compile et on execute. Ca va créer un ~KeyStore dans \u0026ldquo;jssecacerts\u0026rdquo; et y placer le certificat de votre serveur. Après, pour que le client appelé depuis Cameleon soit capable de retrouver ce certificat, il faut le placer dans le ~KeyStore par défaut de la JRE utilisé par le serveur Cameleon.\nJe vous invite vivement à vérifier le ~JAVA_HOME dans le .bat ou .sh de lancement de JBoss. Une fois que vous êtes sur du JAVA_HOME, le ~KeyStore par défaut est dans ~JAVA_HOME/lib/security. Vous y recopiez le contenu de \u0026ldquo;jssecacerts\u0026rdquo;.\nOn redémarre le JBoss et hop ça fonctionne normalement !\nLiens Par ordre d\u0026rsquo;intérêt :\n http://blogs.sun.com/andreas/entry/no_more_unable_to_find http://www.google.com/support/forum/p/Google%20Apps/thread?tid=0b9d3f130628f63b\u0026amp;hl=en http://www.forumeasy.com/forums/archive/ldappro/200707/p118350434574.html  ", 
    "breadcrumb": " > development > java > ajouter-un-certificat-pour-les-connexions-ssl-java.html"
},
{
    
    "uri": "/development/java/ajouter-une-police-de-caractere-a-java.html",
    "title": "Ajouter une Police de caractère à Java",
    "tags": ["development", "java", "linux"],
    "description": "",
    "content": "La question se pose notement pour Jasper quand il faut lui rajouter des fonts.\nSous Linux, il faut mettre le fichier TTF dans le répertoire : /usr/lib/jvm/java-6-sun/jre/lib/fonts ou créer un lien dans ce répertoire vers le TTF.\n", 
    "breadcrumb": " > development > java > ajouter-une-police-de-caractere-a-java.html"
},
{
    
    "uri": "/linux/administration/analyze-systemd-boot-speed.html",
    "title": "Analyze systemd boot speed",
    "tags": ["linux", "sysadmin", "systemd"],
    "description": "",
    "content": " Quelques commandes permettant d’analyser les perfs de Systemd systemd-analyze time systemd-analyze blame systemd-analyze critical-chain systemd-analyze plot \u0026gt; plot.svg  Links  http://blog.touret.info/blog/2018/01/20/ameliorer-le-boot-sous-debian-9/#utm_source=rss\u0026amp;utm_medium=rss  ", 
    "breadcrumb": " > linux > administration > analyze-systemd-boot-speed.html"
},
{
    
    "uri": "/tags/ansible.html",
    "title": "Ansible",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > ansible.html"
},
{
    
    "uri": "/tags/ant.html",
    "title": "Ant",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > ant.html"
},
{
    
    "uri": "/tags/apache.html",
    "title": "Apache",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > apache.html"
},
{
    
    "uri": "/development/java/appeler-un-web-service-au-travers-d-un-proxy.html",
    "title": "Appeler un Web-Service au travers d’un proxy",
    "tags": ["development", "java", "ws"],
    "description": "",
    "content": "Pour l\u0026rsquo;instant je n\u0026rsquo;ai pas eu le cas avec Axis 1 donc on verra comment ça marche pour Axis 1, là c\u0026rsquo;est une solution Axis 2.\nDonc depuis le Stub généré par Axis 2, voilà le code :\nOptions options = service._getServiceClient().getOptions(); HttpTransportProperties.ProxyProperties pp = new HttpTransportProperties.ProxyProperties(); pp.setProxyName(\u0026#34;proxy.tl.internal\u0026#34;); pp.setProxyPort(3128); options.setProperty(HTTPConstants.PROXY,pp); service étant l\u0026rsquo;instance de la classe Stub.\n", 
    "breadcrumb": " > development > java > appeler-un-web-service-au-travers-d-un-proxy.html"
},
{
    
    "uri": "/tags/apt.html",
    "title": "Apt",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > apt.html"
},
{
    
    "uri": "/linux/network/authentification-ssh-par-certificat.html",
    "title": "Authentification SSH par certificat",
    "tags": ["linux", "network", "ssh", "secutity"],
    "description": "",
    "content": "L\u0026rsquo;objectif est de pouvoir se connecté via SSH à un serveur sans fournir de mot de passe. Cela ne sera bien-sur possible de depuis une machine ayant la clé privé d\u0026rsquo;installé !\nOn commence par générer la paire de clés si c\u0026rsquo;est pas déjà fait :\nssh-keygen -t rsa -C \u0026#34;your_email@example.com\u0026#34; Laisser les clés se générer dans l\u0026rsquo;emplacement par défaut et laisser la passphrase vide sinon il faudra à chaque fois renseigner la passphrase.\nEnsuite il faut installer la clé publique sur le serveur :\nssh-copy-id -i ~/.ssh/id_dsa.pub user@machine Cette commande ne fait qu’ajouter votre clé publique dans un fichier sur le serveur. Voici une commande équivalente :\ncat ~/.ssh/id_rsa.pub | ssh user@machine \u0026#34;cat - \u0026gt;\u0026gt; ~/.ssh/authorized_keys\u0026#34; Grâce à cette technique plus besoin de mot de passe pour les crons par exemple et la password se balade pas sur le réseau. Par contre quiconque possède votre clé privé ou accède à votre compte local peut se connecter au serveur.\n", 
    "breadcrumb": " > linux > network > authentification-ssh-par-certificat.html"
},
{
    
    "uri": "/linux/divers/allow-xhost-to-docker.html",
    "title": "Autoriser XHost à Docker",
    "tags": ["linux", "misc", "docker", "x11"],
    "description": "",
    "content": "Pour une raison que j\u0026rsquo;ignore, une mise à jour récente de Debian bloque l\u0026rsquo;accès Xhost au container Docker locaux. Pour authoriser à nouveau il faut un xhost +. Pour éviter le mode open bar, on utilise la commande suivante :\nxhost +local:docker Pour l\u0026rsquo;activer au boot de la machine j\u0026rsquo;ai modifié /etc/rc.local et rajouter :\n... # By default this script does nothing.  /usr/bin/xhost +local:docker exit 0 ...", 
    "breadcrumb": " > linux > divers > allow-xhost-to-docker.html"
},
{
    
    "uri": "/outils/docker/autoriser-lacces-au-display-pour-un-container-specifique.html",
    "title": "Autoriser l’acces au DISPLAY pour un container specifique",
    "tags": ["development", "docker", "network"],
    "description": "",
    "content": "Avec les nouvelles version du noyau, le DISPLAY est plus verrouillé qu’avant et des problèmes de DISPLAY interviennent lorsque l’on veut accéder à une appli graphique à l’intérieur d’un docker. Pour résoudre ce problème il faut autoriser le DISPLAY pour le container, pour cela :\n{% raw %}xhost +local:`docker inspect --format=\u0026#39;{{ .Config.Hostname }}\u0026#39; $containerId`{% endraw %} L’idéal serait de la mettre ensuite dans le démarrage du système \u0026hellip;\n", 
    "breadcrumb": " > outils > docker > autoriser-lacces-au-display-pour-un-container-specifique.html"
},
{
    
    "uri": "/linux/network/avoir-plusieurs-adresses-ip-sur-la-meme-interface.html",
    "title": "Avoir plusieurs adresses IP sur la meme interface",
    "tags": ["linux", "network", "interface"],
    "description": "",
    "content": " Avec Linux (je sais pas pour Windows) il est possible de configurer plusieurs adresse IP pour une même carte réseau.\nUtilité A quoi ça sert ? Le cas qui m\u0026rsquo;intéressait au moment de trouver ça c\u0026rsquo;est le cas d\u0026rsquo;un serveur Apache publiant plusieurs serveurs virtuels en SSL. Dans ce cas Apache n\u0026rsquo;est pas capable pour des raisons technique de fournir cette fonctionnalité a partir d\u0026rsquo;un seul couple IP:PORT. Mais si notre serveur apache dispose de plusieurs IP il devient capable de faire ça.\nConfiguration Il faut éditer le fichier /etc/network/interfaces comme suit :\n# This file describes the network interfaces available on your system # and how to activate them. For more information, see interfaces(5). # The loopback network interface auto lo iface lo inet loopback # The primary network interface allow-hotplug eth0 #iface eth0 inet dhcp iface eth0 inet static address 172.16.139.20 netmask 255.255.255.0 gateway 172.16.139.254 up ip addr add 172.16.139.21/24 dev eth0 label eth0:0 down ip addr del 172.16.139.21/24 dev eth0 label eth0:0 up ip addr add 172.16.139.22/24 dev eth0 label eth0:1 down ip addr del 172.16.139.22/24 dev eth0 label eth0:1  Ici on se sert de l\u0026rsquo;outil \u0026ldquo;ip\u0026rdquo; (futur remplaçant du vieillissant \u0026ldquo;ifconfig\u0026rdquo;) pour ajouter des adresses à des label d\u0026rsquo;eth0 (l\u0026rsquo;interface par défaut).\nRemarque : Pour les VMs le gateway par défaut est x.x.x.2\nUne fois le réseau redémarré, on obtiendra donc l\u0026rsquo;affichage suivant :\nroot@ubuntu:~# ip addr show 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 16436 qdisc noqueue state UNKNOWN link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eth0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:0c:29:3c:85:09 brd ff:ff:ff:ff:ff:ff inet 172.16.139.20/24 brd 172.16.139.255 scope global eth0 inet 172.16.139.21/24 scope global secondary eth0:0 inet 172.16.139.22/24 scope global secondary eth0:1 inet6 fe80::20c:29ff:fe3c:8509/64 scope link valid_lft forever preferred_lft forever Liens  http://www.cyberciti.biz/faq/bind-alias-range-of-ip-address-in-linux/ http://wiki.debian.org/NetworkConfiguration#Multiple_IP_addresses_on_One_Interface  ", 
    "breadcrumb": " > linux > network > avoir-plusieurs-adresses-ip-sur-la-meme-interface.html"
},
{
    
    "uri": "/tags/backup.html",
    "title": "Backup",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > backup.html"
},
{
    
    "uri": "/tags/bash.html",
    "title": "Bash",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > bash.html"
},
{
    
    "uri": "/tags/branching.html",
    "title": "Branching",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > branching.html"
},
{
    
    "uri": "/tags/browser.html",
    "title": "Browser",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > browser.html"
},
{
    
    "uri": "/tags/bsd.html",
    "title": "Bsd",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > bsd.html"
},
{
    
    "uri": "/tags/cache.html",
    "title": "Cache",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > cache.html"
},
{
    
    "uri": "/categories.html",
    "title": "Categories",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > categories.html"
},
{
    
    "uri": "/tags/cec.html",
    "title": "Cec",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > cec.html"
},
{
    
    "uri": "/linux/administration/change-active-directory-password-on-linux.html",
    "title": "Change Active Directory password on Linux",
    "tags": ["linux", "sysadmin", "security"],
    "description": "",
    "content": "Pour changer son mot de passe ActiveDirectory sans passer par une machine Windows, il est possible de le faire en ligne de commande :\napt-get install samba-common-bin smbpasswd -r \u0026lt;IP ActiveDirectory\u0026gt; -U \u0026lt;user\u0026gt; Si le DNS est configuré correctement, il est possible de trouver son IP avec la commande suivante :\nnslookup _ldap._tcp.dc._msdcs.\u0026lt;DOMAIN\u0026gt;", 
    "breadcrumb": " > linux > administration > change-active-directory-password-on-linux.html"
},
{
    
    "uri": "/outils/git/changelog-depuis-milestone-gitlab.html",
    "title": "Changelog depuis milestone gitlab",
    "tags": ["outils", "git", "script", "gitlab"],
    "description": "",
    "content": "En utilisant les Milestones dans gitlab il est possible de générer des fichiers de change log pour une version donnée.\nfunction changelog { if [ $# -lt 2 ]; then echo -e \u0026#34;USAGE: $0\u0026lt;project\u0026gt; \u0026lt;milestone\u0026gt;\u0026#34; return fi local gitlab=\u0026#34;http://framagit.org/api/v3/\u0026#34; local projectName=${1} local milestone=${2} local projectId=`curl -s -H \u0026#34;PRIVATE-TOKEN: ${GITLAB_PRIVATE_TOKEN}\u0026#34; ${gitlab}projects/search/cosmos | jq -r \u0026#39;.[0].id\u0026#39;` curl -s -H \u0026#34;PRIVATE-TOKEN: ${GITLAB_PRIVATE_TOKEN}\u0026#34; ${gitlab}projects/${projectId}/issues\\?milestone\\=${milestone}\\\u0026amp;state\\=closed\\\u0026amp;order_by\\=updated_at | \\  jq -r \u0026#39;.[] | \u0026#34; * #\\(.iid): \\(.title)\u0026#34;\u0026#39; } GITLAB_PRIVATE_TOKEN contient le token privé d\u0026rsquo;accès a gitlab.\n", 
    "breadcrumb": " > outils > git > changelog-depuis-milestone-gitlab.html"
},
{
    
    "uri": "/linux/administration/changer-le-nom-dune-machine.html",
    "title": "Changer le nom d’une machine",
    "tags": ["linux", "sysadmin"],
    "description": "",
    "content": "En tant que root éditer les fichiers suivant pour remplacer l\u0026rsquo;ancien nom par le nouveau :\n /etc/hostname /etc/hosts  Si base oracle installé -\u0026gt; Changer le nom d’une machine sans casser XE\n", 
    "breadcrumb": " > linux > administration > changer-le-nom-dune-machine.html"
},
{
    
    "uri": "/serveurs/oracle/changer-le-nom-d-une-machine-sans-casser-xe.html",
    "title": "Changer le nom d’une machine sans péter XE",
    "tags": ["server", "oracle", "database"],
    "description": "",
    "content": " Changer le nom d’une machine   Puis chercher les fichiers de configuration réseau (par défaut dans : /usr/lib/oracle/xe/app/oracle/product/10.2.0/server/network/admin ) * listener.ora * tnsnames.ora\nRemplacer l\u0026rsquo;ancien par le nouveau nom.\nAttention, faut remplacer le NOM de la machine par le nouveau NOM, pas par l\u0026rsquo;IP ni par localhost ni un truc du genre sinon ça fonctionne pas !\n", 
    "breadcrumb": " > serveurs > oracle > changer-le-nom-d-une-machine-sans-casser-xe.html"
},
{
    
    "uri": "/serveurs/oracle/changer-le-port-de-linterface-dadmin-de-xe.html",
    "title": "Changer le port de l’interface d’admin de XE",
    "tags": ["server", "oracle", "database"],
    "description": "",
    "content": "Si vous utilisez CCM avec une base Oracle XE, vous allez devoir changer le port de l\u0026rsquo;interface d\u0026rsquo;administration de votre base Oracle XE. En effet, si l\u0026rsquo;interface d\u0026rsquo;Oracle XE est déjà installé sur le port 8080 et que vous installez ensuite CCM, CCM va quand même choisir le port 8080.\nIl faut donc exécuter la manipulation suivante :\n Veiller à ce que votre base tourne Ouvrir une invite de commande Lancer sqlplus Se logguer en tant qu\u0026rsquo;utilisateur système Exécuter la commande suivante (exemple : on change le port 8080 pour le port 8280):  EXEC DBMS_XDB.SETHTTPPORT(\u0026#39;8082\u0026#39;);", 
    "breadcrumb": " > serveurs > oracle > changer-le-port-de-linterface-dadmin-de-xe.html"
},
{
    
    "uri": "/outils/docker/changer-le-repertoire-des-images-docker.html",
    "title": "Changer le répertoire des images docker",
    "tags": ["development", "docker"],
    "description": "",
    "content": "Comme j\u0026rsquo;ai deux disques durs et que je voudrais pas charger le SSD pour rien, je voulais changer le répertoire des images.\nPour ça, dans le fichier /etc/default/docker.io il faut rajouter l\u0026rsquo;option -g dans les options :\n# Use DOCKER_OPTS to modify the daemon startup options. #DOCKER_OPTS=\u0026quot;--dns 8.8.8.8 --dns 8.8.4.4\u0026quot; DOCKER_OPTS=\u0026quot;-g /home/docker\u0026quot;  pensez à créer le nouveau répertoire, voire à y copier le contenu de l\u0026rsquo;ancien répertoire /var/lib/docker\n", 
    "breadcrumb": " > outils > docker > changer-le-repertoire-des-images-docker.html"
},
{
    
    "uri": "/tags/charset.html",
    "title": "Charset",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > charset.html"
},
{
    
    "uri": "/tags/class.html",
    "title": "Class",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > class.html"
},
{
    
    "uri": "/development/javascript/classes-javascript-propres.html",
    "title": "Classes Javascript propres",
    "tags": ["development", "javascript", "class"],
    "description": "",
    "content": "Il existe de nombreuses façons de coder en Javascript. La plus fréquente est malheureusement la plus crade : lister ses fonctions les une après les autres\u0026hellip;\nVoici une méthode pour faire ça moins crados, il en existe peut être de meilleures mais celle-ci a le mérite d\u0026rsquo;être très propre et très lisible.\nvar monProjet = {}; // On crée une variable globale du nom du projet // Ce tableau permettra de stocker et récupérer facilement les objets créés. //Je lui donne le nom de la classe avec un \u0026#39;s\u0026#39; marquant le pluriel : monProjet.maClasses = new Array(); /** * Classe maClasse : fait des choses. */ monProjet.maClasse = function(attr) { // Variable  this.variable1 = attr; monProjet.maClasses.push(this); this.init(); }; // Méthodes publiques de maClasse monProjet.maClasse.prototype = { /** * Initialisation, méthode apellée à l\u0026#39;instanciation de l\u0026#39;objet */ init: function() { this.variable2 = this.variable1 + 10; }, /** * Autre méthode qui va afficher variable2 dans une alerte */ autreMethode: function() { alert(this.variable2); }, /** * Retourne une valeur de l\u0026#39;objet sous forme de chaine */ toString: function() { return \u0026#34;maClasse Object\u0026#34;; } }; // Méthodes Statiques /** * Une methode statique qui retourne \u0026#39;yo\u0026#39; */ monProjet.maClasse.uneMethodeStatique = function() { return \u0026#34;yo\u0026#34;; }; /** * Le genre de méthode statique où on se rends compte de l\u0026#39;utilité du tableau d\u0026#39;objets vu plus haut */ monProjet.maClasse.getByVar1 = function(var1) { for (var i = 0; monProjet.maClasses[i]; i++) { if (monProjet.maClasses[i].variable1 == var1) { return monProjet.maClasses[i]; } } return false; }; //On peut ensuite créer un objet maClasse et apeller ses méthodes publiques et statiques :  var unObjet = new monProjet.maClasse(\u0026#34;youpi\u0026#34;); unObjet.toString(); // retourne \u0026#34;maClasse Object\u0026#34; monProjet.maClasse.getByVar1(\u0026#34;youpi\u0026#34;); // retourne l\u0026#39;objet \u0026#34;unObjet\u0026#34; monProjet.maClasse.getByVar1(\u0026#34;youpi\u0026#34;).autreMethode(); // Crée une alerte ", 
    "breadcrumb": " > development > javascript > classes-javascript-propres.html"
},
{
    
    "uri": "/tags/cleanup.html",
    "title": "Cleanup",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > cleanup.html"
},
{
    
    "uri": "/linux/divers/cle-usb-en-lecture-seule.html",
    "title": "Clé USB en lecture seule",
    "tags": ["linux", "misc", "usb"],
    "description": "",
    "content": " Pour diverses raisons, une clé USB (notamment celles en FAT32) peuvent se retrouver monté en lecture seule. Il y a une multitude de cas et de solutions :\nMontage auto \u0026ldquo;as root\u0026rdquo; C\u0026rsquo;est le montage automatique de Gnome ou autre qui se fait en tant que root et du coup impossible d\u0026rsquo;écrire autrement qu\u0026rsquo;en étant root. Dans ce cas vérifier dans /etc/fstab qu\u0026rsquo;il n\u0026rsquo;y ai pas les lignes suivantes :\n/dev/sdc1 /media/usb0 auto rw,user,noauto 0 0 /dev/sdc2 /media/usb1 auto rw,user,noauto 0 0  Mieux vaut pas de configuration par défaut qu\u0026rsquo;une mauvaise conf.\n", 
    "breadcrumb": " > linux > divers > cle-usb-en-lecture-seule.html"
},
{
    
    "uri": "/tags/code.html",
    "title": "Code",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > code.html"
},
{
    
    "uri": "/linux/administration/comment-installer-une-alternative.html",
    "title": "Comment installer une alternative",
    "tags": ["linux", "sysadmin"],
    "description": "",
    "content": "Sous Debian il y a un outil pas mal qui permet de switcher entre plusieurs versions d\u0026rsquo;un même exécutable, c\u0026rsquo;est update-alternatives. Pour installer une nouvelle alternative, de java par exemple :\nsudo update-alternatives --install /usr/bin/javac javac /opt/java/jdk1.7.0_51/bin/javac 100 Pour lister les alternatives de java :\nupdate-alternatives --list java Pour configurer une alternative à java :\nsudo update-alternatives --config java", 
    "breadcrumb": " > linux > administration > comment-installer-une-alternative.html"
},
{
    
    "uri": "/linux/divers/comparaisons-avec-diff.html",
    "title": "Comparaisons avec diff",
    "tags": ["linux", "misc", "diff", "bash"],
    "description": "",
    "content": " Quelques commandes diff pour comparer des répertoires et des fichiers :\nRépertoires Comparaison de répertoire en récursif sans tenir compte des espaces :\ndiff -rqwB rep-original rep-modifié | sort \u0026gt; modification-rep.diff Fichiers Comparaison des fichiers sans tenir compte des espaces et au format universel (patch):\ndiff -wBu fichier-original.txt fichier-modifié.txt \u0026gt; fichier-diff.diff", 
    "breadcrumb": " > linux > divers > comparaisons-avec-diff.html"
},
{
    
    "uri": "/tags/concepts.html",
    "title": "Concepts",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > concepts.html"
},
{
    
    "uri": "/development/java/concepts-interessant-en-java-7.html",
    "title": "Concepts intéressants en Java",
    "tags": ["development", "java", "jigsaw", "concepts", "fork", "join", "string", "try"],
    "description": "",
    "content": " Java 9 Jigsaw Java 8 Dépendence des packages L\u0026rsquo;outil jdeps permet de visualiser les dépendances inter-package d\u0026rsquo;un jar. L\u0026rsquo;option -jdkinternal permet de mettre en évidence l\u0026rsquo;utilisation d\u0026rsquo;API interne à Java.\nConcaténation de chaine L\u0026rsquo;utilisation d\u0026rsquo;un StringBuilder n\u0026rsquo;est utile que dans le cas d\u0026rsquo;une boucle. En dehors de ça, le + aura plus de chance d\u0026rsquo;être optimisé correctement par le JIT.\nArrayList vs LinkedList Avec les CPU ressant, les ArrayList ont des perf meilleure que les LinkedList. C\u0026rsquo;est du au fait que les ArrayList ont des données contigue en mémoire qui sont chargé plus facilement dans la mémoire L1 du CPU. Cela évite les allez-retour nécessaire pour parcourir une liste chainée.\nDe manière général, le parcours de pointeur a un cout.\nTemporalAdjuster Depuis Java 8 pour les dates il est posssible d\u0026rsquo;utiliser un TemporalAdjuster qui va permettre de récupérer par exemple le premier jour du mois ou de la semaine.\nClock Cette class dispo depuis Java 8 permet de surcharger l\u0026rsquo;heure système et peut donc être utilisé pour les tests. Elle permet de fixer l\u0026rsquo;heure.\nJava 7 En fouinant sur la toile et en assistant a des conférences j\u0026rsquo;ai vu quelques concepts Java 7 intéressant que je ne voudrais pas oublier :\nLe patern Fork/Join C\u0026rsquo;est un truc incorpéré en Java 7 qui permet de dispatcher une tache sécable sur les différents thread (ou coeur) du processeur. Java 7 fourni des classes pré-implémenté pour faire ça :\nRecursiveAction, classe permettant de découper une tâche ne renvoyant aucune valeur particulière. Elle hérite de ForkJoinTask\u0026lt;Void\u0026gt;.\nRecursiveTask\u0026lt;V\u0026gt; identique à la classe précédente mais retourne une valeur, de type \u0026lt;V\u0026gt;, en fin de traitement.\nLes objets qui permettent le découpage en sous-tâches fournissent trois méthodes qui permettent cette gestion :\n compute() : méthode abstraite à redéfinir dans l\u0026rsquo;objet héritant afin de définir le traitement à effectuer ; fork() : méthode qui crée un nouveau thread dans le pool de thread (ForkJoinPool) ; join() : méthode qui permet de récupérer le résultat de la méthode compute().  En gros on crée un pool de tache géré par \u0026ldquo;~ForkJoinPool\u0026rdquo; qui s\u0026rsquo;occupe de dispatcher le tout sur les différents coeur. Puis on lance les différentes taches et sous-taches avec fork() et on récupère la valeur avec join().\nLe try-With-resource Encore un truc spécial J7, c\u0026rsquo;est une syntaxe qui permettre d\u0026rsquo;éviter le finally quand on joue avec les streams.\ntry(DirectoryStream\u0026lt;Path\u0026gt; listing = Files.newDirectoryStream(path)){\tfor(Path nom : listing){ //S\u0026#39;il s\u0026#39;agit d\u0026#39;un dossier, on le scanne grâce à notre objet \tif(Files.isDirectory(nom.toAbsolutePath())){ FolderScanner f = new FolderScanner(nom.toAbsolutePath(), this.filter); result += f.sequentialScan(); } } } catch (IOException e) { e.printStackTrace();} Ce qui se trouve dans le bloc () entre try et { est automatiquement fermé. A condition que la classe en question implémente AutoClosable.\n", 
    "breadcrumb": " > development > java > concepts-interessant-en-java-7.html"
},
{
    
    "uri": "/tags/configuration.html",
    "title": "Configuration",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > configuration.html"
},
{
    
    "uri": "/development/java/configuration-log4j.html",
    "title": "Configuration Log4j",
    "tags": ["development", "java", "logs", "log4j"],
    "description": "",
    "content": "Voilà un log4j.properties qui va bien :\nlog4j.rootLogger = INFO, console log4j.logger.com.livingobjects.pmin = DEBUG log4j.appender.console=org.apache.log4j.ConsoleAppender log4j.appender.console.layout=org.apache.log4j.PatternLayout log4j.appender.console.layout.conversionPattern=%d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %t %C{1}:%L - %m%n log4j.appender.METRICS=org.apache.log4j.ConsoleAppender log4j.appender.METRICS.layout=org.apache.log4j.PatternLayout log4j.appender.METRICS.layout.conversionPattern=%d{yyyy-MM-dd HH:mm:ss} %p - %m%n log4j.logger.com.livingobjects.pmin.cdr.metrics = INFO, METRICS log4j.additivity.com.livingobjects.pmin.cdr.metrics = false", 
    "breadcrumb": " > development > java > configuration-log4j.html"
},
{
    
    "uri": "/linux/administration/configuration-swappiness.html",
    "title": "Configuration Swappiness",
    "tags": ["linux", "sysadmin", "swap", "memory"],
    "description": "",
    "content": " Sur Linux, il y a un paramètre qui indique au système d\u0026rsquo;aller écrire en SWAP même si la RAM est pas pleine. Par défaut, ce paramètre est à 60 ce qui signifie qu\u0026rsquo;a partir du moment où la RAM est pleine à 40%, le système se défoule sur les disques.\nPour le modifier voilà comment faire :\n Ouvrez le fichier /etc/sysctl.conf Modifiez ou ajoutez le paramètre  vm.swappiness=10 De cette façon, le système ne se mettra à swapper qu\u0026rsquo;à partir de 90% de remplissage.\nRemarque Il est possible de connaître le réglage actuel avec la commande suivante :\ncat /proc/sys/vm/swappiness Pour le changer immédiatement pour la session en cours :\nsysctl vm.swappiness=10 Note que dans ce cas, la configuration reviendra à la normale après le prochain démarrage si vous n\u0026rsquo;avez pas modifier le paramètre dans le fichier.\n", 
    "breadcrumb": " > linux > administration > configuration-swappiness.html"
},
{
    
    "uri": "/linux/network/configuration-firewall-iptables.html",
    "title": "Configuration firewall IPTables",
    "tags": ["linux", "network", "firewall", "iptables"],
    "description": "",
    "content": "Voilà une configuration valable pour un firewall IPTables qui donne sur internet à mettre dans un fichier /etc/init.d/firewall.\nPour l\u0026rsquo;installer : update-rc.d firewall defaults  Pour le désinstaller : update-rc.d -f firewall remove\n#!/bin/sh ### BEGIN INIT INFO # Provides: custom firewall # Required-Start: $remote_fs $syslog $network # Required-Stop: $remote_fs $syslog $network # Default-Start: 2 3 4 5 # Default-Stop: 0 1 6 # Short-Description: firewall initscript # Description: Custom Firewall ### END INIT INFO  # Vider les tables actuelles iptables -t filter -F # Vider les règles personnelles iptables -t filter -X # Interdire toute connexion entrante et sortante iptables -t filter -P INPUT DROP iptables -t filter -P FORWARD DROP iptables -t filter -P OUTPUT DROP # ---  # Ne pas casser les connexions etablies iptables -A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT iptables -A OUTPUT -m state --state RELATED,ESTABLISHED -j ACCEPT # Autoriser loopback iptables -t filter -A INPUT -i lo -j ACCEPT iptables -t filter -A OUTPUT -o lo -j ACCEPT # ICMP (Ping) iptables -t filter -A INPUT -p icmp -j ACCEPT iptables -t filter -A OUTPUT -p icmp -j ACCEPT # ---  # SSH In iptables -t filter -A INPUT -p tcp --dport 2222 -j ACCEPT # SSH Out iptables -t filter -A OUTPUT -p tcp --dport 2222 -j ACCEPT # DNS In/Out iptables -t filter -A OUTPUT -p tcp --dport 53 -j ACCEPT iptables -t filter -A OUTPUT -p udp --dport 53 -j ACCEPT iptables -t filter -A INPUT -p tcp --dport 53 -j ACCEPT iptables -t filter -A INPUT -p udp --dport 53 -j ACCEPT # NTP Out iptables -t filter -A OUTPUT -p udp --dport 123 -j ACCEPT # WOL Out iptables -t filter -A OUTPUT -p udp --dport 9 -j ACCEPT # HTTP + HTTPS Out iptables -t filter -A OUTPUT -p tcp --dport 80 -j ACCEPT iptables -t filter -A OUTPUT -p tcp --dport 443 -j ACCEPT # HTTP + HTTPS In iptables -t filter -A INPUT -p tcp --dport 80 -j ACCEPT iptables -t filter -A INPUT -p tcp --dport 443 -j ACCEPT iptables -t filter -A INPUT -p tcp --dport 8443 -j ACCEPT", 
    "breadcrumb": " > linux > network > configuration-firewall-iptables.html"
},
{
    
    "uri": "/linux/network/configuration-reseau-sur-vm-linux.html",
    "title": "Configuration réseau sur VM Linux",
    "tags": ["linux", "network", "vmware"],
    "description": "",
    "content": " Lors de la manipulation de VM on peut avoir besoin de changer l\u0026rsquo;adresse IP ou de mettre la VM en DHCP, ce tuto explique comment faire.\nToutes les commandes se tapent connecté root :\n$ sudo -i Interface réseau Les interfaces réseau se configurent dans le fichier /etc/network/interfaces, c\u0026rsquo;est là que l\u0026rsquo;on décide si le réseau est en DHCP ou en statique.\nLa VM par défaut est configuré en mode DHCP, on va donc voir comment la mettre en statique.\nIP DHCP vers IP statique Déjà, allez vérifiez le nom de l\u0026rsquo;interface réseau :\n# ifconfig -a L\u0026rsquo;interface qui nous intéresse est l\u0026rsquo;interface ethx (eth0, eth1, \u0026hellip;).\nEnsuite ouvrez le fichier interfaces :\n# vi /etc/network/interfaces Recherchez la configuration de l\u0026rsquo;interface qui nous intéresse, ça devrait ressembler à ça :\nauto eth0 iface eth0 inet dhcp  Remplacez par la configuration statique :\nauto eth0 iface eth0 inet static address 172.16.139.29 netmask 255.255.255.0 gateway 172.16.139.2 network 172.16.139.0 broadcast 172.16.139.255 dns-nameservers 172.16.139.2 Par exemple. On remplacera le début de l\u0026rsquo;adresse par le bon sous-réseau. Par défaut la gateway sur une VM VMWare est l\u0026rsquo;adresse 2, cette adresse sert aussi de DNS.\nEnsuite, il faut mettre a jour le DNS en éditant le fichier resolv.conf :\n# vi /etc/resolv.conf Ajouter une ligne nameserver avec l\u0026rsquo;adresse du serveur DNS 172.16.139.2.\nPuis, si vous êtes sur de ne pas avoir besoin de repasser en DHCP, vous pouvez supprimer le paquet dhcp-client :\n# apt-get remove dhcp-client Enfin, il faut relancer l\u0026rsquo;interface réseau pour que les nouveaux paramètres soient pris en compte :\n# /etc/init.d/networking restart Configuration Proxy apt Pour configurer le proxy pour apt, c\u0026rsquo;est dans le fichier /etc/apt/apt.conf, il faut ajouter la ligne :\nAcquire::http::Proxy \u0026#34;http://proxy.tl.internal:3128/\u0026#34;; Liens  http://www.howtogeek.com/howto/ubuntu/change-ubuntu-server-from-dhcp-to-a-static-ip-address/   ", 
    "breadcrumb": " > linux > network > configuration-reseau-sur-vm-linux.html"
},
{
    
    "uri": "/serveurs/oracle/connaitre-lencodage-de-sa-base.html",
    "title": "Connaître l’encodage de sa base",
    "tags": ["server", "oracle", "database", "charset"],
    "description": "",
    "content": "Il peut être utile de vérifier l\u0026rsquo;encodage de sa base Oracle, notamment en cas de problèmes de montage d\u0026rsquo;un dump. Pour cela, il existe un moyen très simple de le faire, à l\u0026rsquo;aide de la requête SQL suivante :\nSELECT value$ FROM sys.props$ WHERE name = \u0026#39;NLS_CHARACTERSET\u0026#39;; On peut également exécuter cette autre requête SQL, qui remonte tous les paramètres de la base :\nSELECT * FROM NLS_DATABASE_PARAMETERS;", 
    "breadcrumb": " > serveurs > oracle > connaitre-lencodage-de-sa-base.html"
},
{
    
    "uri": "/development/spring/secure-connection-and-reverse-proxy.html",
    "title": "Connection securisé et reverse proxy",
    "tags": ["development", "spring", "java", "https", "security", "proxy"],
    "description": "",
    "content": "Dans le cadre d’un projet, j’ai une configuration Spring Secure tel que :\n... http.requiresChannel() .antMatchers(\u0026#34;/client/\u0026#34;).requiresSecure() .antMatchers(\u0026#34;/fr/client/\u0026#34;).requiresSecure() .antMatchers(\u0026#34;/es/client/\u0026#34;).requiresSecure() .antMatchers(\u0026#34;/en/client/\u0026#34;).requiresSecure() ... Ce qui provoque un redirect vers https si l’on tente d’accéder à l’une de ces URL en http.\nLe problème est lorsque que je mets un reverse proxy NginX devant, c’est lui qui s’occupe du https, il redirige vers mes routes Spring en http. Spring ne permet de configurer qu’un seul port de serveur qui peut faire http/https. Mais tous mes certif ne sont pas configurés et j’ai pas forcément envie de faire du https entre NginX et Spring.\nDu coup, pour que Spring détecte la connexion comme sécurisé et ne fasse pas une redirection infinie entre lui et le reverse proxy, il faut que la configuration NginX ressemble à ça :\nserver { listen 443 ssl; server_name monsite.fr ssl on; ssl_certificate /etc/ssl/monsite.fr.crt; ssl_certificate_key /etc/ssl/monsite.fr.key; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_ciphers HIGH:!aNULL:!MD5; location / { proxy_set_header Host $host; proxy_set_header X-Is-Secure true; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-Host $host; proxy_set_header X-Forwarded-Port 443; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto https; proxy_pass http://localhost:9003/; } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } } L’important étant les headers X-Forwarded.\n", 
    "breadcrumb": " > development > spring > secure-connection-and-reverse-proxy.html"
},
{
    
    "uri": "/serveurs/jboss/connexion-jboss-sql-server.html",
    "title": "Connexion JBoss SQL-Server",
    "tags": ["server", "jboss", "sql", "database"],
    "description": "",
    "content": " La configuration se fait dans les fichiers *-ds.xml présent dans le répertoire deploy du JBoss. Le but de cette page est surtout de donner la configuration pour les driver SQL-Server 2005 qui présentent des performence et une stabilité meilleure sur les versions récentes de SQL-Server.\nDriver SQL-Server 2000 C\u0026rsquo;est la configuration par défaut à l\u0026rsquo;installation de CCS.\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;datasources\u0026gt; \u0026lt;local-tx-datasource\u0026gt; \u0026lt;jndi-name\u0026gt;ccpDataSource\u0026lt;/jndi-name\u0026gt; \u0026lt;connection-url\u0026gt;jdbc:microsoft:sqlserver://127.0.0.1:1433;DatabaseName=CCS44Gb;SelectMethod=cursor\u0026lt;/connection-url\u0026gt; \u0026lt;driver-class\u0026gt;com.microsoft.jdbc.sqlserver.SQLServerDriver\u0026lt;/driver-class\u0026gt; \u0026lt;user-name\u0026gt;sa\u0026lt;/user-name\u0026gt; \u0026lt;password\u0026gt;sa\u0026lt;/password\u0026gt; \u0026lt;!-- sql to call on an existing pooled connection when it is obtained from pool --\u0026gt; \u0026lt;check-valid-connection-sql\u0026gt;select \u0026#39;x\u0026#39;\u0026lt;/check-valid-connection-sql\u0026gt; \u0026lt;/local-tx-datasource\u0026gt; \u0026lt;/datasources\u0026gt; Les librairies de ce driver se composent de trois fichiers jar dans le répertoire deploy : * msbase.jar * msutil.jar * mssqlserver.jar\nDriver SQL-Server 2005 Pour les serveurs 2005, les drivers 2000 fonctionnent mais il est conseillé d\u0026rsquo;utiliser la version 2005. Ces drivers ont été ré-écrit complètement par Micro$oft et donne des performances vu qu\u0026rsquo;ils exploitent mieux les fonctionnalités de SQL-Server 2005.\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;datasources\u0026gt; \u0026lt;local-tx-datasource\u0026gt; \u0026lt;jndi-name\u0026gt;ccpDataSource\u0026lt;/jndi-name\u0026gt; \u0026lt;connection-url\u0026gt;jdbc:sqlserver://localhost:1433;DatabaseName=CCS44Gb;SelectMethod=cursor\u0026lt;/connection-url\u0026gt; \u0026lt;driver-class\u0026gt;com.microsoft.sqlserver.jdbc.SQLServerDriver\u0026lt;/driver-class\u0026gt; \u0026lt;user-name\u0026gt;sa\u0026lt;/user-name\u0026gt; \u0026lt;password\u0026gt;sa\u0026lt;/password\u0026gt; \u0026lt;!-- sql to call on an existing pooled connection when it is obtained from pool --\u0026gt; \u0026lt;check-valid-connection-sql\u0026gt;select \u0026#39;x\u0026#39;\u0026lt;/check-valid-connection-sql\u0026gt; \u0026lt;/local-tx-datasource\u0026gt; \u0026lt;/datasources\u0026gt; Cette version n\u0026rsquo;est constituée que d\u0026rsquo;un seul fichier à mettre dans le répertoire deploy, sqljdbc.jar.\n", 
    "breadcrumb": " > serveurs > jboss > connexion-jboss-sql-server.html"
},
{
    
    "uri": "/development/java/connexion-ldap-exchange.html",
    "title": "Connexion LDAP Exchange",
    "tags": ["development", "java", "ldap", "exchange"],
    "description": "",
    "content": " L\u0026rsquo;idée est d\u0026rsquo;expliquer comment, depuis Java, se connecter à un serveur LDAP. Mais pas n\u0026rsquo;importe lequel, un serveur LDAP Exchange. Ben oui, Microsoft à beau se standardisé un minimum, de là à faire tout comme les autres, faudrait pas pousser !\nImplémentation via JNDI Plusieurs type d\u0026rsquo;implémentation existent pour se connecter à un serveur LDAP. La plus utilisé et la plus \u0026ldquo;standard\u0026rdquo; est de se connecter en utilisant les interfaces JNDI. C\u0026rsquo;est donc là dessus qu\u0026rsquo;on va se pencher.\nCréation du tableau de paramètres La première chose à faire c\u0026rsquo;est donc, comme pour un accès JNDI normal, de créer le table de paramètres. Mais dans notre cas, on va avoir quelques nouveau paramètres concernant le LDAP.\nHashtable\u0026lt;String, String\u0026gt; env = new Hashtable\u0026lt;String, String\u0026gt;(); env.put(Context.INITIAL_CONTEXT_FACTORY, \u0026#34;com.sun.jndi.ldap.LdapCtxFactory\u0026#34;); env.put(Context.PROVIDER_URL, \u0026#34;ldap://centrale.springfield.com:389\u0026#34;); // Authenticate as user@ldapDomain and password password env.put(Context.SECURITY_AUTHENTICATION, \u0026#34;simple\u0026#34;); env.put(Context.SECURITY_PRINCIPAL, \u0026#34;hsimpson@springfield\u0026#34;); env.put(Context.SECURITY_CREDENTIALS, password); SECURITY_AUTHENTICATION : Il s\u0026rsquo;agit de spécifier le mécanisme d\u0026rsquo;authentification. Trois sont possible (none, simple, sasl_mech). None est pour une authentification anonyme et sasl est pour une authentification par certificat (si j\u0026rsquo;ai bien compris). Dans notre cas, c\u0026rsquo;est simple qui nous intéresse puisqu\u0026rsquo;il s\u0026rsquo;agit de s\u0026rsquo;authentifier par utilisateur et mot de passe.\nSECURITY_PRINCIPAL : C\u0026rsquo;est ici que l\u0026rsquo;on spécifie l\u0026rsquo;utilisateur à connecter. Et c\u0026rsquo;est là aussi que ça va varier entre Microsoft et le reste du monde. Pour un LDAP classique (type [[https://www.opends.org/]]) le user ressemble un peu à ça : \u0026ldquo;uid=hsimpson, ou=people, o=springfield\u0026rdquo;. Mais chez Microsoft, le user DOIT être écrit sous la forme : \u0026ldquo;hsimpson@springfield\u0026rdquo; et pas autrement.\nSECURITY_CREDENTIALS : C\u0026rsquo;est le mot de passe de l\u0026rsquo;utilisateur, rien de compliqué ici.\nCréation du contexte initial Donc une fois les paramètres renseigné, on va créer le contexte initial; C\u0026rsquo;est cette création qui va permettre de déterminer si les le couple utilisateur/mot de passe est valide ou non.\n// Create the initial context try { ctx = new InitialDirContext(env); LOGGER.debug(\u0026#34;User \u0026#34;+user+\u0026#34; logged in LDAP ...\u0026#34;); } catch (AuthenticationException e) { LOGGER.debug(\u0026#34;User and password for \u0026#34;+user+\u0026#34; invalid !\u0026#34;); } Si on passe cette étape, l\u0026rsquo;utilisateur et le mot de passe sont valide. En cas d\u0026rsquo;authentification incorrecte, une erreur AuthenticationException est levée.\nRécupération des informations du compte Voilà, maintenant qu\u0026rsquo;on sait que l\u0026rsquo;utilisateur est valable, on pourrait avoir envie de récupérer des informations sur cet utilisateur afin de pouvoir le créer dans l\u0026rsquo;application que l\u0026rsquo;on intègre (CCS pour ne pas la citer) ou pour synchroniser les données de cet utilisateur.\nPour cela, on va utiliser la fonction find :\nSearchControls constraints = new SearchControls(); constraints.setSearchScope(SearchControls.SUBTREE_SCOPE); String[] attrIDs = { \u0026#34;distinguishedName\u0026#34;, \u0026#34;sn\u0026#34;, \u0026#34;givenname\u0026#34;, \u0026#34;mail\u0026#34;, \u0026#34;telephonenumber\u0026#34; }; constraints.setReturningAttributes(attrIDs); NamingEnumeration\u0026lt;SearchResult\u0026gt; answer = ctx.search(ldapRootContext, \u0026#34;sAMAccountName=\u0026#34;+ user, constraints); if (!answer.hasMore()) { LOGGER.warn(\u0026#34;No user informations found in LDAP for \u0026#34;+user); if (!userExist) throw new LdapLoginException(\u0026#34;Cannot create user in CCS whitout LDAP informations !\u0026#34;); } Attributes attrs = ((SearchResult) answer.next()).getAttributes(); if (LOGGER.isDebugEnabled()) { LOGGER.debug(attrs.get(\u0026#34;distinguishedName\u0026#34;)); LOGGER.debug(attrs.get(\u0026#34;givenname\u0026#34;)); LOGGER.debug(attrs.get(\u0026#34;sn\u0026#34;)); LOGGER.debug(attrs.get(\u0026#34;mail\u0026#34;)); LOGGER.debug(attrs.get(\u0026#34;telephonenumber\u0026#34;)); } } C\u0026rsquo;est encore une particularité de Microsoft, sur un serveur LDAP normal, la recherche ne se fait pas avec ces paramètres là. sAMAccountName représente l\u0026rsquo;uid chez Microsoft en fait.\nAprès, de ce que j\u0026rsquo;ai peu lire, chaque LDAP est différent et on peut y mettre un peu tout et n\u0026rsquo;importe quoi ! Donc faut toujours plus ou moins s\u0026rsquo;adapter au LDAP que l\u0026rsquo;on utilise. Voici un liste non exhaustive des attributs standard que l\u0026rsquo;on retrouve en général dans un LDAP :  oOrganization (Société) ouOrganizational unit (Service) cnCommon name (Nom courant genre \u0026ldquo;John Smith) snSurname (Nom de famille) givennameFirst name (Prénom) uidUserid (Chez Microsoft c\u0026rsquo;est un numéro) dnDistinguished name (Nom LDAP avec tout les groupes) mailEmail address (Adresse mail) \nConclusion Voilà, arrivé ici, c\u0026rsquo;est plus trop compliqué, à partir de ces infos on peut créer ou mettre à jour un utilisateur de notre appli.\nGardez quand même en tête que le code ci-dessus permet de se connecter au serveur Exchange de Cameleon Software mais qu\u0026rsquo;il peut y avoir besoin de quelques ré-ajustement pour un autre serveur LDAP.\nLiens Quelques liens qui m\u0026rsquo;ont bien aidé, surtout le premier qui se trouve être le seul bout de code que j\u0026rsquo;ai trouvé qui fonctionne sur un serveur Exchange !\n http://www.myhomepageindia.com/index.php/2009/09/24/retrieve-basic-user-attributes-from-active-directory-using-ldap-in-java.html http://download.oracle.com/javase/tutorial/jndi/ldap/index.html http://www.javaworld.com/javaworld/jw-03-2000/jw-0324-ldap.html  ", 
    "breadcrumb": " > development > java > connexion-ldap-exchange.html"
},
{
    
    "uri": "/linux/divers/connexion-vpn-rackspace-depuis-linux.html",
    "title": "Connexion VPN Rackspace depuis Linux",
    "tags": ["linux", "misc", "vpn", "bash"],
    "description": "",
    "content": " Sous Linux il n\u0026rsquo;existe pas de client VPN Cisco officiel mais Rackspace préconise l\u0026rsquo;utilisation de VPNC qui fonctionne très bien.\nInstallation du package Sous Debian \u0026amp; Co c\u0026rsquo;est simple :\n$ sudo apt-get install vpnc Configuration Commencez par créer un fichier dans /etc/vpnc\n$ sudo vi /etc/vpnc/rackspace.conf Ajoutez y les lignes suivante :\nIPSec gateway 184.106.120.83 IPSec ID CameleonAdmins IPSec secret {password group ? voir avec RSE} Xauth username fcombes@cameleon-software.com  Activer la connexion Dans une console :\n$ sudo vpnc rackspace Il vous faut alors renseigner votre password.\nDésactiver la connexion $ sudo vpnc-disconnect ", 
    "breadcrumb": " > linux > divers > connexion-vpn-rackspace-depuis-linux.html"
},
{
    
    "uri": "/tags/console.html",
    "title": "Console",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > console.html"
},
{
    
    "uri": "/tags/convention.html",
    "title": "Convention",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > convention.html"
},
{
    
    "uri": "/outils/git/convention-de-message-de-commit.html",
    "title": "Convention de message de commit",
    "tags": ["outils", "git", "convention", "naming"],
    "description": "",
    "content": "  The contens of this page are partly based on the angular commit messages document.\n Objectif Le message de commit est ce qui décrit votre contribution. C\u0026rsquo;est pourquoi le but est de décrire ce que le commit apporte au projet;\nL\u0026rsquo;entête doit être aussi explicite que possible car elle est toujours lu avec les autres message de commit.\nLe corps doit fournir les informations nécessaires pour ceux qui souhaitent comprendre le commit.\nLe pied peut contenir des références à des documents externes (incidents résolut, \u0026hellip;) aussi bien que des notes sur les changements structurels.\nCela s\u0026rsquo;applique à tout les type de projet.\nFormat Forme courte (uniquement la ligne de sujet) \u0026lt;type\u0026gt;(\u0026lt;scope\u0026gt;): \u0026lt;subject\u0026gt;  Forme longue (avec le corps) \u0026lt;type\u0026gt;(\u0026lt;scope\u0026gt;): \u0026lt;subject\u0026gt; \u0026lt;BLANK LINE\u0026gt; \u0026lt;body\u0026gt; \u0026lt;BLANK LINE\u0026gt; \u0026lt;footer\u0026gt;  La première ligne de doit pas dépasser les 70 caractères. La ligne suivante est toujours une ligne blanche et les lignes suivante doivent se limiter à 80 caractères! Cela rend le message plus facile à lire sur github comme sur la plus part des autres outils git.\nLigne de sujet Le sujet contient une description succincte des changements.\n\u0026lt;type\u0026gt; autorisé  feat (fonctionnalité) fix (correction de bug) docs (documentation) style (formatage, oublie divers, \u0026hellip;) refactor test (lors d\u0026rsquo;ajout de test manquant) chore (maintenance) improve (amélioration)  \u0026lt;scope\u0026gt; autorisé Le scope peut spécifier l\u0026rsquo;emplacement des changements du commit. Par exemple dans le camunda-modeler project cela peut être \u0026ldquo;import\u0026rdquo;, \u0026ldquo;export\u0026rdquo;, \u0026ldquo;label\u0026rdquo;, \u0026hellip;\nTexte du \u0026lt;subject\u0026gt;  Utiliser l\u0026rsquo;impératif présent: change et pas changed ou changes ou changing Ne pas mettre la première lettre en majuscule ne pas mettre de \u0026lsquo;.\u0026rsquo; à la fin  Corps du message  Comme pour le sujet, utiliser l\u0026rsquo;impératif présent Ajoutez les motivations du changement et comparez avec la version précédente  Pied du message Changements \u0026ldquo;cassant\u0026rdquo; Tous les changement cassants doivent être mentionné dans le pied avec la description du changement, la justification et la procédure de migration.\n BREAKING CHANGE: Id editing feature temporarily removed\nAs a work around, change the id in XML using replace all or friends\n Incidents référencés Bug fermé / feature requests / problèmes doivent être listé sur une ligne séparé dans le pied préfixé du mot clé \u0026ldquo;Closes\u0026rdquo; comme :\n Closes #234\n or in case of multiple issues:\n Closes #123, #245, #992\n Plus sur les bonnes pratiques de commit  http://365git.tumblr.com/post/3308646748/writing-git-commit-messages http://tbaggery.com/2008/04/19/a-note-about-git-commit-messages.html  FAQ pour les Geeks Pourquoi utiliser l\u0026rsquo;impératif présent dans les messages ? Faire de log de commit plus lisible. Voyez le travail que vous faite pendant le commit comme un travail sur le un problème dont le commit en est la solution. Maintenant comment le commit est une solution au problème.\nExemple: Vous écrivez un test pour la fonction #foo. Vous commitez le test. Vous utilisez le message de commit add test for #foo. Pourquoi ? Car c\u0026rsquo;est ce que le commit résout.\nComment caractériser les commit qui résultent directement d\u0026rsquo;un merge ? Utilisez chore(merge): \u0026lt;what\u0026gt;.\nJe veux commiter un micro-changement, que dois je faire ? Demandez vous pourquoi c\u0026rsquo;est un micro-changement. Utilisez feat = docs, style or chore selon le changement. SVP regarder la prochaine question si vous commiter un travail en cours.\nJe veux committer un travail en cours. Que dois je faire? Ne le faite pas ou faite le sur une branche non publique (pas sur develop ni sur master).\nQuand vous avez finir le travail committer avec un message cohérent et poussez sur la branche publique.\n", 
    "breadcrumb": " > outils > git > convention-de-message-de-commit.html"
},
{
    
    "uri": "/development/java/convertir-un-date-entre-timezone-avec-jodatime.html",
    "title": "Convertir un date entre Timezone avec Jodatime",
    "tags": ["development", "java", "date", "timezone"],
    "description": "",
    "content": "DateTime initialTime = new DateTime(timestamp.getTime()) .withZoneRetainFields(vpnNeType.getTimeZone().get()) .withZone(targetTimeZone); dataDate = initialTime.toLocalDateTime().toDate(); Le new Datetime() crée un object Datetime à partir d\u0026rsquo;un timestamp mais en partant du principe que la TZ est la default. withZoneRetainFields change la TZ sans changer la date, withZone change la TZ et converti la date. .toLocalDateTime().toDate() re-converti en date en tenant compte de la TZ.\n", 
    "breadcrumb": " > development > java > convertir-un-date-entre-timezone-avec-jodatime.html"
},
{
    
    "uri": "/tags/cook.html",
    "title": "Cook",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > cook.html"
},
{
    
    "uri": "/tags/cpl.html",
    "title": "Cpl",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > cpl.html"
},
{
    
    "uri": "/tags/crash.html",
    "title": "Crash",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > crash.html"
},
{
    
    "uri": "/outils/eclipse/crash-d-eclipse.html",
    "title": "Crash d’Eclipse",
    "tags": ["outils", "eclipse", "crash"],
    "description": "",
    "content": "J\u0026rsquo;ai eu a faire face à un problème avec Eclipse qui, suite à une mise à jour Debian (je pense que c\u0026rsquo;est ça mais sans certitude) s\u0026rsquo;est mis à planter assez facilement et notamment à l\u0026rsquo;affichage des infos-bulles. Après pas mal de recherche il semble que la solution soit de rajouter :\n-Dorg.eclipse.swt.browser.DefaultType=mozilla  Dans le fichier eclipse.ini.\n", 
    "breadcrumb": " > outils > eclipse > crash-d-eclipse.html"
},
{
    
    "uri": "/linux/divers/creer-une-cle-usb-dinstall-debian.html",
    "title": "Creer une clé USB d’install Debian",
    "tags": ["linux", "misc", "usb", "debian"],
    "description": "",
    "content": "C\u0026rsquo;est pas bien compliqué mais on cherche longtemps la bonne méthode dans la multitude de conneries du web.\nDéjà, oublier l\u0026rsquo;idée de faire ça avec Windows, c\u0026rsquo;est de la merde et ça marche pas à moitié !\nEnsuite, et c\u0026rsquo;est d\u0026rsquo;autant plus valable avec les PC de dernière génération et leur boot UEFI, prendre un image Debian \u0026ldquo;testing\u0026rdquo; plutôt qu\u0026rsquo;une stable. La stable ne possède pas les fichier nécessaire pour l\u0026rsquo;EFI alors qu\u0026rsquo;avec l\u0026rsquo;ISO testing ça se fait tout seul.\nDonc déjà pour télécharger l\u0026rsquo;ISO le mieux est d\u0026rsquo;utiliser Jigdo :\n# apt-get install jigdo-file $ jigdo-lite http://cdimage.debian.org/cdimage/weekly-builds/amd64/jigdo-cd/debian-testing-amd64-CD-1.jigdo Le nom de l\u0026rsquo;image à peut-être changé !\nUne fois qu\u0026rsquo;on a l\u0026rsquo;image, c\u0026rsquo;est simple comme bonjour, pas besoin de logiciel spécialisé, on fait juste :\n# umount /dev/sde1 # cat debian-testing-amd64-CD-1.iso \u0026gt; /dev/sde Attention au nom du périphérique ! Allez pas écraser des partitions utilisé ! Dans mon cas c\u0026rsquo;est sde. Il faut faire attention aussi que toute les partitions du périphérique soient bien démonté, sinon la commande cat s\u0026rsquo;arrète sans le moindre message d\u0026rsquo;erreur et ça fonctionnera pas !\nEnsuite avec fdisk vous pouvez vérifier le résultat :\n# fdisk -l /dev/sde Attention : identifiant de table de partitions GPT (GUID) détecté sur « /dev/sde » ! L'utilitaire sfdisk ne prend pas GPT en charge. Utilisez GNU Parted. Disque /dev/sde : 2019 Mo, 2019557376 octets 255 têtes, 63 secteurs/piste, 245 cylindres, total 3944448 secteurs Unités = secteurs de 1 * 512 = 512 octets Taille de secteur (logique / physique) : 512 octets / 512 octets taille d'E/S (minimale / optimale) : 512 octets / 512 octets Identifiant de disque : 0x6d433c0a Périphérique Amorce Début Fin Blocs Id Système /dev/sde1 * 0 1327103 663552 0 Vide /dev/sde2 26200 27031 416 ef EFI (FAT-12/16/32)  Comme on peut le voir, ca a automatiquement créé la partition EFI \u0026hellip;\n", 
    "breadcrumb": " > linux > divers > creer-une-cle-usb-dinstall-debian.html"
},
{
    
    "uri": "/linux/shell/crypter-tar.gz-avec-mot-de-passe.html",
    "title": "Crypter tar.gz avec mot de passe",
    "tags": ["linux", "shell", "security", "crypting"],
    "description": "",
    "content": "Il est très simple de crypter un fichier tar.gz et de lui assigner un mot de passe pour par exemple l\u0026rsquo;envoyer sur un cloud public :\ntar cz folder_to_encrypt | \\ \topenssl enc -aes-256-cbc -e \u0026gt; out.tar.gz.enc Le mot de passe est alors demandé.\nPour decrypter/décompresser :\nopenssl aes-256-cbc -d -in out.tar.gz.enc | tar xz Et voilà\n", 
    "breadcrumb": " > linux > shell > crypter-tar.gz-avec-mot-de-passe.html"
},
{
    
    "uri": "/tags/crypting.html",
    "title": "Crypting",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > crypting.html"
},
{
    
    "uri": "/tags/crypto.html",
    "title": "Crypto",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > crypto.html"
},
{
    
    "uri": "/linux/divers/creer-un-conteneur-chiffre.html",
    "title": "Créer un conteneur chiffré",
    "tags": ["linux", "misc", "security"],
    "description": "",
    "content": " L\u0026rsquo;idée ici est de créer un fichier n\u0026rsquo;importe où sur le disque. Se fichier pourra être monté comme une partition et contiendra des données chiffrées.\nCréation du fichier Attention, c\u0026rsquo;est ici que l\u0026rsquo;on détermine la taille du fichier et c\u0026rsquo;est pas facile à changer par la suite\ndd if=/dev/urandom of=testfile bs=1M count=5000  Ici on a un fichier de 5 Go.\nCréation de la partition On va ensuite monter ce ficheir comme une partition grâce à losetup\n# losetup -f /dev/loop0 # losetup /dev/loop0 testfile Attention, la commande suivante efface tout ce qui se trouve dans le fichier :\n# cryptsetup luksFormat /dev/loop0 Répondre YES puis saisir le mot de passe du volume.\n# cryptsetup luksOpen /dev/loop0 testfs # mkfs.ext4 /dev/mapper/testfs A ce stade, la partition est créée. Il ne reste plus qu\u0026rsquo;à la monter avec :\n# mount -o rw /dev/mapper/testfs /media/testfs Après le redémarrage Le conteneur ne reste monté que le temps d\u0026rsquo;un démarrage si vous ne le démontez pas entre temps. Après un redémarrage, pour monté/démonter un conteneur voici un script qui permet de le faire en automatique :\n#!/usr/bin/env bash ################################### # Montage de containeur chiffré # #\t$1 = fichier containeur à monter #  if [[ $(/usr/bin/id -u) -ne 0 ]]; then echo \u0026#34;Doit être lancé en tant que ROOT\u0026#34; exit fi mntrep=$1 mntfile=${mntrep##*/} mntrep=\u0026#34;/media/$mntfile\u0026#34; mount|grep -q $mntfile if [ $? == 1 ] then loopdev=$(losetup -f) losetup $loopdev $1 cryptsetup luksOpen $loopdev $mntfile mkdir $mntrep mount -o rw /dev/mapper/$mntfile $mntrep chmod 777 $mntrep echo \u0026#34;Partition cryptfs monté !\u0026#34; else umount $mntrep rm -Rf $mntrep cryptsetup luksClose /dev/mapper/$mntfile output=$(losetup -a | grep $mntfile) for LINE in ${output} ; do echo ${LINE} losetupres=${LINE} loopdev=${losetupres%%:*} losetup -d $loopdev done echo \u0026#34;Partition cryptfs démonté !\u0026#34; fi Trucs \u0026amp; Astuces  Une bonne idée est ensuite de cacher le fichier dans un répertoire \u0026hellip; caché, exemple \u0026ldquo;.trucmachinquiarienavoir\u0026rdquo; Le fichier peut être renommé autant de fois que nécessaire  ", 
    "breadcrumb": " > linux > divers > creer-un-conteneur-chiffre.html"
},
{
    
    "uri": "/linux/divers/creer-une-partition-chiffre-sur-une-cle-usb.html",
    "title": "Créer une partition chiffré sur une clé USB",
    "tags": ["linux", "misc", "usb", "security"],
    "description": "",
    "content": "  Do not remove this line (it will not be displayed) {:toc}  L\u0026rsquo;idée est de créer sur une clé USB une partition chiffré. Mais pas seulement, afin de rendre cette partition lisible sur Windows comme sur Linux et de pas avoir un clé vide sous Windows, ce qui semblerait louche, il faut avoir deux partition, une en clair et une en crypté. C\u0026rsquo;est pas si simple que ça en a l\u0026rsquo;air.\nCe tuto est plus ou moins un copie des tutos en liens que je met là au cas où le site disparaîtrait. J\u0026rsquo;ai quand même modifié quelques truc qui ne fonctionnaient pas à cause de traductions \u0026ldquo;à l\u0026rsquo;arrache\u0026rdquo;.\nIntroduction Au cas où vous perdriez votre clé USB, toutes les données stockées dessus seront perdues et ce qui est plus important, elles seront très probablement dans les mains d\u0026rsquo;une autre personne qui aura alors accès à vos informations privées et utiliser ces informations si elle le juge approprié. C\u0026rsquo;est l\u0026rsquo;une des craintes de nombreux utilisateurs de clé USB. Une solution qui peut être facilement appliquée, consiste à ne pas stocker toute information privée sur une clé USB, mais cela va diminuer l\u0026rsquo;atout principal de votre clé USB à un strict minimum limité à toutes les données non-privée depuis qu\u0026rsquo;elles peuvent être presque toujours téléchargées n\u0026rsquo;importe quand et n\u0026rsquo;importe où à partir d\u0026rsquo;Internet. Une autre solution est de chiffrer (crypter) votre clé USB, celle-ci sera accessible uniquement aux utilisateurs qui possèdent le mot de passe correct qui permettra de déchiffrer les données.\nBien que le cryptage d\u0026rsquo;une clé USB semble être la meilleure solution et la plus facile, il faut indiquer qu\u0026rsquo;elle contient de nombreux inconvénients. Un inconvénient est que le décryptage de la clé USB doit être fait en utilisant un système Linux avec la version du noyau 2.6 ou supérieur, qui a un module « dm_crypt » chargé dans le noyau en cours d\u0026rsquo;exécution. Il existe quand même un moyen de déchiffrer une clé chiffré comme on va le faire depuis Windows grâce à un logiciel gratuit que l\u0026rsquo;on placera sur la partition non crypté de la clé.\nNOTE: Toutes les données sur votre clé USB seront détruits de manière Sauvegardez votre clé USB avant de continuer. Remplacer /dev/sdX avec le nom de fichier de votre périphérique bloc USB.\nNOTE: Toutes les manip sont effectuée en super-utilisateur. Vérifiez toutes les commandes à deux fois avant de les valider, elles sont irréversible, si vous vous trompé de partition c\u0026rsquo;est mort !\nPartitionnement d\u0026rsquo;une clé USB {: .center-image }\nCommençons par savoir sur quel périphérique est votre clé :\nmount sysfs on /sys type sysfs (rw,nosuid,nodev,noexec,relatime) proc on /proc type proc (rw,nosuid,nodev,noexec,relatime) udev on /dev type devtmpfs (rw,relatime,size=10240k,nr_inodes=1014025,mode=755) devpts on /dev/pts type devpts (rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000) tmpfs on /run type tmpfs (rw,nosuid,noexec,relatime,size=812588k,mode=755) /dev/disk/by-uuid/60b1acc3-2986-4799-b2ac-c6f61f6b3ce2 on / type ext4 (rw,relatime,errors=remount-ro,user_xattr,barrier=1,data=ordered) tmpfs on /run/lock type tmpfs (rw,nosuid,nodev,noexec,relatime,size=5120k) tmpfs on /run/shm type tmpfs (rw,nosuid,nodev,noexec,relatime,size=3287720k) /dev/sdc1 on /boot/efi type vfat (rw,relatime,fmask=0022,dmask=0022,codepage=cp437,iocharset=utf8,shortname=mixed,errors=remount-ro) /dev/md0 on /home type ext4 (rw,relatime,errors=remount-ro,user_xattr,barrier=1,stripe=128,data=ordered) rpc_pipefs on /var/lib/nfs/rpc_pipefs type rpc_pipefs (rw,relatime) binfmt_misc on /proc/sys/fs/binfmt_misc type binfmt_misc (rw,nosuid,nodev,noexec,relatime) fusectl on /sys/fs/fuse/connections type fusectl (rw,relatime) vmware-vmblock on /run/vmblock-fuse type fuse.vmware-vmblock (rw,nosuid,nodev,relatime,user_id=0,group_id=0,default_permissions,allow_other) /dev/sde1 on /media/NOUVEAU NOM type vfat (rw,nosuid,nodev,relatime,uid=1000,gid=1000,fmask=0022,dmask=0077,codepage=cp437,iocharset=utf8,shortname=mixed,showexec,utf8,flush,errors=remount-ro,uhelper=udisks) Ici on voit la clé sur la dernière ligne, en vfat. Notre périphérique est donc /dev/sde.\nOn peut maintenant partitionner notre clé (as root) :\ncfdisk /dev/sde Dans la capture ci-à-droite, on peut voir comment doivent se retrouver les partitions. On en crée 2, la première en principale sera la partition non crypté, la deuxième partition logique, sera la partition crypté. Dans ce cas, chaque partition fait 1 Go. Il faut \u0026ldquo;Maximiser\u0026rdquo; les partitions pour que la partition prenne réellement 1Go.\nPensez bien à enregistrer les modifications dans la table de partition ! Déconnectez la clé puis la rebrancher pour être sur que le système a bien pris en compte les nouvelles partitions.\nEcrire des données aléatoires Pour éviter des attaques par motifs du chiffrement, il est conseillé d\u0026rsquo;écrire des données aléatoires sur une partition avant de procéder à un chiffrement. La commande suivante \u0026ldquo;dd\u0026rdquo; peut être utilisé pour écrire ces données à votre partition, cela peut prendre un certain temps. Celui-ci dépend de l\u0026rsquo;entropie générée par votre système. Dans notre cas, on voit que la partition qui va être chiffré est la /dev/sde5 :\ndd bs=4K if=/dev/urandom of=/dev/sde5  Chiffrement de partition Maintenant il est temps de chiffrer la partition nouvellement créée. À cette fin, nous allons utiliser l\u0026rsquo;outil cryptsetup. Si la commande cryptsetup n\u0026rsquo;est pas disponible sur votre système, assurez-vous que le paquet cryptsetup soit installé.\napt-get install cryptsetup La commande suivante va chiffrer /dev/sdX1 partitionner avec 256-bit AES XTS algorithme. Cet algorithme est disponible sur n’importe quel noyau de version supérieur à 2.6.24.\ncryptsetup -h sha256 -c aes-xts-plain -s 256 luksFormat /dev/sde5  ATTENTION! ======== Cela va écraser les données sur /dev/sdX1 de façon irrévocable. Etes-vous sûr? (Majuscules Type oui): OUI Enter LUKS passphrase: Vérifiez mot de passe: Command succès.  Prenez bien garde à entrer un mot de passe dont vous vous souviendrez, c\u0026rsquo;est pas récupérable si vous l\u0026rsquo;oubliez !\nMontage USB partition et le déchiffrement Dans l\u0026rsquo;étape suivante, nous allons définir le nom de notre partition chiffrée pour être reconnu par le mappeur de périphériques du système. Vous pouvez choisir n\u0026rsquo;importe quel nom. Par exemple, nous pouvons utiliser le nom de «private»:\ncryptsetup luksOpen /dev/sde5 private Enter LUKS passphrase: sous la touche 0 déverrouillé. Command succès. Après l\u0026rsquo;exécution de cette commande, votre partition chiffrée sera disponible pour votre système, comme /dev/mapper/private, //private// étant le nom que vous avez donnez à luksOpen. Nous pouvons maintenant créer le système de fichiers et monter la partition dans /dev/mapper/private :\nmkfs.ext4 /dev/mapper/private Optionnel NOTE: Je laisse cette partie à titre indicatif mais elle n\u0026rsquo;est pas nécessaire, on peut passer direct au grand paragraphe suivant.\nCréer un point de montage et monter une partition:\nmkdir /media/private mount /dev/mapper/private /media/private chown-R myusername.myusername /media/private Maintenant, votre partition chiffrée est disponible dans /media/private. Si vous ne souhaitez pas d\u0026rsquo;avoir un accès à la partition cryptée sur votre clé USB de plus vous devez d\u0026rsquo;abord le démonter du système, puis utiliser la commande cryptsetup pour fermer la protection connecté.\numount /media/private cryptsetup luksClose /dev/mapper/private Utilisation de la clé Linux C\u0026rsquo;est le plus simple sous Linux, la plus part des système récent détectent automatiquement les partitions LUKS et demandent la passphrase au branchement.\nWindows Sous Windows il ne voit même pas la partition crypté.\nLa première chose à faire est de formater la partition normale en FAT 32.\nRemarque : Si on a une grosse clé et que l\u0026rsquo;on veut des gros fichiers, on peut faire en NTFS ou en [[exFAT|https://mabouali.wordpress.com/2012/10/27/exfat-for-your-usb-flash-drives/]].\nEnsuite, pour pouvoir lire la partition sur Windows, on peut utiliser [[FreeOTFE|http://www.freeotfe.org/download.html]]. La bonne idée c\u0026rsquo;est d\u0026rsquo;installer la version \u0026ldquo;portable\u0026rdquo; sur la partition non chiffré ce qui permet alors de lire la partition non chiffré. Faite quand même attention que le mot de passe ne traîne pas sur la partition non chiffré !!\nLiens  en \u0026rarr; [[http://www.linuxconfig.org/usb-stick-encryption-using-linux]] fr \u0026rarr; [[http://monblog.system-linux.net/blog/2011/03/19/chiffrement-du-systeme-de-fichiers-dune-cle-usb/]]  ", 
    "breadcrumb": " > linux > divers > creer-une-partition-chiffre-sur-une-cle-usb.html"
},
{
    
    "uri": "/tags/css.html",
    "title": "Css",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > css.html"
},
{
    
    "uri": "/tags/cuisine.html",
    "title": "Cuisine",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > cuisine.html"
},
{
    
    "uri": "/tags/curl.html",
    "title": "Curl",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > curl.html"
},
{
    
    "uri": "/serveurs/neo4j/cypher-node-sans-label.html",
    "title": "Cypher: Nodes sans Label",
    "tags": ["server", "neo4j", "database"],
    "description": "",
    "content": "Pour lister (et supprimer) les Nodes n\u0026rsquo;aillant aucun label :\nMATCH (n) WHERE SIZE(LABELS(n)) = 0 DETACH DELETE n;", 
    "breadcrumb": " > serveurs > neo4j > cypher-node-sans-label.html"
},
{
    
    "uri": "/serveurs/neo4j/cypher-paths-sans-path-intermediaire.html",
    "title": "Cypher: Paths sans path intermédiaire",
    "tags": ["server", "neo4j", "database"],
    "description": "",
    "content": "Quand on demande un path a Neo4j il va nous donner avec les paths intermédiaire. Par exemple :\nMATCH (:Attribute {_type:\u0026#39;realm\u0026#39;,name:\u0026#39;iwan\u0026#39;})\u0026lt;-[:Attribute]-(s:Planet), path = (s)-[:MdxPath*0..10]-\u0026gt;(:Planet) RETURN nodes(path) va retourner :\n interface interface / application interface / application / cos cpe  Alors que ce qui m\u0026rsquo;intéresse c\u0026rsquo;est juste les deux derniers. Je ne veux pas les chemins intermédiaires.\nPour n\u0026rsquo;avoir que les paths complet, la solution est de demander les paths pour lesquels le dernier node n\u0026rsquo;a pas de lien vers une Planet (dans le cas de l\u0026rsquo;exemple).\nMATCH (:Attribute {_type:\u0026#39;realm\u0026#39;,name:\u0026#39;iwan\u0026#39;})\u0026lt;-[:Attribute]-(s:Planet), path = (s)-[:MdxPath*0..10]-\u0026gt;(p:Planet) WHERE NOT ((p)-[:MdxPath]-\u0026gt;(:Planet)) RETURN nodes(path)", 
    "breadcrumb": " > serveurs > neo4j > cypher-paths-sans-path-intermediaire.html"
},
{
    
    "uri": "/linux/network/dns-securise.html",
    "title": "DNS Securise",
    "tags": ["linux", "network", "dns", "security"],
    "description": "",
    "content": "L\u0026rsquo;objectif est de sécuriser les requêtes DNS en les cryptant. Sachant que Tor par défaut ne le fait pas. Il est possible de le configurer de façon a ce que les requêtes DNS passent elles aussi par Tor mais cela demande à changer le resolv.conf et a paramétrer Tor correctement. C\u0026rsquo;est pas forcément pratique même si c\u0026rsquo;est plus anonymisant que le cryptage du DNS vu que le serveur DNS ne sait pas qui lui a fait la requête.\n", 
    "breadcrumb": " > linux > network > dns-securise.html"
},
{
    
    "uri": "/tags/database.html",
    "title": "Database",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > database.html"
},
{
    
    "uri": "/tags/datapump.html",
    "title": "Datapump",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > datapump.html"
},
{
    
    "uri": "/tags/date.html",
    "title": "Date",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > date.html"
},
{
    
    "uri": "/tags/debian.html",
    "title": "Debian",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > debian.html"
},
{
    
    "uri": "/tags/debug.html",
    "title": "Debug",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > debug.html"
},
{
    
    "uri": "/linux/administration/demarrage-de-jboss-en-service.html",
    "title": "Demarrage de JBoss en service",
    "tags": ["linux", "sysadmin", "jboss", "service", "server"],
    "description": "",
    "content": " Ce qui est pas si évident que ça a faire sous Linux. C\u0026rsquo;est utile si on n\u0026rsquo;a pas accès à la console de la machine ou pour que le JBoss se lance tout seul au démarrage.\nScript init.d Il faut commencé par créer un script d\u0026rsquo;init. Pour cela on peut se servir de celui se trouvant à la fin de la page.\nAttention de bien penser à changer la variable \u0026ldquo;CAMELEON_DIR\u0026rdquo; avec le bon chemin d\u0026rsquo;installation de JBoss.\nInstallation Ensuite on installe le script. Pour cela, en root :\nchmod +x cameleon cp cameleon /etc/init.d/ Là le script est installé mais si vous voulez qu\u0026rsquo;il se lance seul au démarrage il faut l\u0026rsquo;ajouter aux runlevel :\nupdate-rc.d cameleon defaults \u0026ldquo;Remarque :\u0026rdquo; pour le retirer\nupdate-rc.d -f cameleon remove FIXME J\u0026rsquo;ai pas trop tester ça donc y aller avec des pincettes\nUtilisation Une fois le script installé, pour l\u0026rsquo;utiliser :\nsudo -i [sudo] password for administrateur: service cameleon start Starting Cameleon Edge: cameleon. exit déconnexion  Afin de suivre le lancement du JBoss, on pourra utiliser la commande suivante :\ntail -f /opt/cameleon-edge/jboss-4.2.0.GA/server/cameleon/log/server.log Le tail affiche la fin du fichier server.log en temps réel du coup ça simule une console. Pour en sortir il suffit de faire Ctrl+C FIXME Mettre la commande tail dans le status\nPour arrêter ou redémarrer le serveur, utiliser respectivement :\nservice cameleon stop service cameleon restart Files [[include:.doc/init-d-jboss.sh]]\n", 
    "breadcrumb": " > linux > administration > demarrage-de-jboss-en-service.html"
},
{
    
    "uri": "/linux/administration/desactiver-l-interface-graphique.html",
    "title": "Desactiver l’interface graphique",
    "tags": ["linux", "sysadmin", "graphic"],
    "description": "",
    "content": "Pour qu\u0026rsquo;une machine Linux démarre plus vite et avec moins de RAM, si vous vous le sentez, il est possible de ne pas lancer l\u0026rsquo;interface graphique. Pour cela il faut modifier le fichier /etc/default/grub.\nRemplacer la valeur de GRUB_CMDLINE_LINUX par text\nGRUB_CMDLINE_LINUX=\u0026quot;text\u0026quot;  Puis dans une ligne de commande root :\nupdate-grub Redémarrer\n", 
    "breadcrumb": " > linux > administration > desactiver-l-interface-graphique.html"
},
{
    
    "uri": "/tags/development.html",
    "title": "Development",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > development.html"
},
{
    
    "uri": "/tags/diff.html",
    "title": "Diff",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > diff.html"
},
{
    
    "uri": "/tags/dns.html",
    "title": "Dns",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > dns.html"
},
{
    
    "uri": "/tags/dock.html",
    "title": "Dock",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > dock.html"
},
{
    
    "uri": "/tags/docker.html",
    "title": "Docker",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > docker.html"
},
{
    
    "uri": "/outils/docker/docker--failed-to-create-image-rootfs.html",
    "title": "Docker failed to create image rootfs",
    "tags": ["development", "docker"],
    "description": "",
    "content": "En voulant puller une image docker depuis le registry j\u0026rsquo;ai cette erreur quand je suis en device-mapper :\n82f4a6e0947d: Error pulling image (jre7u67) from my-docker-hub/java, Driver devicemapper failed to create image rootfs 511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158: Can't set task name / dev/mapper/docker-8:2-5637992-pool name /dev/mapper/docker-8:2-5637992-pool 2014/11/25 15:05:47 Error pulling image (jre7u67) from my-docker-hub/java, Driver devicemapper failed to create image rootfs 511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158: Can't set task name /dev/mapper/docker-8:2-5637992-pool  Pour corriger :\nsudo service docker stop sudo rm -Rf /var/lib/docker sudo service docker start", 
    "breadcrumb": " > outils > docker > docker--failed-to-create-image-rootfs.html"
},
{
    
    "uri": "/linux/gnomeshell/documents-recents-dans-gnome-shell.html",
    "title": "Documents recents dans gnome-shell",
    "tags": ["linux", "gnome", "shell", "debian"],
    "description": "",
    "content": " Pour les dernières versions de gnome, on peut retrouver le fichier d\u0026rsquo;historique à l\u0026rsquo;emplacement suivant: ~/.local/share/recently-used.xbel, pour les versions antérieures il se trouve à ~/.recently-used.xbel.\nEffacer l\u0026rsquo;intégralité du fichier Dans un terminal, taper :\ncat /dev/null \u0026gt; ~/.local/share/recently-used.xbel Puis relancer gnome-shell (Alt+F2, puis r)\nPasser en mode \u0026ldquo;privé\u0026rdquo; Pour désactiver l\u0026rsquo;enregistrement des documents récents: Dans un terminal, taper :\nsudo chattr +i ~/.local/share/recently-used.xbel Pour réactiver l\u0026rsquo;enregistrement des documents récents: Dans un terminal, taper :\nsudo chattr -i ~/.local/share/recently-used.xbel Le script suivant vous permettra de basculer dans un mode ou dans l\u0026rsquo;autre:\n#!/bin/bash # Bloquer les écritures dans .recently-used.xbel (documents récents du gnome-shell) # Attention au répertoire : version ancienne de gnome : ~/.recently-used.xbel # A adapter selon le chemin au fichier fichier=\u0026#34;/\u0026lt;chemin\u0026gt;/\u0026lt;d\u0026#39;accès\u0026gt;/\u0026lt;au\u0026gt;/.local/share/recently-used.xbel\u0026#34; if [ $USER = root ] then echo \u0026#34;Vous êtes root\u0026#34; else echo \u0026#34;Vous n\u0026#39;êtes pas root, execution du script en sudo\u0026#34; gksu $0 exit fi attribut=` sudo lsattr \u0026#34;$fichier\u0026#34;| cut -d\u0026#39;-\u0026#39; -f5 ` \u0026amp;\u0026amp; if [ \u0026#34;$attribut\u0026#34; == \u0026#34;i\u0026#34; ] ; then zenity --question --text=\u0026#34;Gnome est en mode confidentiel, voulez-vous quitter ce mode ?\u0026#34; \u0026amp;\u0026amp; sudo chattr -i \u0026#34;$fichier\u0026#34; \u0026amp;\u0026amp; zenity --info --text=\u0026#34; \u0026lt;b\u0026gt;\u0026lt;span color=\\\u0026#34;red\\\u0026#34;\u0026gt; Gnome n\u0026#39;est plus en mode confidentiel \u0026lt;/span\u0026gt;\u0026lt;/b\u0026gt; \u0026#34; || exit else zenity --question --text=\u0026#34;\u0026lt;b\u0026gt;\u0026lt;span color=\\\u0026#34;red\\\u0026#34;\u0026gt; Gnome n\u0026#39;est pas en mode confidentiel \u0026lt;/span\u0026gt;\u0026lt;/b\u0026gt;, voulez-vous l\u0026#39;activer ?\u0026#34; \u0026amp;\u0026amp; sudo chattr +i \u0026#34;$fichier\u0026#34; \u0026amp;\u0026amp; zenity --info --text=\u0026#34;Gnome est maintenant en mode \u0026lt;b\u0026gt;\u0026lt;span color=\\\u0026#34;red\\\u0026#34;\u0026gt; confidentiel \u0026lt;/span\u0026gt;\u0026lt;/b\u0026gt;\u0026#34; || exit fi exit", 
    "breadcrumb": " > linux > gnomeshell > documents-recents-dans-gnome-shell.html"
},
{
    
    "uri": "/outils/latex/dollar-en-latex.html",
    "title": "Dollar en latex",
    "tags": ["outils", "latex"],
    "description": "",
    "content": "Problème de $ en LaTeX, le caractère ne prend pas la police qu’on lui donne.\nApparement en LaTeX le dollar est codé en dur dans le code de Latex et du coup ne ressort pas toujours correctement.\nhttps://tug.org/pipermail/xetex/2007-October/007560.html\nYou'll at least need to load the xunicode package for this to work. Hopefully that's the only problem with your example, although you seen to be using a slightly older version of fontspec. The reason is that \\$ is somewhat hard-coded in LaTeX (shock, I know) and xunicode fixes it up (and many many other symbols) to use the proper unicode code points. Hope this helps, Will  ", 
    "breadcrumb": " > outils > latex > dollar-en-latex.html"
},
{
    
    "uri": "/linux/network/domaines-de-recherche-pour-networkmanager.html",
    "title": "Domaines de recherche pour NetworkManager",
    "tags": ["linux", "network", "dns"],
    "description": "",
    "content": " Dans les versions récentes de Gnome Shell, l\u0026rsquo;interface de NetworkManager ne donne plus la possibilité de saisir les domaines de rechercher pour les serveurs DNS. Il est possible de les mettre dans le resolv.conf mais le fichier est géré par NetworkManager et peut être écrasé n\u0026rsquo;importe quand. Un solution est donc de mettre manuellement la configuration de NetworkManager à jour dans le fichier /etc/NetworkManager/system-connections/Wired connection 1:\n[connection] id=Wired connection 1 uuid=86fe8169-0452-488e-8662-0cb1dc834333 type=ethernet timestamp=1423488595 [ipv6] method=auto ip6-privacy=2 [ipv4] method=auto dns=172.17.10.31;172.17.11.100;208.67.222.222;208.67.220.220; dns-search=livingobjects.com;lo.internal; ignore-auto-dns=true en rajoutant la ligne dns-search.\nLiens  https://developer.gnome.org/NetworkManager/stable/ref-settings.html  ", 
    "breadcrumb": " > linux > network > domaines-de-recherche-pour-networkmanager.html"
},
{
    
    "uri": "/tags/dotnet.html",
    "title": "Dotnet",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > dotnet.html"
},
{
    
    "uri": "/tags/download.html",
    "title": "Download",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > download.html"
},
{
    
    "uri": "/linux/network/download-fichier-via-ssh-avec-reprise.html",
    "title": "Download fichier via SSH avec reprise",
    "tags": ["linux", "network", "ssh", "rsync"],
    "description": "",
    "content": "La plus part du temps on utilise SCP mais sur des fichiers de taille conséquente, il est pratique de pouvoir reprendre le téléchargement si la connexion a été rompue ou autres. RSync permet de faire ça.\nrsync --partial --progress --rsh=ssh user@1.1.1.1:/home/user/database.tar.gz database.tar.gz", 
    "breadcrumb": " > linux > network > download-fichier-via-ssh-avec-reprise.html"
},
{
    
    "uri": "/tags/dump.html",
    "title": "Dump",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > dump.html"
},
{
    
    "uri": "/linux/network/debloquer-le-wifi.html",
    "title": "Débloquer le wifi",
    "tags": ["linux", "network", "dns", "security"],
    "description": "",
    "content": "Après une dés-activation du wifi dans network manager de ~GnomeShell, il est impossible de le ré-activer. Pour pouvoir le ré-activer voilà la manœuvre :\n$ sudo rfkill list 0: sony-wifi: Wireless LAN Soft blocked: yes Hard blocked: no 1: sony-bluetooth: Bluetooth Soft blocked: no Hard blocked: no 3: phy0: Wireless LAN Soft blocked: yes Hard blocked: yes 5: hci0: Bluetooth Soft blocked: no Hard blocked: nosudo rfkill unblock 0 Il est maintenant possible de ré-activer le wifi dans l\u0026rsquo;interface.\n", 
    "breadcrumb": " > linux > network > debloquer-le-wifi.html"
},
{
    
    "uri": "/linux/shell/decouper-et-rattacher-un-gros-fichier.html",
    "title": "Découper et rattacher un gros fichier",
    "tags": ["linux", "shell"],
    "description": "",
    "content": "split -b 4000m fichier.tar.gz newfichier.tar.gzcat newfichier.tar.gz.* \u0026gt; fichier.tar.gz", 
    "breadcrumb": " > linux > shell > decouper-et-rattacher-un-gros-fichier.html"
},
{
    
    "uri": "/serveurs/oracle/demarrage-arret-automatique-doracle-sous-linux.html",
    "title": "Démarrage/Arrêt automatique d’Oracle sous Linux",
    "tags": ["server", "oracle", "database", "linux", "service", "script"],
    "description": "",
    "content": " Modifier le fichier /etc/oratab (ou /var/opt/oracle selon install) Ajouter \u0026lt;SID\u0026gt;:\u0026lt;ORACLE_HOME\u0026gt;:Y le \u0026ldquo;Y\u0026rdquo; indiquant si la base doit être ou non démarré par \u0026ldquo;dbstart\u0026rdquo; et \u0026ldquo;dbshut\u0026rdquo;.\nCréer le script \u0026ldquo;/etc/init.d/dboracle\u0026rdquo; contenant :\n#!/bin/bash  #  # chkconfig: 35 99 10  # description: Starts and stops Oracle processes  #  # Set ORA_HOME to be equivalent to the $ORACLE_HOME  # from which you wish to execute dbstart and dbshut;  #  # Set ORA_OWNER to the user id of the owner of the # Oracle database in ORA_HOME.  #  ORA_HOME=\u0026lt;Type your ORACLE_HOME in full path here\u0026gt; ORA_OWNER=\u0026lt;Type your Oracle account name here\u0026gt; case \u0026#34;$1\u0026#34; in \u0026#39;start\u0026#39;) # Start the TNS Listener  su - $ORA_OWNER -c \u0026#34;$ORA_HOME/bin/lsnrctl start\u0026#34; # Start the Oracle databases: # The following command assumes that the oracle login  # will not prompt the user for any values  su - $ORA_OWNER -c $ORA_HOME/bin/dbstart # Start the Intelligent Agent  if [ -f $ORA_HOME/bin/emctl ]; then su - $ORA_OWNER -c \u0026#34;$ORA_HOME/bin/emctl start agent\u0026#34; elif [ -f $ORA_HOME/bin/agentctl ]; then su - $ORA_OWNER -c \u0026#34;$ORA_HOME/bin/agentctl start\u0026#34; else su - $ORA_OWNER -c \u0026#34;$ORA_HOME/bin/lsnrctl dbsnmp_start\u0026#34; fi # Start Management Server  if [ -f $ORA_HOME/bin/emctl ]; then su - $ORA_OWNER -c \u0026#34;$ORA_HOME/bin/emctl start dbconsole\u0026#34; elif [ -f $ORA_HOME/bin/oemctl ]; then su - $ORA_OWNER -c \u0026#34;$ORA_HOME/bin/oemctl start oms\u0026#34; fi # Start HTTP Server  if [ -f $ORA_HOME/Apache/Apache/bin/apachectl ]; then su - $ORA_OWNER -c \u0026#34;$ORA_HOME/Apache/Apache/bin/apachectl start\u0026#34; fi touch /var/lock/subsys/dbora ;; \u0026#39;stop\u0026#39;) # Stop HTTP Server  if [ -f $ORA_HOME/Apache/Apache/bin/apachectl ]; then su - $ORA_OWNER -c \u0026#34;$ORA_HOME/Apache/Apache/bin/apachectl stop\u0026#34; fi # Stop the TNS Listener  su - $ORA_OWNER -c \u0026#34;$ORA_HOME/bin/lsnrctl stop\u0026#34; # Stop the Oracle databases:  # The following command assumes that the oracle login # will not prompt the user for any values  su - $ORA_OWNER -c $ORA_HOME/bin/dbshut rm -f /var/lock/subsys/dbora ;; esac # End of script dbora Modifier les permissions du script :\nchmod 755 /etc/init.d/dbora Enregistrer le service :\n/sbin/chkconfig --add dbora Cette action enregistre le services dans les mécanisme de démarrage de Linux. Sur SuSE SLES7 et Red Hat Advanced Server 2.1 ça crée des lien symbolique dans le répertoire rc\u0026lt;runlevel\u0026gt;.d vers /etc/init.d/dbora script.\nLiens  http://www.togaware.com/linux/survivor/Starting_Stopping.html   ", 
    "breadcrumb": " > serveurs > oracle > demarrage-arret-automatique-doracle-sous-linux.html"
},
{
    
    "uri": "/serveurs/oracle/demarrer-arreter-une-base-en-ligne-de-commande-windows.html",
    "title": "Démarrer/Arrêter une base en ligne de commande Windows",
    "tags": ["server", "oracle", "database", "windows"],
    "description": "",
    "content": "Pour démarrer :\nC:\\\u0026gt; %ORACLE_HOME%\\bin\\oradim -startup -sid ORCL92 -usrpwd manager -starttype SRVC,INST -pfile C:\\oracle9i\\admin\\ORCL92\\pfile\\init.ora  Pour arrêter :\nC:\\\u0026gt; %ORACLE_HOME%\\bin\\oradim -shutdown -sid ORCL92 -shutttype SRVC,INST -shutmode A  ", 
    "breadcrumb": " > serveurs > oracle > demarrer-arreter-une-base-en-ligne-de-commande-windows.html"
},
{
    
    "uri": "/misc/desactiver-le-fixup-mozilla.html",
    "title": "Désactiver le fixup Mozilla",
    "tags": ["misc", "firefox"],
    "description": "",
    "content": "Un truc vraiment pénible dans Firefox, quand on saisie l\u0026rsquo;adresse localhost http://localhost/ et que cette dernière ne répond pas il remplace par http://www.localhost.com/ et chaque fois qu’on veut raffraichir on retape \u0026hellip;\nPour désactiver ça :\n about:config Chercher fixup Pour browser.fixup.alternate.enabled mettre false  ", 
    "breadcrumb": " > misc > desactiver-le-fixup-mozilla.html"
},
{
    
    "uri": "/serveurs/oracle/desinstaller-oracle-xe-sous-linux.html",
    "title": "Désinstaller Oracle XE sous Linux",
    "tags": ["server", "oracle", "database", "linux"],
    "description": "",
    "content": " C\u0026rsquo;est pas bien compliqué mais pour le faire proprement ya quelques trucs à pas oublier.\nDésinstallation des packages Debian apt-get remove oracle-xe-universal apt-get purge oracle-xe-universal Suppression des fichiers restant rm -Rf /usr/lib/oracle/xe rm -Rf /etc/oratab rm -Rf /etc/init.d/oracle-xe rm -Rf /etc/sysconfig/oracle-xe rm -Rf $(find . -name *oracle*)", 
    "breadcrumb": " > serveurs > oracle > desinstaller-oracle-xe-sous-linux.html"
},
{
    
    "uri": "/outils/vmware/esxi-4.1-avec-vmplayer.html",
    "title": "ESXi 4.1 avec VMPlayer",
    "tags": ["outils", "vmware", "vmplayer", "esxi"],
    "description": "",
    "content": " Le serveur ESXi ne dispose pas d\u0026rsquo;accès HTML pour administrer les VMs, donc plus d\u0026rsquo;accès depuis un navigateur. L\u0026rsquo;utilisation des VMs se fait par un client lourd ou en accès console via VMPlayer ou VMRC.\nREMARQUE Apparemment sous Windows ça ne fonctionne pas.\nOuvrir une Remote Console Là ya pas de feinte dans Firefox, dans tout les cas, il n\u0026rsquo;est pas possible d\u0026rsquo;ouvrir une console depuis le navigateur. Par contre, il est possible de le faire depuis VMPlayer 3. C\u0026rsquo;est une option caché mais qui fonctionne (avec la version 3, pas avec les versions précédentes).\nSi vous appelez la commande\nvmplayer -h Cela ouvre VMPlayer avec une boite de connexion dans laquelle vous pouvez saisir l\u0026rsquo;adresse et le port du serveur de VM ainsi que les identifiants pour s\u0026rsquo;y connecter.\nLe mieux est de se faire un raccourcis comme suit :\nvmplayer -h atlantis.tl.internal -u \u0026lt;loggin\u0026gt; -p \u0026lt;password\u0026gt; Cela vous ouvre directement la liste des VMs disponible sur le serveur.\nEt en rajoutant l\u0026rsquo;option -M à la ligne de commande, vous pouvez faire un raccourcis directement sur une VM particulière :\nvmplayer -h atlantis.tl.internal -u \u0026lt;loggin\u0026gt; -p \u0026lt;password\u0026gt; -M \u0026lt;idVM\u0026gt; Sous Linux, si vous prenez un message d\u0026rsquo;erreur de transfert de données, essayez de lancer la commande avec \u0026ldquo;root\u0026rdquo;.\nLiens  http://planetvm.net/blog/?p=1316  ", 
    "breadcrumb": " > outils > vmware > esxi-4.1-avec-vmplayer.html"
},
{
    
    "uri": "/tags/eclipse.html",
    "title": "Eclipse",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > eclipse.html"
},
{
    
    "uri": "/linux/shell/effacer-definitivement-un-disque-dur.html",
    "title": "Effacer définitivement un disque dur",
    "tags": ["linux", "shell", "cleanup", "security"],
    "description": "",
    "content": " L\u0026rsquo;objectif est de supprimer les données d\u0026rsquo;un disque dur de façon définitive pour se protéger d\u0026rsquo;une reconstruction de la table d\u0026rsquo;index\nL’utilitaire « dd » fourni avec tous les *nix et dérivés, permet de faire de nombreuses manipulations sur des fichiers ou des systèmes de fichiers comme (liste non exhaustive): Formatter une disquette à partir d’une image, découper un fichier en plusieurs morceaux, faire une image d’un DVD ou encore — et c’est ce qui nous intéresse ici — détruire les données d’un disque dur en le remplissant de zéros ou de données aléatoires.\n\u0026ldquo;Il faut toujours faire attention lorsqu’on utilise dd. Le simple fait d’oublier une option ou d’échanger les arguments aura des conséquences désastreuses.\u0026rdquo;\nMéthode rapide et peu sûre dd if=/dev/zero of=/dev/sdX bs=512 conv=notrunc  L’argument conv=notrunc permet de ne pas limiter la taille du fichier de sortie.\nMéthode lente et moyennement sûre On met du random au lieu du zero\ndd if=/dev/urandom of=/dev/sdX bs=512 conv=notrunc  Méthode très lente et sûre Ici on fait en plusieurs passes (32)\nfor n in `seq 32`; do dd if=/dev/zero of=/dev/sdX bs=512 conv=notrunc; done  Notez qu’ici j’ai défini le paramètre bs qui correspond au nombre d’octets spécifié écrit en une seule fois. Ce qui permet de raccourcir considérablement le temps de chaque passe.\nMéthode vraiment très lente et très sûre Enfin en plusieurs passes (16) avec du ramdom, c\u0026rsquo;est méga super long !\nfor n in `seq 16`; do dd if=/dev/urandom of=/dev/sdX bs=512 conv=notrunc; done  Liens  https://www.crashdump.fr/debian/effacer-definivement-toutes-les-donnees-dun-disque-dur-sous-nux-677/  ", 
    "breadcrumb": " > linux > shell > effacer-definitivement-un-disque-dur.html"
},
{
    
    "uri": "/linux/shell/envoyer-post-avec-curl.html",
    "title": "Envoyer POST avec cURL",
    "tags": ["linux", "shell", "curl", "network"],
    "description": "",
    "content": "Comment envoyer une requête POST avec le contenu d\u0026rsquo;un fichier dans le body. Le tout avec un header :\ncurl -H \u0026#34;Content-Type: application/json\u0026#34; \\  --data \u0026#34;@salut.txt\u0026#34; http://localhost:8082/export \\  | python -m json.tool Le fichier salut.txt est dans le répertoire courant \u0026hellip;\nEn bonus on notera la dernière ligne qui permet, si la requète retourne du JSON, de formater lisiblement le résultat.\n", 
    "breadcrumb": " > linux > shell > envoyer-post-avec-curl.html"
},
{
    
    "uri": "/outils/eclipse/erreur-cannot-connect-to-vm-lors-dun-debug.html",
    "title": "Erreur Cannot connect to VM lors dun debug",
    "tags": ["outils", "eclipse", "debug"],
    "description": "",
    "content": "Lors d\u0026rsquo;un lancement en mode débug, Eclipse se connect à la JVM. Pour cela il utilise localhost comme nom de machine sur la VM. Si dans le fichier /etc/hosts, localhost ne pointe pas sur 127.0.0.1 l\u0026rsquo;erreur Connexion à la VM impossible est levée. Il suffit donc de corriger le fichier hosts.\n", 
    "breadcrumb": " > outils > eclipse > erreur-cannot-connect-to-vm-lors-dun-debug.html"
},
{
    
    "uri": "/serveurs/mysql/erreur-de-read-only-status-avec-jdbc.html",
    "title": "Erreur de read-only status avec JDBC",
    "tags": ["server", "mysql", "database", "jdbc", "java"],
    "description": "",
    "content": " Symptômes J\u0026rsquo;ai rencontré l\u0026rsquo;erreur suivante :\njava.io.IOException: java.sql.SQLException: Could not retrieve transation read-only status server Caused by: java.sql.SQLException: Could not retrieve transation read-only status server Caused by: java.sql.SQLException: REPLACE INTO `db`.`test_table` (`timestamp`,`database`,`table`,`columns`,`lines`,`before`,`write`,`wait`) VALUES (?,?,?,?,?,?,?,?) ... 4 common frames omitted Caused by: java.sql.SQLException: Could not retrieve transation read-only status server at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1086) ~[mysql-connector-java-5.1.28.jar:na] at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:989) ~[mysql-connector-java-5.1.28.jar:na] at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:975) ~[mysql-connector-java-5.1.28.jar:na] at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:920) ~[mysql-connector-java-5.1.28.jar:na] at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:951) ~[mysql-connector-java-5.1.28.jar:na] at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:941) ~[mysql-connector-java-5.1.28.jar:na] at com.mysql.jdbc.ConnectionImpl.isReadOnly(ConnectionImpl.java:3955) ~[mysql-connector-java-5.1.28.jar:na] at com.mysql.jdbc.ConnectionImpl.isReadOnly(ConnectionImpl.java:3926) ~[mysql-connector-java-5.1.28.jar:na] at com.mysql.jdbc.PreparedStatement.executeBatch(PreparedStatement.java:1430) ~[mysql-connector-java-5.1.28.jar:na] at com.zaxxer.hikari.proxy.StatementProxy.executeBatch(StatementProxy.java:116) ~[HikariCP-java6-2.3.2.jar:na] at com.zaxxer.hikari.proxy.PreparedStatementJavassistProxy.executeBatch(PreparedStatementJavassistProxy.java) ~[HikariCP-java6-2.3.2.jar:na] ... 3 common frames omitted Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLNonTransientConnectionException: Could not create connection to database server. Attempted reconnect 3 times. Giving up. at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[na:1.8.0_51] at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[na:1.8.0_51] at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[na:1.8.0_51] at java.lang.reflect.Constructor.newInstance(Constructor.java:422) ~[na:1.8.0_51] at com.mysql.jdbc.Util.handleNewInstance(Util.java:411) ~[mysql-connector-java-5.1.28.jar:na] at com.mysql.jdbc.Util.getInstance(Util.java:386) ~[mysql-connector-java-5.1.28.jar:na] at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1015) ~[mysql-connector-java-5.1.28.jar:na] at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:989) ~[mysql-connector-java-5.1.28.jar:na] at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:975) ~[mysql-connector-java-5.1.28.jar:na] at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:920) ~[mysql-connector-java-5.1.28.jar:na] at com.mysql.jdbc.ConnectionImpl.connectWithRetries(ConnectionImpl.java:2395) ~[mysql-connector-java-5.1.28.jar:na] at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2316) ~[mysql-connector-java-5.1.28.jar:na] at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2807) ~[mysql-connector-java-5.1.28.jar:na] at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2768) ~[mysql-connector-java-5.1.28.jar:na] at com.mysql.jdbc.StatementImpl.executeQuery(StatementImpl.java:1651) ~[mysql-connector-java-5.1.28.jar:na] at com.mysql.jdbc.ConnectionImpl.isReadOnly(ConnectionImpl.java:3949) ~[mysql-connector-java-5.1.28.jar:na] ... 8 common frames omitted Caused by: java.sql.SQLException: No database selected at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1086) ~[mysql-connector-java-5.1.28.jar:na] at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4237) ~[mysql-connector-java-5.1.28.jar:na] at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4169) ~[mysql-connector-java-5.1.28.jar:na] at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2617) ~[mysql-connector-java-5.1.28.jar:na] at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2778) ~[mysql-connector-java-5.1.28.jar:na] at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2819) ~[mysql-connector-java-5.1.28.jar:na] at com.mysql.jdbc.ConnectionImpl.setCatalog(ConnectionImpl.java:5443) ~[mysql-connector-java-5.1.28.jar:na] at com.mysql.jdbc.ConnectionImpl.connectWithRetries(ConnectionImpl.java:2368) ~[mysql-connector-java-5.1.28.jar:na] ... 13 common frames omitted  Au fil de la stacktrace on voit tout un tas de root cause la dernière étant No database selected. Mais toute ces ne sont que la conséquence de quelque chose de plus grave. Il m\u0026rsquo;est aussi arrivé d\u0026rsquo;avoir :\nCaused by: java.sql.SQLException: Streaming result set com.mysql.jdbc.RowDataDynamic@62f3413a is still active. No statements may be issued when any streaming result sets are open and in use on a given connection. Ensure that you have called .close() on any active streaming result sets before attempting more queries.  Alors que toutes les connections sont systématiquement rendu au pool !\nSolution Plusieurs solutions m\u0026rsquo;ont été proposées mais seulement des solutions pour les problèmes visibles dans les stacktraces. Le résultat c\u0026rsquo;est qu\u0026rsquo;il n\u0026rsquo;y a plus d\u0026rsquo;erreurs visible mais l\u0026rsquo;exécution est bloqué en attente d\u0026rsquo;on ne sait quoi.\nLa source réelle du problème est une option de performance dans le driver JDBC :\nconfig.addDataSourceProperty(\u0026#34;cachePrepStmts\u0026#34;, \u0026#34;true\u0026#34;); config.addDataSourceProperty(\u0026#34;prepStmtCacheSize\u0026#34;, \u0026#34;250\u0026#34;); config.addDataSourceProperty(\u0026#34;prepStmtCacheSqlLimit\u0026#34;, \u0026#34;2048\u0026#34;); config.addDataSourceProperty(\u0026#34;useServerPrepStmts\u0026#34;, \u0026#34;true\u0026#34;); J\u0026rsquo;utilise HikariCP mais c\u0026rsquo;est valable pour tout les gestionnaires de pool. L\u0026rsquo;option useServerPrepStmts est sencé améliorer les performance en cachant les PrepareStatement coté serveur. Mais MySQL 5.6 réagit mal a cette option . Les statement se ferment mal. Du coup supprimer cette option résoud le problème.\n", 
    "breadcrumb": " > serveurs > mysql > erreur-de-read-only-status-avec-jdbc.html"
},
{
    
    "uri": "/tags/error.html",
    "title": "Error",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > error.html"
},
{
    
    "uri": "/tags/esxi.html",
    "title": "Esxi",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > esxi.html"
},
{
    
    "uri": "/tags/exchange.html",
    "title": "Exchange",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > exchange.html"
},
{
    
    "uri": "/outils/docker/export-importer-une-image-docker.html",
    "title": "Export Importer une image Docker",
    "tags": ["development", "docker"],
    "description": "",
    "content": "Pour exporter :\ndocker save mynewimage \u0026gt; /tmp/mynewimage.tar bzip2 /tmp/mynewimage.tar Pour ré-importer:\nbzip2 -d /tmp/mynewimage.tar.bz2 docker load \u0026lt; /tmp/mynewimage.tar", 
    "breadcrumb": " > outils > docker > export-importer-une-image-docker.html"
},
{
    
    "uri": "/serveurs/mysql/exporter-importer-une-base-d-un-dump.html",
    "title": "Exporter/Importer une base d’un dump",
    "tags": ["server", "mysql", "database", "jdbc", "java"],
    "description": "",
    "content": "Pour exporter en gzip\nmysqldump -u user -p database | gzip \u0026gt; database.sql.gz Pour importer une seule base à partir d\u0026rsquo;un dump complet, il faut entrer la commande suivante :\nmysql -u USERNAME -p --one-database BASE_A_RESTAURER \u0026lt; dumpcomplet.sql Remplacez BASE_A_RESTAURER par le nom de la base de votre choix qui est contenue dans le fichier dumpcomplet.sql.\nLa même chose avec un fichier d\u0026rsquo;export compressé en tar.gz\nzcat your_db_dump.sql.tar.gz | mysql -u USERNAME -p BASE_A_RESTAURER", 
    "breadcrumb": " > serveurs > mysql > exporter-importer-une-base-d-un-dump.html"
},
{
    
    "uri": "/serveurs/oracle/exporter-importer-un-schema-de-base.html",
    "title": "Exporter/importer un schéma de base",
    "tags": ["server", "oracle", "database"],
    "description": "",
    "content": "c:\\\u0026gt; exp edge/leon file=c:\\temp\\EDGE_SP3_FIX016_AXA.dmp direct=y  SQL\u0026gt; create user edge identified by leon; SQL\u0026gt; grant connect,resource to edgec:\\\u0026gt; imp system/manager file=c:\\temp\\EDGE_SP3_FIX016_AXA.dmp ignore=y fromuser=edge touser=edge Voir aussi \u0026rarr; Utiliser DataPump en ligne de commande\n", 
    "breadcrumb": " > serveurs > oracle > exporter-importer-un-schema-de-base.html"
},
{
    
    "uri": "/tags/firebug.html",
    "title": "Firebug",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > firebug.html"
},
{
    
    "uri": "/tags/firefox.html",
    "title": "Firefox",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > firefox.html"
},
{
    
    "uri": "/linux/network/firefox-commandline-options.html",
    "title": "Firefox CommandLine Options",
    "tags": ["linux", "network", "firefox"],
    "description": "",
    "content": "Voici les Paramètres concernant la gestion des profils :\n -CreateProfile nomduprofil - Vous permet de créer un nouveau profil, mais cela ne le lance pas. -CreateProfile \u0026ldquo;nomduprofil dossier\u0026rdquo; - Même chose, mais cette fois vous spécifiez l\u0026rsquo;emplacement du profil. -ProfileManager, ou -P - Lance Firefox en affichant le gestionnaire de profil. -P \u0026ldquo;nomduprofil\u0026rdquo; - Démarre Firefox directement avec le profil indiqué. -no-remote - Couplé au paramètre -P, celui-ci permet de lancer plusieurs instances du même navigateur avec des profils différents.  Maintenant voici les paramètres qui vont modifier le comportement du navigateur :\n -headless - Permet de démarrer Firefox en mode headless. C\u0026rsquo;est un mode spécial qui permet de lancer Firefox sans ouvrir aucune fenêtre. Il est très utilisé pour réaliser des tests unitaires automatisés. Ce mode est dispo uniquement dans Firefox \u0026gt;= 55 sous Linux, Firefox \u0026gt;= 56 sous Windows et macOS -new-tab URL - Lance Firefox avec l\u0026rsquo;URL de votre choix. -new-window URL - Lance Firefox avec l\u0026rsquo;URL de votre choix affiché dans une nouvelle fenêtre. -private - Lance Firefox en mode navigation privée (alias le mode \u0026ldquo;Portail magique vers le Pornivers\u0026rdquo;) -private-window - Lance Firefox et ouvre en plus une fenêtre navigation privée. -private-window URL - Lance Firefox et ouvre en plus une fenêtre navigation privée vers le site de votre choix. -search motsclés - Lance Firefox avec une recherche vers les mots clés voulus -url URL1, URL2, URL3 - Lance Firefox et ouvre un ou plusieurs sites web de votre choix. Vous pouvez séparer les URL par des virgules.  Et sinon, il y a aussi ces paramètres :\n -safe-mode - Lance Firefox en mode \u0026ldquo;Safe\u0026rdquo;, c\u0026rsquo;est-à-dire avec toutes les extensions désactivées. Vous pouvez aussi maintenir la touche \u0026ldquo;Shift\u0026rdquo; lorsque vous cliquez sur votre raccourci. -devtools - Lance Firefox avec les outils de développement web activés. -inspector URL - Active l\u0026rsquo;inspecteur DOM sur l\u0026rsquo;URL de votre choix. -jsconsole - Lance Firefox avec la console active. -tray - Lance Firefox réduit dans la barre des tâches.  ", 
    "breadcrumb": " > linux > network > firefox-commandline-options.html"
},
{
    
    "uri": "/tags/firewall.html",
    "title": "Firewall",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > firewall.html"
},
{
    
    "uri": "/tags/fork.html",
    "title": "Fork",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > fork.html"
},
{
    
    "uri": "/linux/shell/formater-simplement-un-xml.html",
    "title": "Formater simplement un XML",
    "tags": ["linux", "shell", "xml"],
    "description": "",
    "content": "Comme Linux c\u0026rsquo;est trop bien, ya un petite commande très simple qui permet de formater proprement un XML. C\u0026rsquo;est pratique par exemple pour pouvoir l\u0026rsquo;ouvrir dans Eclipse car si le XML est formaté sur une seule ligne Eclipse pète une durite quand on veut l\u0026rsquo;ouvrir \u0026hellip;\nxmllint -format -recover nonformater.xml \u0026gt; formater.xml", 
    "breadcrumb": " > linux > shell > formater-simplement-un-xml.html"
},
{
    
    "uri": "/tags/front.html",
    "title": "Front",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > front.html"
},
{
    
    "uri": "/serveurs/oracle/fusionner-les-espaces-libre-contigus.html",
    "title": "Fusionner les espaces libre contigus",
    "tags": ["server", "oracle", "database", "sql"],
    "description": "",
    "content": "Très pratique surtout pour les anciennes version:\nALTER TABLESPACE toto COALESCE", 
    "breadcrumb": " > serveurs > oracle > fusionner-les-espaces-libre-contigus.html"
},
{
    
    "uri": "/linux/gnomeshell/join-application-icon-in-dock.html",
    "title": "Fusionner les icones d’application dans le dock",
    "tags": ["linux", "gnome", "shell", "debian", "dock"],
    "description": "",
    "content": " Il y a un truc vraiment pénible avec GnomeShell et le Dock, c\u0026rsquo;est quand une application apparaît plusieurs fois. Pour une raison X ou Y, le raccourcis d\u0026rsquo;une application n\u0026rsquo;est plus regroupé avec ses instances dans le dock ce qui fait que l\u0026rsquo;on a rapidement tendance à ouvrir de nouvelle applications a chaque fois qu\u0026rsquo;on veut accédé à l\u0026rsquo;application.\nC\u0026rsquo;est du à la WM_CLASS de l\u0026rsquo;application qui n\u0026rsquo;est pas configuré correctement dans le raccourcis bureau. Prennons comme exemple chromium.\nRécupérer le WM_CLASS de l\u0026rsquo;application Pour ça:\nxprop | grep WM_CLASS Et pointer avec la croix la fenêtre de l\u0026rsquo;application ce qui devrait donner :\nWM_CLASS(STRING) = \u0026quot;chromium-browser\u0026quot;, \u0026quot;chromium-browser\u0026quot;  Editer le raccourcis Editer le fichier ~/.local/share/applications/chromium.desktop ou/et /usr/share/applications/chromium.desktop puis ajouter/remplace la ligne:\nStartupWMClass=chromium-browser  Et voilà\n", 
    "breadcrumb": " > linux > gnomeshell > join-application-icon-in-dock.html"
},
{
    
    "uri": "/outils/vmware/geler-l-heure-d-une-vm.html",
    "title": "Geler l’heure d une VM",
    "tags": ["outils", "vmware", "windows"],
    "description": "",
    "content": "Chose qui peut être pratique si l\u0026rsquo;on a une VM valable 30 jours ou avec des logiciels valable un certain laps de temps \u0026hellip;\nDans le fichier .vmx, ajouter les lignes suivantes :\ntools.syncTime = \u0026quot;FALSE\u0026quot; time.synchronize.continue = \u0026quot;FALSE\u0026quot; time.synchronize.restore = \u0026quot;FALSE\u0026quot; time.synchronize.resume.disk = \u0026quot;FALSE\u0026quot; time.synchronize.shrink = \u0026quot;FALSE\u0026quot; time.synchronize.tools.startup = \u0026quot;FALSE\u0026quot; time.synchronize.tools.enable = \u0026quot;FALSE\u0026quot; time.synchronize.resume.host = \u0026quot;FALSE\u0026quot; rtc.startTime = \u0026quot;1342101600\u0026quot;  Attention il se peut que tools.syncTime soit déjà déclaré dans le fichier. Pour rtc.startTime c\u0026rsquo;est le timestamp qui correspond à l\u0026rsquo;heure de la VM souhaité. Pour s\u0026rsquo;aider à la convertion heure/timestamp on peut aller sur ce site : http://www.4webhelp.net/us/timestamp.php\n", 
    "breadcrumb": " > outils > vmware > geler-l-heure-d-une-vm.html"
},
{
    
    "uri": "/serveurs/oracle/gestion-des-dates-oracle.html",
    "title": "Gestion des dates Oracle",
    "tags": ["server", "oracle", "database", "sql"],
    "description": "",
    "content": "La pseudo colonne SYSDATE affiche la date et l\u0026rsquo;heure courante. Ajouter 1 à SYSDATE avance la date d\u0026rsquo;un jour. On peut alors utiliser des fractions pour ajouter des heures/minutes/secondes. Voilà un exemple :\nSQL\u0026gt; select sysdate, sysdate+1/24, sysdate +1/1440, sysdate + 1/86400 from dual; SYSDATE SYSDATE+1/24 SYSDATE+1/1440 SYSDATE+1/86400 -------------------- -------------------- -------------------- -------------------- 03-Jul-2002 08:32:12 03-Jul-2002 09:32:12 03-Jul-2002 08:33:12 03-Jul-2002 08:32:13 Et quelques exemples possible :\n   Description Date Expression     Maintenant SYSDATE   Lendemain SYSDATE + 1   Dans 7 jours SYSDATE + 7   Dans 1 heure SYSDATE + 1\u0026frasl;24   Dans 3 heures SYSDATE + 3\u0026frasl;24   Dans une demi-heure SYSDATE + 1\u0026frasl;48   Dans 10mn SYSDATE + 10\u0026frasl;1440   Dans 30s SYSDATE + 30\u0026frasl;86400   Demain à minuit TRUNC(SYSDATE + 1)   Demain à 8h TRUNC(SYSDATE + 1) + 8\u0026frasl;24   Prochain Lundi midi NEXT_DAY(TRUNC(SYSDATE), \u0026lsquo;MONDAY\u0026rsquo;) + 12\u0026frasl;24   Premier jour du moi à minuit TRUNC(LAST_DAY(SYSDATE ) + 1)   Prochain Lundi, Mercredi et Vendredy à 9h TRUNC(LEAST(NEXT_DAY(sysdate,\u0026ldquo;MONDAY\u0026rsquo; \u0026lsquo; ),NEXT_DAY(sysdate,\u0026ldquo;WEDNESDAY\u0026rdquo;), NEXT_DAY(sysdate,\u0026ldquo;FRIDAY\u0026rdquo; ))) + (9\u0026frasl;24)    ", 
    "breadcrumb": " > serveurs > oracle > gestion-des-dates-oracle.html"
},
{
    
    "uri": "/tags/ghostview.html",
    "title": "Ghostview",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > ghostview.html"
},
{
    
    "uri": "/tags/git.html",
    "title": "Git",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > git.html"
},
{
    
    "uri": "/outils/git/git-en-ligne-de-commande.html",
    "title": "Git en ligne de commande",
    "tags": ["outils", "git"],
    "description": "",
    "content": "  Do not remove {:toc}  Voici un glossaire des lignes de commandes utile sous GIT :\nCréation d’un dépôt créez un nouveau dossier, ouvrez-le et exécutez la commande git init  pour créer un nouveau dépôt.\nCloner un dépôt créez une copie de votre dépôt local en exécutant la commande git clone /path/to/repository si vous utilisez un serveur distant, cette commande sera git clone username@host:/path/to/repository\nAjouter \u0026amp; valider Vous pouvez proposer un changement (l’ajouter à l’Index) en exécutant les commandes git add \u0026lt;filename\u0026gt; git add * C’est la première étape dans un workflow git basique. Pour valider ces changements, utilisez git commit -m \u0026quot;Message de validation\u0026quot; Le fichier est donc ajouté au HEAD, mais pas encore dans votre dépôt distant.\nAjout partiel d’un fichier git add -p \u0026lt;filename\u0026gt;\nAvec cette commande, git passe chaque block de changement du fichier en revue et demande s’il doit l\u0026rsquo;ajouter. Les options sont : * y Ajoute le bloc pour le prochain commit * n n’ajoute pas pour le prochain commit * q quitte, n’ajoute pas le bloc ni aucun des blocs suivant * a Ajoute le bloc et tous les blocs suivant * d n’ajoute ni ce bloc ni les blocs suivant * g Sélectionne le bloc auquel on veut se rendre * / Cherche un bloc * j Laisser ce bloc comme non décidé et passer au suivant non décidé * J Laisser ce bloc comme non décidé et passer au suivant * k Laisser ce bloc comme non décidé et revenir au bloc non décidé précédent * K Laisser ce bloc comme non décidé et revenir au bloc précédent * s Diviser le bloc courant en plusieurs blocs plus petit * e Éditer manuellement le bloc courant * ? Imprimer l’aide\nEnvoyer des changements Vos changements sont maintenant dans le HEAD de la copie de votre dépôt local. Pour les envoyer à votre dépôt distant, exécutez la commande git push origin master Remplacez master par la branche dans laquelle vous souhaitez envoyer vos changements.\nSi vous n’avez pas cloné votre dépôt existant et voulez le connecter à votre dépôt sur un serveur distant, vous devez l’ajouter avec git remote add origin \u0026lt;server\u0026gt; Maintenant, vous pouvez envoyer vos changements vers le serveur distant sélectionné\nBranches Créer une nouvelle branche nommée \u0026ldquo;feature_x\u0026rdquo; et passer dessus pour l’utiliser git checkout -b feature_x retourner sur la branche principale git checkout master et supprimer la branche git branch -d feature_x une branche n\u0026rsquo;est pas disponible pour les autres tant que vous ne l\u0026rsquo;aurez pas envoyée vers votre dépôt distant git push origin \u0026lt;branch\u0026gt;\nPour supprimer toutes les références distantes à des branches supprimées : git remote prune origin\nRenomer une branche git branch --move old_name new_name git checkout new_name git branch --unset-upstream git push origin :new_name git push --set-upstream origin new_name Mettre à jour \u0026amp; fusionner pour mettre à jour votre dépôt local vers les dernières validations, exécutez la commande git pull dans votre espace de travail pour récupérer et fusionner les changements distants.\nA noter que par défaut git fait un merge. Mais on peut lui demander de faire un rebase. A peine plus risqué en termes de conflicts, ça permet de garder un historique propre. A partir de la 2.9 il est possible d’utiliser en plus l’autostash  git pull --rebase --autostash.\nPour fusionner une autre branche avec la branche active (par exemple master), utilisez git merge \u0026lt;branch\u0026gt; dans les deux cas, git tente d\u0026rsquo;auto-fusionner les changements. Malheureusement, ça n’est pas toujours possible et résulte par des conflits. Vous devez alors régler ces conflits manuellement en éditant les fichiers indiqués par git. Après l\u0026rsquo;avoir fait, vous devez les marquer comme fusionnés avec git add \u0026lt;filename\u0026gt; après avoir fusionné les changements, vous pouvez en avoir un aperçu en utilisant git diff \u0026lt;source_branch\u0026gt; \u0026lt;target_branch\u0026gt;.\ntags il est recommandé de créer des tags pour les releases de programmes. C’est un concept connu, qui existe aussi dans SVN. Vous pouvez créer un tag nommé 1.0.0 en exécutant la commande git tag 1.0.0 1b2e1d63ff le 1b2e1d63ff désigne les 10 premiers caractères de l\u0026rsquo;identifiant du changement que vous voulez référencer avec ce tag. Vous pouvez obtenir cet identifiant avec git log vous pouvez utiliser moins de caractères de cet identifiant, il doit juste rester unique.\nRemplacer les changements locaux Dans le cas où vous auriez fait quelque chose de travers (ce qui bien entendu n’arrive jamais ;) vous pouvez annuler les changements locaux en utilisant cette commande git checkout -- \u0026lt;filename\u0026gt; cela remplacera les changements dans votre arbre de travail avec le dernier contenu du HEAD. Les changements déjà ajoutés à l\u0026rsquo;index, aussi bien les nouveaux fichiers, seront gardés.\nSi à la place vous voulez supprimer tous les changements et validations locaux, récupérez le dernier historique depuis le serveur et pointez la branche principale locale dessus comme ceci git fetch origin git reset --hard origin/master\nComptage des commits par utilisateur git shortlog -s -n\nConseils utiles utiliser des couleurs dans la sortie de git git config color.ui true\nafficher le journal sur une seule ligne pour chaque validation git config format.pretty oneline\nutiliser l\u0026rsquo;ajout interactif git add -i\nLiens  http://rogerdudler.github.io/git-guide/index.fr.html http://book.git-scm.com/ http://progit.org/book/ http://think-like-a-git.net/ http://help.github.com/   ", 
    "breadcrumb": " > outils > git > git-en-ligne-de-commande.html"
},
{
    
    "uri": "/outils/git/git-rerere.html",
    "title": "Git rerere",
    "tags": ["outils", "git"],
    "description": "",
    "content": " Avec git, un des truc simpa c\u0026rsquo;est le rebase, seulement voilà, avec des rebases on se retrouve souvent avec des conflits et on en vient vite à passer plus de temps à résoudre les conflits qu\u0026rsquo;a coder.\nMais il se trouve que Git propose une commande au nom improblable pour préserver notre fragile état mental, la commande rerere.\nPour activer rerere, la seule chose à faire est de l’indiquer en configuration :\ngit config --global rerere.enabled true Une fois activé, Git va se souvenir de la façon dont vous résolvez les conflits, sans votre intervention. Par exemple, avec un fichier nommé bonjour contenant sur master :\nhello ninjas  Une branche french est créée pour la version française :\nbonjour ninjas  Alors que sur master, une modification est appliquée\nhello ninjas!  Si la branche french est mergée, alors un conflit survient :\nAuto-merging bonjour CONFLICT (content): Merge conflict in bonjour Recorded preimage for 'bonjour' Automatic merge failed; fix conflicts and then commit the result.  Si l’on édite le fichier, on a bien un conflit :\n\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; HEAD hello ninjas! ======= bonjour ninjas \u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; french  Vous pouvez voir les fichiers en conflit surveillés par rerere :\n$ git rerere status bonjour Vous corrigez le conflit, pour conserver :\nbonjour ninjas!  Vous pouvez voir ce que rerere retient de votre résolution avec :\n$ git rerere diff--- a/bonjour +++ b/bonjour @@ -1,5 +1 @@ -\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; -bonjour ninjas -======= -hello ninjas! -\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; +bonjour ninjas!  Une fois terminée la résolution du conflit (add et commit), vous pouvez voir la présence d’un nouveau répertoire dans le dossier .git, nommé rr-cache, qui contient maintenant un dossier correspondant à notre résolution dans lequel un fichier conserve le conflit (preimage) et la résolution (postimage).\nMaintenant, vous vous rendez compte que vous vous préféreriez un rebase plutôt qu’un merge. Pas de problème, on reset le dernier merge :\n$ git reset --hard HEAD~1 On se place sur la branche french et on rebase.\n$ git checkout french $ git rebase master ... Falling back to patching base and 3-way merge... Auto-merging bonjour CONFLICT (content): Merge conflict in bonjour Resolved \u0026#39;bonjour\u0026#39; using previous resolution. Failed to merge in the changes. Patch failed at 0001 bonjour ninjas! Nous avons le même conflit que précédemment, mais cette fois on peut voir « Resolved bonjour using previous resolution. ». Et si nous ouvrons le fichier bonjour, le conflit a été résolu automatiquement!\nPar défaut, rerere n’ajoute pas le fichier à l’index, vous laissant le soin de vérifier la résolution et de continuer le rebase. Il est possible avec l’option ‘rerere.autoupdate’ de faire cet ajout automatiquement à l’index (je préfère personnellement laisser cette option à ‘false’ et vérifier moi-même)!\nA noter qu’il serait possible de remettre le fichier avec son conflit (si la résolution automatique ne vous convenait pas) :\n$ git checkout --conflict=merge bonjour Le fichier est alors à nouveau en conflit :\n\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; HEAD hello ninjas! ======= bonjour ninjas \u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; french  Vous pouvez re-résoudre automatiquement le conflit avec :\n$ git rerere Liens  https://hypedrivendev.wordpress.com/2013/08/30/git-rerere-ma-commande-preferee/  ", 
    "breadcrumb": " > outils > git > git-rerere.html"
},
{
    
    "uri": "/outils/git/git-un-modele-de-branches-efficace.html",
    "title": "Git un modèle de branches efficace",
    "tags": ["outils", "git", "branching"],
    "description": "",
    "content": " La gestion des branches dans Subversion ou CVS n’est pas suffisamment simple et rapide pour encourager les développeurs à s’y frotter, voire les en dissuade\nPartant de ce constat, tous les développeurs restent dans « le trunk », avec tous les inconvénients que cela peut avoir :\n Mr X commit en deux parties son code, rendant l’espace de quelques instants l’intégralité du projet instable Mr X commit une fonctionnalité en cours de développement, rendant le projet impossible à livrer tant qu’il n’aura pas terminé sa fonctionnalité Mr Y commit lui aussi une fonctionnalité en cours de développement, rendant le projet encore moins possible à livrer tant qu’il n’aura pas terminé sa fonctionnalité.  Et nous nous retrouvons avec un trunk complètement instable ou un « hotfix » devient impossible à réaliser.\nC’est là que Git intervient en proposant une gestion des branches simple et rapide.\nGit, peux-tu faire quelque chose pour nous ? Oui, il le peut, en nous permettant de respecter ce schéma facilement: {: .center-image}\nLe master {: .right-image}\nLe master correspond à la version de production : Personne ne travaille directement sur la production mais il est possible, en permanence, de créer une branche à partir du master (pour des corrections de bug urgents par exemple).\nLa branche develop La branche develop correspond à la dernière version de développement stable. Develop est la branche d’intégration, c’est à partir de cette dernière que la prochaine version de production sera créée.\nAucune fonctionnalité en cours de développement n’est envoyée directement à develop, seules les fonctionnalités terminées y sont envoyées.\nTout le monde devrait pouvoir, à moindre risque, se reposer sur la branche develop.\nBranche de nouvelle fonctionnalité (feature) Branche réalisée à partir de : develop La branche sera réintroduite dans : develop\nLorsque nous voulons créer une nouvelle fonctionnalité dans notre projet, nous allons créer une branche portant le nom de cette fonctionnalité en partant de la branche develop (qui est la branche la plus récente).\nCette branche restera ouverte tant que la fonctionnalité ne sera pas terminée.\nCréation de la branche git checkout -b nom_fonctionnalite develop Switched to a new branch \u0026#34;nom_fonctionnalite\u0026#34; Si l’on souhaite que la branche soit connue de tous (qu’elle soit présente sur le dépôt d’origine), il est possible de le faire.\nEnvoi de la branche sur l’origine git push origin nom_fonctionnalite Lorsque la fonctionnalité est terminée, nous pouvons l’inclure dans la branche develop.\nIncorporation de la branche dans develop #on va sur la branche develop pour y effectuer l\u0026#39;opération de merge git checkout develop #On demande le merge de la fonctionnalite dans develop git merge nom_fonctionnalite #On peut supprimer la branche nom_fonctionnalite devenue inutile git branch -d nom_fonctionnalite #On demande l\u0026#39;envois des modifications sur le dépôt d\u0026#39;origine git push origin develop Créer une nouvelle version (release) Branche réalisée à partir de : develop La branche sera réintroduite dans : master et develop\nLorsqu’un ensemble de fonctionnalité est terminé il est temps de livrer en production. La plus part du temps nous devons passer par une phase de recette (utilisateurs ou technique) : Cette recette sera réalisée sur cette branche.\nNous pourrons lors de cette recette corriger des bugs mineurs, peaufiner quelques détails d’interface ou simplement mettre à jour les informations de version (fichiers README, numéro de version, \u0026hellip;).\nPendant cette recette, il sera possible aux autres développeurs de continuer de travailler sur les fonctionnalités pour les versions suivantes (dans les branches de fonctionnalités).\nCréer la branche de nouvelle version #creation avec la convention release-numversion git checkout -b release-x.y develop #mettez à jour votre README avec le numéro de version git add README git commit -m \u0026#34;Version x.y\u0026#34; Une fois la recette validée par les utilisateurs et tous les correctifs mineurs intégrés, réintégrez la branche dans le master\nIntégration de la version dans le master #on se positionne sur le master pour y intégrer la branche git checkout master #Intégration de la branche avec l\u0026#39;option --no-ff pour tracer la fusion git merge --no-ff release-x.y #On tag la version git tag -a x.y # On envoie le contenu du master sur le dépôt d\u0026#39;origine # L\u0026#39;option --tags indique à git que l\u0026#39;on souhaite # envoyer l\u0026#39;information de tag git push --tags Bien sûr, il ne faut pas oublier de réintégrer dans la version develop les changements intervenus lors de la recette.\nIntégration de la version dans la branche develop git checkout develop git merge --no-ff release-x.y La branche de version n’a plus lieu d’être\nSupression de la branche de version git branch -d release-x.y Corrections de bugs en production {: .right-image}\nBranche réalisée à partir de : master La branche sera réintroduite dans : master et develop\nLorsqu’un bug critique est détecté en production et que sa correction est urgente nous aurons recours à la branche de correction à chaud.\nCette branche est réalisée à partir du master, qui rappelons le est la copie conforme de la production.\nCette branche permet d’isoler le correctif de production du cycle de développement normal du produit (réalisé dans la branche develop).\nUne fois le correctif appliqué, il sera intégré au master et à la branche develop. {: .clearfix}\nCréation de la branche correctif (hotfix) #création de la branche hotfix à partir du master git checkout -b hotfix-x.y.z master #Mise à jour du numéro de version # ... édition du readme \u0026amp; autres # ... (processus identique à celui de la release)  # Validation de la mise à jour de version git commit -a -m \u0026#34;Correctif x.y.z\u0026#34; Le travail de correction est effectué…\nEnvois des modifications git commit -m \u0026#34;Correctif du problème blabla signalé par Bob\u0026#34; Et le travail d’introduction classique aux branches commence :\nIntégration du Hotfix en production git checkout master git merge --no-ff hotfix-x.y.z git tag -a x.y.z git push --tags Intégration du Hotfix dans la branche develop git checkout develop git merge --no-ff hotfix-x.y.z Supression de la branche hotfix devenue inutile git branch -d hotfix-1.2.1 Conclusions La puissance de Git réside dans les nouvelles possibilités qui nous sont offertes. Migrer vers Git implique de changer ses méthodes de travail pour en bénéficier.\nGit Flow Git flow est un outil qui permet de simplifier les commandes git pour utiliser ce modèle de branches.\nLiens  http://www.croes.org/gerald/blog/git-modele-de-branche-efficace/649/ http://nvie.com/posts/a-successful-git-branching-model/  ", 
    "breadcrumb": " > outils > git > git-un-modele-de-branches-efficace.html"
},
{
    
    "uri": "/tags/gitlab.html",
    "title": "Gitlab",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > gitlab.html"
},
{
    
    "uri": "/tags/gnome.html",
    "title": "Gnome",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > gnome.html"
},
{
    
    "uri": "/tags/gollum.html",
    "title": "Gollum",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > gollum.html"
},
{
    
    "uri": "/outils/gollum-at-startup.html",
    "title": "Gollum at startup",
    "tags": ["outils", "gollum"],
    "description": "",
    "content": "Normalement ici gollum est installé, reste plus qu\u0026rsquo;a le faire démarrer en même temps que la machine. Sur Debian c\u0026rsquo;est systemd le nouveau gestionnaire de service donc voici un script systemd à placer dans : ~/.config/systemd/user/gollum.service\n[Unit] Description=Gollum server After=syslog.target Wants=network-online.target After=network-online.target [Service] Environment=\u0026#34;WIKI_DIR=/home/${USER}/workspace/marthym-wiki\u0026#34; ExecStart=/usr/local/bin/gollum --no-edit --show-all ${WIKI_DIR} ExecStartPost=/usr/bin/git --git-dir=${WIKI_DIR}/.git --work-tree=${WIKI_DIR} pull # Give a reasonable amount of time for the server to start up/shut down TimeoutSec=10 [Install] WantedBy=multi-user.targetsystemctl --user daemon-reload systemctl --user start gollum On note le ExecStartPre qui va permettre de synchroniser le wiki au démarrage.\nAprès on peut aussi mettre en place un chron pour l\u0026rsquo;actualiser régulièrement mais dans le cas d\u0026rsquo;un wiki perso c\u0026rsquo;est pas forcément nécessaire.\nOn lance gollum avec --no-edit pour éviter que les personnes sur le même réseau ne modifient le wiki.\n", 
    "breadcrumb": " > outils > gollum-at-startup.html"
},
{
    
    "uri": "/tags/graphic.html",
    "title": "Graphic",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > graphic.html"
},
{
    
    "uri": "/tags/gs.html",
    "title": "Gs",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > gs.html"
},
{
    
    "uri": "/tags/gwt.html",
    "title": "Gwt",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > gwt.html"
},
{
    
    "uri": "/serveurs/jboss/gerer-l-ordre-de-deploiement-dans-jboss.html",
    "title": "Gérer l’ordre de déploiement dans JBoss",
    "tags": ["server", "jboss", "sql", "database"],
    "description": "",
    "content": "C\u0026rsquo;est un question qui peut paraître inutile mais dans au moins un cas c\u0026rsquo;est crucial :p\nDans le cas de la création d\u0026rsquo;un WS Axis2, on utilise une classe AxisServlet qui se trouve être déployé dans l\u0026rsquo;EAR de CameleonEdge ! Dans ce cas, il est important que la webapp que l\u0026rsquo;on crée avec le WS soit déployé après l\u0026rsquo;EAR de Cameleon. Or, par défaut dans JBoss, les WAR sont quoi qu\u0026rsquo;il arrivent déployer avant les EAR, donc problème \u0026hellip;\nLa solution est de créer dans le répertoire deploy du server JBoss un sous-répertoire \u0026ldquo;deploy.last\u0026rdquo; (en fait c\u0026rsquo;est surtout le .last qui compte) et d\u0026rsquo;y mettre les webapps a déployer en dernier ! La solution a été testé pour CrystalRock et fonctionne très bien !\nOn notera tout de même qu\u0026rsquo;il est possible d\u0026rsquo;influencer l\u0026rsquo;ordre des WAR en les préfixant avec des chiffres (ex : 1_MonAppliWebService.war, \u0026hellip;) dans ce cas, les WAR sans préfixe seront déployer puis les WAR avec préfixe dans l\u0026rsquo;ordre de leur préfixe.\nPour finir avec cette histoire d\u0026rsquo;ordre, il est possible de modifier l\u0026rsquo;ordre de déploiement par défaut de JBoss en modifiant le fichier jboss-4.2.0.GA\\server\\cameleon\\conf\\xmdesc\\org.jboss.deployment.MainDeployer-xmbean.xml comme suit :\n\u0026lt;value value=\u0026#34;250:.rar,300:-ds.xml,400:.jar,450:cameleon.ear,500:.war,550:.jse,650:.ear,800:.bsh\u0026#34;/\u0026gt; au lieu de\n\u0026lt;value value=\u0026#34;250:.rar,300:-ds.xml,400:.jar,500:.war,550:.jse,650:.ear,800:.bsh\u0026#34;/\u0026gt; C\u0026rsquo;est pas la meilleure solution, toucher à la configuration de JBoss peut entraîner des problèmes si plus tard la R\u0026amp;D décide de changer l\u0026rsquo;ordre de déploiement ou de se servir de cet ordre par défaut pour je ne sais qu\u0026rsquo;elle fonctionnalité magique \u0026hellip;\n", 
    "breadcrumb": " > serveurs > jboss > gerer-l-ordre-de-deploiement-dans-jboss.html"
},
{
    
    "uri": "/serveurs/http/https-securise-avec-nginx.html",
    "title": "HTTPS Securisé avec Nginx",
    "tags": ["server", "http", "nginx", "security"],
    "description": "",
    "content": " Générer des certificats SSL correctement configuré n\u0026rsquo;est pas aussi facile qu\u0026rsquo;il n\u0026rsquo;y parait !\nGénération des certificats Creation de l\u0026rsquo;autorité de certification (CA) On commence par créer la clé, en 2048 sinon c\u0026rsquo;est pas suffisamment sécurisé :\nopenssl genrsa -des3 -out dumydomain-ca.key 2048 Puis on crée le certificat de la CA :\nopenssl req -new -x509 -days 3650 -key dumydomain-ca.key -out dumydomain-ca.crt Valable 10 ans.\nCréation du certificat pour notre serveur Dans cette procédure, le FDQN doit être exactement le même que le nom de domaine du serveur. Il est possible d\u0026rsquo;utiliser des wildcard pour avoir un certificat multi-domaine : *.dumydomain.fr\nComme pour la CA on commence par créer la clé :\nopenssl genrsa -des3 -out dumydomain.fr.key 1024 Ensuite on crée la demande ce certificat :\nopenssl req -new -nodes -newkey rsa:2048 -keyout dumydomain.fr.key -out dumydomain.fr.csr -days 3650 Enfin on crée le certificat grace à l\u0026rsquo;autorité et la demande précédemment généré :\nopenssl x509 -req -in dumydomain.fr.csr -out dumydomain.fr.crt -sha256 -CA dumydomain-ca.crt -CAkey dumydomain-ca.key -CAcreateserial -days 365 Il est important d\u0026rsquo;utiliser SHA256 et non SHA1 car l\u0026rsquo;algo SHA1 n\u0026rsquo;est pas considéré comme suffisamment sur.\nConfiguration Nginx Diffie Hellman Parameters Il est nécessaire, pour éviter certaines failles de redéfinir les paramètres DH sur 2048 (cf. https://weakdh.org/sysadmin.html\nopenssl dhparam -out dhparams.pem 2048 Déplacer le fichier obtenu dans /etc/nginx/ssl.\nConfiguration SSL Déplacer les fichier de certificats et de clé dans le répertoire /etc/nginx/ssl.\nDans /etc/nginx/conf.d ajouter un fichier ssl.conf avec le contenu suivant :\nssl_certificate /etc/nginx/ssl/dumydomain.fr.crt; ssl_certificate_key /etc/nginx/ssl/dumydomain.fr.key; ssl_session_timeout 5m; ssl_session_cache shared:SSL:10m; ssl_protocols SSLv3 TLSv1 TLSv1.1 TLSv1.2; ssl_ciphers \u0026#39;ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:DHE-DSS-AES128-GCM-SHA256:kEDH+AESGCM:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA:ECDHE-ECDSA-AES128-SHA:ECDHE-RSA-AES256-SHA384:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA:ECDHE-ECDSA-AES256-SHA:DHE-RSA-AES128-SHA256:DHE-RSA-AES128-SHA:DHE-DSS-AES128-SHA256:DHE-RSA-AES256-SHA256:DHE-DSS-AES256-SHA:DHE-RSA-AES256-SHA:AES128-GCM-SHA256:AES256-GCM-SHA384:AES128-SHA256:AES256-SHA256:AES128-SHA:AES256-SHA:AES:CAMELLIA:DES-CBC3-SHA:!aNULL:!eNULL:!EXPORT:!DES:!RC4:!MD5:!PSK:!aECDH:!EDH-DSS-DES-CBC3-SHA:!EDH-RSA-DES-CBC3-SHA:!KRB5-DES-CBC3-SHA\u0026#39;; ssl_dhparam /etc/nginx/ssl/dhparams.pem; Il ne reste plus qu\u0026rsquo;a activer SSL dans la configuration du server et de tester via SSLabs\n", 
    "breadcrumb": " > serveurs > http > https-securise-avec-nginx.html"
},
{
    
    "uri": "/tags/hack.html",
    "title": "Hack",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > hack.html"
},
{
    
    "uri": "/development/hacks-css.html",
    "title": "Hack CSS",
    "tags": ["development", "css", "hack", "front"],
    "description": "",
    "content": " Pour ceux que le savent pas, un hack CSS consiste à exploiter un bug ou plutôt un manque fonctionnel afin d\u0026rsquo;écrire un seul et unique fichier CSS compatible avec un maximum de navigateur. Par exemple, un hack très répandu mais qui ne fonctionne plus maintenant était la directive !IMPORTANT dans les propriétés CSS.\nComment effacer le texte d\u0026rsquo;un bouton .button { background-image: url(\u0026#39;my-picture-with-text.jpg\u0026#39;); text-indent: -9999px; } Sauf que ça ne fonctionne pas sur IE. Une solution est :\n.button { background-image: url(\u0026#39;my-picture-with-text.jpg\u0026#39;); line-height: 999px; /* Set it higher than your image height */ overflow: hidden; /* Hide the text */ font-size: 0; /* FF2 doesn’t like the above */ } Internet Explorer VS Firefox 3 Voici un exemple d\u0026rsquo;un hack qui différentie IE de FF 3.0 et de FF 3.5 :\n.close { margin-top: -5px; } .close, #ie8#fix { margin-top: -32px; } body:nth-of-type(1) .close { margin-top: -22px; } Ces trois déclaration de margin porte toutes sur la même classe mais sont interprété différemment selon les navigateurs.\n.close { margin-top: -5px; } Celle ci est interprété par tout les navigateurs, c\u0026rsquo;est en quelque sorte la valeur par défaut.\n.close, #ie8#fix { margin-top: -32px; } IE, quelque soit la version n\u0026rsquo;interprètera pas cette écriture, il ne sait lire #ie8#fix du coup il ignore l\u0026rsquo;instruction complète, les autres navigateurs lisent cette déclaration correctement et en tiennent compte.\nbody:nth-of-type(1) .close { margin-top: -22px; } Cette déclaration elle n\u0026rsquo;est lut que par Firefox 3.5. En effet le moteur Gecko ayant changé, on constate des différences entre FF 3.0 et 3.5, cette écriture permet de rattraper le coup. On utilise là une des nouvelles fonctionnalité de FF qui permet l\u0026rsquo;utilisation de pseudo-classe via la notation nth-.\nIE7 VS IE8 .close { margin-top: -5px; /* Lu par IE7 et IE8 */ *margin-top: 0px; /* Lu uniquement par IE7 */ } Cette écriture permet de différencier IE7 et IE8. La première ligne sera lu par tout les navigateurs, la seconde n\u0026rsquo;est lu que par IE7.\nAutres Astuce Il est possible de jouer sur les commentaires. En effet, les navigateurs récents interprète le commentaire // ce qui n\u0026rsquo;est pas le cas des navigateurs plus anciens qui n\u0026rsquo;interprètent que les commentaires /* */.\n.close { margin-top: -5px; //margin-top: 0px; } Du coup, la deuxième ligne sera interprété normalement par les vieux navigateurs mais sera interprété comme un commentaire par les navigateurs récent.\n", 
    "breadcrumb": " > development > hacks-css.html"
},
{
    
    "uri": "/tags/hardware.html",
    "title": "Hardware",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > hardware.html"
},
{
    
    "uri": "/tags/hdd.html",
    "title": "Hdd",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > hdd.html"
},
{
    
    "uri": "/tags/hdmi.html",
    "title": "Hdmi",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > hdmi.html"
},
{
    
    "uri": "/tags/http.html",
    "title": "Http",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > http.html"
},
{
    
    "uri": "/tags/https.html",
    "title": "Https",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > https.html"
},
{
    
    "uri": "/tags/hubic.html",
    "title": "Hubic",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > hubic.html"
},
{
    
    "uri": "/serveurs/http/hotes-virtuels-ssl-multiple.html",
    "title": "Hôtes virtuels SSL multiples",
    "tags": ["server", "http", "apache", "security", "ssl"],
    "description": "",
    "content": " Alors on trouve sur internet une flopper de tuto pour configurer sous Apache des VirtualHost SSL multiple. On a beau les lire et les re-lire, essayer des centaines de combinaisons pour chaque fichier de configuration, on en vient toujours au même résultat : C\u0026rsquo;est le premier vhost qui est retourné !\nExplication technique Ce que la plus part des tuto ne dit pas c\u0026rsquo;est déjà qu\u0026rsquo;il faut avoir des version d\u0026rsquo;Apache et de OpenSSL très récente pour que ça ai une chance de fonctionné. Mais même avec ça, je n\u0026rsquo;ai pas réussi à le faire fonctionner.\nLa raison technique a cette impossibilité est la suivante :\nThe reason is that the SSL protocol is a separate layer which encapsulates the HTTP protocol. So the SSL session is a separate transaction, that takes place before the HTTP session has begun. The server receives an SSL request on IP address X and port Y (usually 443). Since the SSL request did not contain any Host: field, the server had no way to decide which SSL virtual host to use. Usually, it just used the first one it found which matched the port and IP address specified.  Du coup, quand ça ne fonctionne pas, deux choix reste possible pour contourner le problème :\n Utiliser plusieurs adresse IP Utiliser plusieurs ports  Solution La solution pas trop compliqué est donc d\u0026rsquo;utiliser plusieurs port, mais ça demande d\u0026rsquo;avoir un frontal qui va faire la redirection vers les différents port selon le nom de domaine demandé. Mais dans le cas des phases de développement est pas toujours pratique ni facile d\u0026rsquo;avoir ce genre d’architecture en place pour de simple développements.\nDu coup on va configurer Apache sur plusieurs adresse IP. Par contre cette solution ne fonctionne un serveur Apache Windows (ou du moins je sais pas comment) car il est nécessaire de pouvoir configurer plusieurs adresse IP sur une même interface réseau.\nConfiguration système Suivre le tuto pour [[Avoir plusieurs adresses IP sur la même interface|Avoir-plusieurs-adresses-IP-sur-la-meme-interface]] \u0026hellip;\nConfiguration Apache Fichier d\u0026rsquo;écoute Ensuite, une fois que l\u0026rsquo;on a plusieurs adresse IP, il faut demander à Apache d\u0026rsquo;écouter sur ces adresse. Dans /etc/apache2/port.conf\n\u0026lt;IfModule mod_ssl.c\u0026gt; # If you add NameVirtualHost *:443 here, you will also have to change # the VirtualHost statement in /etc/apache2/sites-available/default-ssl # to \u0026lt;VirtualHost *:443\u0026gt; # Server Name Indication for SSL named virtual hosts is currently not # supported by MSIE on Windows XP. NameVirtualHost 172.16.139.21:443 NameVirtualHost 172.16.139.22:443 Listen 443 \u0026lt;/IfModule\u0026gt; On n\u0026rsquo;utilise pas la notation *:443 car il nous demanderait lors du démarrage un vhost par défaut pour *:443 et ce n\u0026rsquo;est pas ce que l\u0026rsquo;on veut faire.\nFichiers des hôtes virtuel Ensuite dans /etc/apache2/sites-available/ on va créer un premier fichier pour le premier serveur virtuel :\n\u0026lt;IfModule mod_ssl.c\u0026gt; \u0026lt;VirtualHost 172.16.139.21:443\u0026gt; ServerAdmin webmaster@dev.local ServerName mnt.dev.local DocumentRoot /var/www/dev/mnt/public \u0026lt;Directory /\u0026gt; Options FollowSymLinks AllowOverride None \u0026lt;/Directory\u0026gt; ... \u0026lt;/VirtualHost\u0026gt; \u0026lt;/IfModule\u0026gt; Et un fichier deuxième fichier pour le deuxième hôte :\n\u0026lt;IfModule mod_ssl.c\u0026gt; \u0026lt;VirtualHost 172.16.139.22:443\u0026gt; ServerAdmin webmaster@dev.local ServerName www.recette.local ServerAlias mnt.recette.local mgp.recette.local DocumentRoot /var/www/recette/public \u0026lt;Directory /\u0026gt; Options FollowSymLinks AllowOverride None \u0026lt;/Directory\u0026gt; ... \u0026lt;/VirtualHost\u0026gt; \u0026lt;/IfModule\u0026gt; Je ne m\u0026rsquo;attarde pas sur les différentes options pour le SSL ou les droits des répertoires, le sujet du tuto n\u0026rsquo;est pas là. Le point d’intérêt se trouve dans les premières lignes de chaque fichier. On remarque que l\u0026rsquo;on utilise les adresse IP au lieu de \u0026ldquo;*\u0026ldquo;. Autre point intéressant dans le deuxième fichier on définit des alias qui vont permettre d\u0026rsquo;arriver sur le même serveur SSL avec des noms de sous-domaine différent.\nPour finir Pour finir, on active les deux sites :\na2ensite mnt.dev.local-ssl a2ensite recette.local-ssl Puis on redémarre le serveur Apache :\nservice apache2 reload Liens  https://httpd.apache.org/docs/2.2/ssl/ssl_faq.html#vhosts2  ", 
    "breadcrumb": " > serveurs > http > hotes-virtuels-ssl-multiple.html"
},
{
    
    "uri": "/tags/ie.html",
    "title": "Ie",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > ie.html"
},
{
    
    "uri": "/development/gwt/illegalstateexception-simplepanel.html",
    "title": "IllegalStateException: SimplePanel",
    "tags": ["development", "gwt"],
    "description": "",
    "content": "Encore un truc qui m\u0026rsquo;a pris la tête ! Après l\u0026rsquo;écriture d\u0026rsquo;une page uiBinder, je me retrouve avec cette error :\nIllegalStateException: SimplePanel can only contain one child widget  Alors que nulle part dans mon code je n\u0026rsquo;utilise de SimplePanel ! Après recherche, il s\u0026rsquo;avère que le FormPanel et le DecoratorPanel dérivent tout deux du SimplePanel qui n\u0026rsquo;accepte pas plus d\u0026rsquo;un seul Widget à l\u0026rsquo;intérieur. Tout s\u0026rsquo;illumine, j\u0026rsquo;ai un FormPanel avec plusieurs widget dedans.\nDommage que j\u0026rsquo;ai pas eu l\u0026rsquo;erreur à la compil ??\n", 
    "breadcrumb": " > development > gwt > illegalstateexception-simplepanel.html"
},
{
    
    "uri": "/outils/docker/inetaddress-doesnt-resolve-ip-on-alpine-docker-container.html",
    "title": "InetAddress does’nt resolve ip on alpine docker container",
    "tags": ["development", "docker", "error"],
    "description": "",
    "content": " C\u0026rsquo;est un problème qu\u0026rsquo;on a rencontré quand on a voulu réduire la taille des docker en utilisant l\u0026rsquo;image Alpine comme image de base.\nSymptome Exception in thread \u0026quot;main\u0026quot; java.net.UnknownHostException: mysql: unknown error at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method) at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:928) at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1323) at java.net.InetAddress.getAllByName0(InetAddress.java:1276) at java.net.InetAddress.getAllByName(InetAddress.java:1192) at java.net.InetAddress.getAllByName(InetAddress.java:1126) at java.net.InetAddress.getByName(InetAddress.java:1076) at SomaDBTest.main(SomaDBTest.java:52)  Solution En gros l\u0026rsquo;image Alpine veut par défaut résoudre les noms de machine pas DNS en priorité au lieu d\u0026rsquo;utiliser d\u0026rsquo;abord les fichiers (hosts).\nRUN echo \u0026#39;hosts: files mdns4_minimal [NOTFOUND=return] dns mdns4\u0026#39; \u0026gt;\u0026gt; /etc/nsswitch.conf", 
    "breadcrumb": " > outils > docker > inetaddress-doesnt-resolve-ip-on-alpine-docker-container.html"
},
{
    
    "uri": "/tags/inode.html",
    "title": "Inode",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > inode.html"
},
{
    
    "uri": "/development/java/installer-java-sun-jdk-sous-debian.html",
    "title": "Installer Java Sun JDK sous Debian",
    "tags": ["development", "java", "debian"],
    "description": "",
    "content": " C\u0026rsquo;est très simple mais faut avoir les bons dépôt activé.\nVérifier la liste de ces dépôts Éditer le fichier /etc/apt/sources.list et faites en sorte qu\u0026rsquo;il ressemble à ça :\n# deb cdrom:[Debian GNU/Linux testing _Wheezy_ - Official Snapshot i386 CD Binary-1 20111205-04:44]/ wheezy main deb http://ftp.fr.debian.org/debian/ testing main contrib non-free deb-src http://ftp.fr.debian.org/debian/ testing main contrib non-free deb http://security.debian.org/ testing/updates main contrib non-free deb-src http://security.debian.org/ testing/updates main contrib non-free deb http://ftp.fr.debian.org/debian/ stable main contrib non-free deb-src http://ftp.fr.debian.org/debian/ stable main contrib non-free deb http://security.debian.org/ stable/updates main contrib non-free deb-src http://security.debian.org/ stable/updates main contrib non-free  Ce qui est important c\u0026rsquo;est d\u0026rsquo;avoir \u0026ldquo;contrib non-free\u0026rdquo; dans ses dépôts car le Java Sun se trouve dans les \u0026ldquo;non-free\u0026rdquo;, sinon c\u0026rsquo;est |OpenJDK qui est dans les \u0026ldquo;main\u0026rdquo; mais qui peut poser des soucis.\nAttention, les lignes \u0026ldquo;testing\u0026rdquo; ne doivent pas être là si vous utiliser un Debian Stable ! Par contre pour une Debian en Testing, il faut les deux, donc rajouter les ligne \u0026ldquo;stable\u0026rdquo; qui n\u0026rsquo;y sont pas par défaut.\nInstaller Java apt-get install sun-java6-jdk Note que si vous n\u0026rsquo;avez besoin que de JRE la procédure est la même mais le paquet est sun-java6-jre. Le paquet JDK contient la JRE, il n\u0026rsquo;est donc pas nécessaire d\u0026rsquo;installer les deux.\nInstaller l\u0026rsquo;alternative [[include:../linux/Comment installer une alternative]]\n", 
    "breadcrumb": " > development > java > installer-java-sun-jdk-sous-debian.html"
},
{
    
    "uri": "/outils/vmware/installer-vmware-tools-sur-un-guest-debian.html",
    "title": "Installer VMWare Tools sur un guest Debian",
    "tags": ["outils", "vmware", "debian"],
    "description": "",
    "content": " Les VMWare Tools ne sont pas obligatoire sur les guests mais ils permettent de faire pas mal de truc comme gérer proprement la sourie si vous avez une interface graphique, activer les répertoires partagés avec l\u0026rsquo;hôte ou réduire le vmdk de votre VM après y avoir fait le ménage.\nLes étapes suivantes nécessites d\u0026rsquo;être root sur la machine. Pour ça soit\nsudo -i soit\nsu - Installation des paquets pré-requis Les VMTools se compilent sur la machine, il faut donc installer au préalables les outils de compil sur la machine :\napt-get update apt-get upgrade Ceci met à jour les dépôts et les paquets sur la machine.\napt-get install gcc linux-headers-`uname -r` libglib2.0-0 Validez que vous souhaitez continuer \u0026hellip; Cela installe le compilateur, ses dépendances ainsi que les entêtes correspondant au noyaux de la machine.\nRécupérer les fichiers d\u0026rsquo;installation Il faut maintenant récupérer les scripts d\u0026rsquo;installation, pour ça, une fois la machine démarré, au niveau de VMPlayer aller dans \u0026ldquo;Virtual Machine -\u0026gt; Install VMWare Tools \u0026hellip;\u0026rdquo; et cliquer sur \u0026ldquo;Install\u0026rdquo;. Contrairement à Windows, cette action ne lance pas l\u0026rsquo;installeur, ça ne fait que simuler l\u0026rsquo;insertion d\u0026rsquo;un CD dans le lecteur de la machine.\nIl faut maintenant monter le lecteur :\nmount /media/cdrom0 Puis copier le fichier d\u0026rsquo;installation sur la VM :\ncp /media/cdrom0/VMwareTools-*.tar.gz ./ tar zxvf VMwareTools-*.tar.gz Les fichiers sont maintenant extrait dans \u0026ldquo;vmware-tools-distrib\u0026rdquo;.\nLancer l\u0026rsquo;installation Lancez maintenant l\u0026rsquo;installeur :\nvmware-tools-distrib/vmware-install.pl Là ya maintenant un série de question toutes plus explicites les unes que les autres ! On fait entrer à chaque fois sauf si on sait ce qu\u0026rsquo;on fait et qu\u0026rsquo;on veut virer des options par défaut. Si tout s\u0026rsquo;est bien passé vous devriez avoir ça à la fin\nEnjoy, --the VMware team Maintenant il suffit de redémarrer la machine :\nshutdown -r now En cas d\u0026rsquo;erreur Normalement les endroits où ça peut merder c\u0026rsquo;est là :\nSearching for GCC...  ou là :\nSearching for a valid kernel header path...  C\u0026rsquo;est en général que vous avez mal fait les étapes d\u0026rsquo;installation des pré-requis ! Il vous manque des paquets, refaite ces étapes.\n", 
    "breadcrumb": " > outils > vmware > installer-vmware-tools-sur-un-guest-debian.html"
},
{
    
    "uri": "/linux/administration/installer-backup-sur-jail-bsd.html",
    "title": "Installer backup sur jail BSD",
    "tags": ["linux", "sysadmin", "bsd", "jail", "hubic", "backup"],
    "description": "",
    "content": "Une fois la jail créée, pour que les scripts de backup fonctionnent il faut install bash et hubic.py. C\u0026rsquo;est pas toujours intuitif alors voilà ce que j\u0026rsquo;ai fais la dernière fois :\npkg install bash python2.7 -m ensurepip pip install --upgrade pip wget https://raw.githubusercontent.com/puzzle1536/hubic-wrapper-to-swift/master/requirements.txt pip install -r requirements.txt wget https://raw.githubusercontent.com/puzzle1536/hubic-wrapper-to-swift/master/hubic.py chmod +x hubic.py ln -s /usr/local/bin/python2.7 /usr/bin/python pkg install gnupg En plus de ça, comme expliqué lors de l\u0026rsquo;installation de bash, il faut ajouter un script postinit à FreeNAS pour qu\u0026rsquo;il monte la partition fdescfs avec la commande mount -t fdescfs null /mnt/storage/jails/\u0026lt;nom de la jail\u0026gt;/dev/fd\n", 
    "breadcrumb": " > linux > administration > installer-backup-sur-jail-bsd.html"
},
{
    
    "uri": "/linux/administration/installer-une-imprimante-ipp.html",
    "title": "Installer une imprimante IPP",
    "tags": ["linux", "sysadmin ipp", "printer"],
    "description": "",
    "content": " Il s\u0026rsquo;agit des imprimantes IP comme par exemple la Canon IR6570. Par défaut, dans l\u0026rsquo;interface d\u0026rsquo;ajout cette possibilité n\u0026rsquo;est pas offerte (pour l\u0026rsquo;instant). Il faudra donc passer par l\u0026rsquo;interface web de CUPS.\nNormalement CUPS est installé par défaut sur les distrib Linux.\nAjouter l\u0026rsquo;imprimante  Via le navigateur, se rendre à l\u0026rsquo;adresse http://localhost:631/ Allez dans l\u0026rsquo;onglet administration. Si il vous est demandé un login/password, ajoutez votre utilisateur au groupe lpadmin :  usermod -a -G lpadmin monuser  Cliquez sur \u0026ldquo;Ajouter une imprimante\u0026rdquo; Dans la liste choisir \u0026ldquo;Internet Printing Protocol (ipp)\u0026rdquo; Pour Connexion, renseignez  socket://ip-imprimante  puis Continuer Renseignez les information sur l\u0026rsquo;imprimante  Sélectionnez le driver de l\u0026rsquo;imprimante, dans le cas de l\u0026rsquo;IR6570, la marque est \u0026ldquo;Canon\u0026rdquo; et le driver \u0026ldquo;Canon imageRunner 6570 Foomatic/pxlmono (recommended) (en)\u0026rdquo; Afin cliquez sur \u0026ldquo;Ajouter une imprimante\u0026rdquo;   ", 
    "breadcrumb": " > linux > administration > installer-une-imprimante-ipp.html"
},
{
    
    "uri": "/tags/intellij.html",
    "title": "Intellij",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > intellij.html"
},
{
    
    "uri": "/tags/interface.html",
    "title": "Interface",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > interface.html"
},
{
    
    "uri": "/outils/git/introduction-a-git.html",
    "title": "Introduction à GIT",
    "tags": ["outils", "git"],
    "description": "",
    "content": " Je retranscrit ici un article très intéressant trouvé sur internet à propos de GIT.\nMigrer de Subversion (ou CVS) vers Git ne se suffit pas en soit pour profiter de ce qui fait de Git… Git.\nGit connait un succès grandissant pour de nombreuses raisons, dont :\n La possibilité de travailler hors ligne La possibilité de définir plusieurs dépôts distants Github L’extrême facilité et rapidité avec laquelle il est possible de gérer des branches L’extrême facilité et rapidité avec laquelle il est possible de gérer des branches Les deux derniers points Surtout les trois derniers points  Après quelques rappels indispensables, nous allons nous concentrer sur le système de branches et proposer un modèle « prêt à l’emploi », largement inspiré de A successul Git branch model.\nQuelques rappels Récupérer un dépôt Git (équivalent de svn checkout) Git est un système décentralisé, c’est à dire que plusieurs dépôts peuvent co-exister.\nLorsque nous souhaitons récupérer les sources d’un dépôt pour les placer sur notre disque dur, nous allons cloner le dépôt dans son ensemble : avec l’historique de toutes les révisions, toutes les branches, tous les tags, \u0026hellip;\nExemple de clone du dépôt du projet « hello-world »\ngit clone git://github.com/geraldcroes/hello-world.git #Nous pouvons maintenant consulter, hors ligne, # l\u0026#39;intégralité de l\u0026#39;historique et des branches du dépôt Modfication, Ajout puis Commit Dans un schéma SVN classique, un fichier peut avoir 3 états :\n Non versionné Modifié A jour  Dans un schéma GIT, un fichier dispose d’un quatrième état :\n En zone de transit (staging area)  Ce nouvel état indique que le fichier a été sélectionné pour le prochain décollage (commit), ajouté à l’index, mais pour autant encore non validé.\nCe petit détail fondamental permet au développeur de se concentrer à chaque commit sur ce qu’il souhaite envoyer vers le dépôt : Terminée la sélection laborieuse de tous les fichiers à envoyer, terminé les commits successifs pour une même fonctionnalité.\nVoyons en détail comment cela se passe\n#modification du fichier1.php, pour fonction 1 #modification de fichier2.php, changement de couleur du titre #création de fichier3.php, pour fonction 1 #modification de fichier4.php, pour fonction 1  #La fonction 1 est terminée, on veut la valider : git add fichier1.php git add fichier3.php git add fichier4.php #on commit la fonctionnalité 1 git commit -m \u0026#34;Fonction 1 terminée\u0026#34; #on en profite pour envoyer la micro modification sur le titre git add fichier2.php git commit -m \u0026#34;Changement de couleur du titre\u0026#34; Note : L’envoi des fichiers (commit) à ce stade est local =\u0026gt; Aucun envois vers le dépôt d’origine n’est effectué. Le commit est donc une opération instantanée.\nGit encourage à commiter souvent. Plus vous commiterez souvent, plus vous profiterez de la puissance de git.\nEnvoyer les fichiers vers le dépôt d’origine Tant que vous ne le demandez pas, vos modifications (ajouts ou commits) ne sont pas envoyées vers le dépôt d’origine et ne sont donc visibles que par vous (raison de plus pour commiter souvent).\nPour envoyer vos modifications locales vers le dépôt d’origine, il vous faut effectuer un « push ».\ngit push # les commits sont maintenant envoyés # vers le dépôt d\u0026#39;origine Récupérer les modifications effectuées par d’autres depuis le dépôt d’origine Ici, aucune surprise, on retrouve un équivalent au « update » de Subversion : le pull.\ngit pull # les modifications présentes sur le dépôt distant # sont récupérées ", 
    "breadcrumb": " > outils > git > introduction-a-git.html"
},
{
    
    "uri": "/tags/iptables.html",
    "title": "Iptables",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > iptables.html"
},
{
    
    "uri": "/tags/ivy.html",
    "title": "Ivy",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > ivy.html"
},
{
    
    "uri": "/tags/jail.html",
    "title": "Jail",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > jail.html"
},
{
    
    "uri": "/tags/java.html",
    "title": "Java",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > java.html"
},
{
    
    "uri": "/tags/javascript.html",
    "title": "Javascript",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > javascript.html"
},
{
    
    "uri": "/tags/jboss.html",
    "title": "Jboss",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > jboss.html"
},
{
    
    "uri": "/tags/jdbc.html",
    "title": "Jdbc",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > jdbc.html"
},
{
    
    "uri": "/tags/jdk.html",
    "title": "Jdk",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > jdk.html"
},
{
    
    "uri": "/tags/jekyll.html",
    "title": "Jekyll",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > jekyll.html"
},
{
    
    "uri": "/outils/jekyll.html",
    "title": "Jekyll",
    "tags": ["outils", "jekyll"],
    "description": "",
    "content": " Quelques trucs en vrac sur Jekyll\nYou have already activated package version, but your Gemfile requires package oldversion. C’est un message bien reloux, qui m’a bien fait galérer mais en fait c’est rien du tout à régler. Mais sur internet on a droit à tout et n’importe quoi comme réponses.\nAu final, il suffit de virer le Gemfile.lock et voilà c’est réglé !\n", 
    "breadcrumb": " > outils > jekyll.html"
},
{
    
    "uri": "/tags/jigsaw.html",
    "title": "Jigsaw",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > jigsaw.html"
},
{
    
    "uri": "/tags/jinja.html",
    "title": "Jinja",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > jinja.html"
},
{
    
    "uri": "/tags/jmx.html",
    "title": "Jmx",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > jmx.html"
},
{
    
    "uri": "/tags/jndi.html",
    "title": "Jndi",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > jndi.html"
},
{
    
    "uri": "/tags/join.html",
    "title": "Join",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > join.html"
},
{
    
    "uri": "/tags/junit.html",
    "title": "Junit",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > junit.html"
},
{
    
    "uri": "/tags/kodi.html",
    "title": "Kodi",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > kodi.html"
},
{
    
    "uri": "/development/java/start-neo4j-impermanent-database-+-rest-server-on-random-port-for-test.html",
    "title": "Lancer Neo4j Impermanent Database + REST Server sur un port aléatoire",
    "tags": ["development", "java", "neo4j", "database", "test"],
    "description": "",
    "content": "Pour des tests par exemple, vu que la licence Neo4j ne permet que l\u0026rsquo;utilisation des API REST, on peut avoir besoin lors de test de lancer un server éphémaire sur lequel brancher les jeux de test. Cela se fait en deux étapes : * Lancement du serveur neo4j * Lancement de la surcouche REST\ndb = new TestGraphDatabaseFactory().newImpermanentDatabase(); boolean available = db.isAvailable(5000); assert available; int start = -1; Random random = new Random(); while (start != 0) { port = RANDOM_PORTS_LOWER_BOUND + random.nextInt(RANDOM_PORTS_COUNT); try { GraphDatabaseAPI api = (GraphDatabaseAPI) db; ServerConfigurator config = new ServerConfigurator(api); config.configuration().addProperty(Configurator.WEBSERVER_ADDRESS_PROPERTY_KEY, NEO4J_EMBEDDED_HOST); config.configuration().addProperty(Configurator.WEBSERVER_PORT_PROPERTY_KEY, port); config.configuration().addProperty(\u0026#34;dbms.security.auth_enabled\u0026#34;, false); neoServerBootstrapper = new WrappingNeoServerBootstrapper(api, config); start = neoServerBootstrapper.start(); } catch (Exception e) { fail(\u0026#34;Unable to start neo4j test server !\u0026#34;, e); } } this.graphFile = graphFile; this.client = new Neo4jRestClient(NEO4J_EMBEDDED_HOST, port, NEO4J_EMBEDDED_PROTOCOL); Il est a noter que les classes utilisées pour lancer le serveur REST sont dépréciés et sont sencés disparaitres prochainement. Cela dit dans la derbière version testé (2.0.0) les classes sont toujours là et aucune solution alternative n\u0026rsquo;existe\u0026hellip;\n", 
    "breadcrumb": " > development > java > start-neo4j-impermanent-database-+-rest-server-on-random-port-for-test.html"
},
{
    
    "uri": "/linux/network/lancer-tcpdump-en-non-root.html",
    "title": "Lancer TCPDump en non root",
    "tags": ["linux", "network", "tcpdump"],
    "description": "",
    "content": "groupadd tcpdump addgroup \u0026lt;username\u0026gt; tcpdump chown root.tcpdump /usr/sbin/tcpdump chmod 0750 /usr/sbin/tcpdump setcap \u0026#34;CAP_NET_RAW+eip\u0026#34; /usr/sbin/tcpdump", 
    "breadcrumb": " > linux > network > lancer-tcpdump-en-non-root.html"
},
{
    
    "uri": "/linux/administration/last-firefox-version-in-testing.html",
    "title": "Last Firefox Version in testing",
    "tags": ["linux", "sysadmin", "firefox", "browser"],
    "description": "",
    "content": " Depuis un moment iceweasel est remplacé par Firefox ESR sous Debian. Par contre, les depots Debian ont une dixaine de version de retard sur les versions release de Firefox. Du coup voici comment mettre à jour Firfox simplement et de façon fiable à la dernière version en cours :\nRécupération de la clé de dépôt wget -q -O - http://mozilla.debian.net/archive.asc | \\  sudo apt-key add - Ajout de mozilla.debian.net dans les repos apt Pour la version RELEASE. On peut choisir BETA aussi\ncat \u0026lt;\u0026lt; EOF | sudo tee /etc/apt/sources.list.d/mozilla-firefox.list deb http://mozilla.debian.net/ jessie-backports firefox-release EOF Assigner une priorité supérieur à ce dépôt cat \u0026lt;\u0026lt; EOF | sudo tee /etc/apt/preferences.d/mozilla-firefox Package: * Pin: origin mozilla.debian.net Pin-Priority: 501 EOF Mise à jour APT sudo apt update Vérification $ apt-cache policy firefox firefox: Installed: (none) Candidate: 45.0.1-1~bpo80+1 Version table: 45.0.1-1~bpo80+1 0 501 http://mozilla.debian.net/ jessie-backports/firefox-release amd64 Packages Installation de la dernière version sudo apt install firefox Liens  https://blog.sleeplessbeastie.eu/2016/03/21/how-to-use-recent-version-of-firefox-in-debian-jessie/  ", 
    "breadcrumb": " > linux > administration > last-firefox-version-in-testing.html"
},
{
    
    "uri": "/tags/latex.html",
    "title": "Latex",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > latex.html"
},
{
    
    "uri": "/tags/ldap.html",
    "title": "Ldap",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > ldap.html"
},
{
    
    "uri": "/linux/shell/lecture-darguments-en-bash.html",
    "title": "Lecture d’arguments en bash",
    "tags": ["linux", "shell", "bash"],
    "description": "",
    "content": "Pour parser les arguments d\u0026rsquo;une commande bash voici un exemple de traitement.\nTEMP=`getopt -o e:m:v:r:s: --long env:,module:,version:repository:,script: \\ \t-n \u0026#39;deploy.sh\u0026#39; -- \u0026#34;$@\u0026#34;` if [ $? != 0 ] ; then echo \u0026#34;Terminating...\u0026#34; \u0026gt;\u0026amp;2 ; exit 1 ; fi eval set -- \u0026#34;$TEMP\u0026#34; while true ; do case \u0026#34;$1\u0026#34; in -e|--env) ENVIRONMENT=$2 ; shift 2 ;; -m|--module) MODULE=$2 ; shift 2 ;; -v|--version) VERSION=$2 ; shift 2 ;; -r|--repository) REPOSITORY=$2 ; shift 2 ;; -s|--script) SCRIPT=$2 ; shift 2 ;; --) shift ; break ;; *) echo \u0026#34;Error with[$1:$2]\u0026#34; ; exit 1 ;; esac done if [ -z \u0026#34;$ENVIRONMENT\u0026#34; ]; then echo \u0026#34;ENVIRONMENT variable is not set\u0026#34;;fi if [ -z \u0026#34;$MODULE\u0026#34; ]; then echo \u0026#34;MODULE variable is not set\u0026#34;;fi if [ -z \u0026#34;$VERSION\u0026#34; ]; then echo \u0026#34;VERSION variable is not set\u0026#34;;fi if [ -z \u0026#34;$REPOSITORY\u0026#34; ]; then REPOSITORY=\u0026#34;releases\u0026#34;;fi if [ -z \u0026#34;$SCRIPT\u0026#34; ]; then SCRIPT=\u0026#34;deploy.yml\u0026#34;;fi", 
    "breadcrumb": " > linux > shell > lecture-darguments-en-bash.html"
},
{
    
    "uri": "/tags/libcogl.html",
    "title": "Libcogl",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > libcogl.html"
},
{
    
    "uri": "/tags/license.html",
    "title": "License",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > license.html"
},
{
    
    "uri": "/tags/linux.html",
    "title": "Linux",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > linux.html"
},
{
    
    "uri": "/linux/administration/lister-le-materiel-present-sur-la-machine.html",
    "title": "Lister le materiel présent sur la machine",
    "tags": ["linux", "sysadmin", "hardware"],
    "description": "",
    "content": "Pour lister tout le matos présent sur un machine Linux:\nsudo lshw -short La commande a besoin d\u0026rsquo;être installé via apt-get install lshw\n", 
    "breadcrumb": " > linux > administration > lister-le-materiel-present-sur-la-machine.html"
},
{
    
    "uri": "/linux/shell/lister-les-fichiers-d-une-arborescence.html",
    "title": "Lister les fichiers d’une arborescence",
    "tags": ["linux", "shell", "bash"],
    "description": "",
    "content": "Très pratique quand on fait un fix pour mettre dans la première partie\nfind -type f A exécuter depuis le répertoire du fix.\n", 
    "breadcrumb": " > linux > shell > lister-les-fichiers-d-une-arborescence.html"
},
{
    
    "uri": "/serveurs/oracle/lister-les-locks-sur-oracle.html",
    "title": "Lister les locks sur Oracle",
    "tags": ["server", "oracle", "database", "locks"],
    "description": "",
    "content": "prompt liste des utilisateurs qui lockent des tables select distinct username,sql_text from v$sqlarea,v$session,v$lock where user#=parsing_user_id and v$lock.sid=v$session.sid and username is not null and users_executing \u0026gt; 0 ;prompt liste des executions sql en cours select distinct username,disk_reads,rows_processed,sql_text from v$sqlarea,v$session where user#=parsing_user_id and username is not null and username not in (\u0026#39;SYS\u0026#39;,\u0026#39;SYSTEM\u0026#39;) and users_executing \u0026gt; 0 ; Plus un script sql pour lister les locks: lock.sql\n{% include_relative .doc/lock.sql %}", 
    "breadcrumb": " > serveurs > oracle > lister-les-locks-sur-oracle.html"
},
{
    
    "uri": "/outils/eclipse/locking-is-not-possible-in-the-directory-....html",
    "title": "Locking is not possible in the directory ...",
    "tags": ["outils", "eclipse", "error"],
    "description": "",
    "content": "En fait c\u0026rsquo;est parce qu\u0026rsquo;Eclipse ne comprends pas qu\u0026rsquo;on est en mode partagé ! Du coup il essaye de verrouiller l\u0026rsquo;appli via le répertoire d\u0026rsquo;install au lieu d\u0026rsquo;allé chercher dans le répertoire de l\u0026rsquo;utilisateur.\nPour régler ça, il faut que le répertoire d\u0026rsquo;install d\u0026rsquo;Eclipse soit en lecture seule pour les utilisateurs. Du coup il ira chercher dans le répertoire de chaque utilisateur.\n", 
    "breadcrumb": " > outils > eclipse > locking-is-not-possible-in-the-directory-....html"
},
{
    
    "uri": "/tags/locks.html",
    "title": "Locks",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > locks.html"
},
{
    
    "uri": "/tags/log4j.html",
    "title": "Log4j",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > log4j.html"
},
{
    
    "uri": "/tags/logs.html",
    "title": "Logs",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > logs.html"
},
{
    
    "uri": "/tags/logstach.html",
    "title": "Logstach",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > logstach.html"
},
{
    
    "uri": "/outils/logstach-config-for-metrics.html",
    "title": "Logstach config for Metrics",
    "tags": ["outils", "logstach"],
    "description": "",
    "content": "Un exemple de configuration qui marche avec Metrics\ninput { file { path =\u0026gt; \u0026quot;/var/log/lo_docker/*/monitoring.log\u0026quot; sincedb_path =\u0026gt; \u0026quot;/\u0026quot; } } filter { grok { patterns_dir =\u0026gt; \u0026quot;/data/techno/logstash/resources/patterns/\u0026quot; match =\u0026gt; [\u0026quot;message\u0026quot;,\u0026quot;%{NOTSPACE:date}%{SPACE}%{GREEDYDATA:data}\u0026quot;] remove_field =\u0026gt; [ \u0026quot;message\u0026quot; ] } grok { patterns_dir =\u0026gt; \u0026quot;/data/techno/logstash/resources/patterns/\u0026quot; match =\u0026gt; [\u0026quot;path\u0026quot;,\u0026quot;/var/log/lo_docker/%{DATA:module}/monitoring.log\u0026quot;] remove_field =\u0026gt; [ \u0026quot;path\u0026quot; ] } kv { source =\u0026gt; \u0026quot;data\u0026quot; field_split =\u0026gt; \u0026quot;, \u0026quot; remove_field =\u0026gt; [ \u0026quot;data\u0026quot; ] } date { match =\u0026gt; [ \u0026quot;date\u0026quot;, \u0026quot;YYYY-MM-dd'T'HH:mm:ss,SSS\u0026quot; ] remove_field =\u0026gt; [ \u0026quot;date\u0026quot; ] } mutate { convert =\u0026gt; { \u0026quot;count\u0026quot; =\u0026gt; \u0026quot;integer\u0026quot; } convert =\u0026gt; { \u0026quot;min\u0026quot; =\u0026gt; \u0026quot;float\u0026quot; } convert =\u0026gt; { \u0026quot;max\u0026quot; =\u0026gt; \u0026quot;float\u0026quot; } convert =\u0026gt; { \u0026quot;mean\u0026quot; =\u0026gt; \u0026quot;float\u0026quot; } convert =\u0026gt; { \u0026quot;stddev\u0026quot; =\u0026gt; \u0026quot;float\u0026quot; } convert =\u0026gt; { \u0026quot;median\u0026quot; =\u0026gt; \u0026quot;float\u0026quot; } convert =\u0026gt; { \u0026quot;p75\u0026quot; =\u0026gt; \u0026quot;float\u0026quot; } convert =\u0026gt; { \u0026quot;p95\u0026quot; =\u0026gt; \u0026quot;float\u0026quot; } convert =\u0026gt; { \u0026quot;p98\u0026quot; =\u0026gt; \u0026quot;float\u0026quot; } convert =\u0026gt; { \u0026quot;p99\u0026quot; =\u0026gt; \u0026quot;float\u0026quot; } convert =\u0026gt; { \u0026quot;p999\u0026quot; =\u0026gt; \u0026quot;float\u0026quot; } convert =\u0026gt; { \u0026quot;mean_rate\u0026quot; =\u0026gt; \u0026quot;float\u0026quot; } convert =\u0026gt; { \u0026quot;m1\u0026quot; =\u0026gt; \u0026quot;float\u0026quot; } convert =\u0026gt; { \u0026quot;m5\u0026quot; =\u0026gt; \u0026quot;float\u0026quot; } convert =\u0026gt; { \u0026quot;m15\u0026quot; =\u0026gt; \u0026quot;float\u0026quot; } } } output { elasticsearch_http { codec =\u0026gt; json_lines host =\u0026gt; \u0026quot;elasticdb\u0026quot; index =\u0026gt; \u0026quot;pmin-%{+YYYY.MM.dd}\u0026quot; } }  Attention cependant, metrics fait déjà des agrégations temporelles comme Kibana ce qui fait que les informations exploitable par Kibana ne sont pas forcément pertinentes.\nLe mieux étant, plutôt que d\u0026rsquo;utiliser des Metrics, faire plutôt ses propres log avec les temps d\u0026rsquo;exécution at tout ce qui sera bien exploité par Kibana par la suite.\n", 
    "breadcrumb": " > outils > logstach-config-for-metrics.html"
},
{
    
    "uri": "/linux/administration/mta-est-long-a-demarrer.html",
    "title": "MTA est long à démarrer",
    "tags": ["linux", "sysadmin", "mta", "startup"],
    "description": "",
    "content": "Déjà c\u0026rsquo;est quoi le MTA ? C\u0026rsquo;est le Mail Transfert Agent, en gros le truc qui s\u0026rsquo;occupe part déjà de distribuer les mails au différents users. Quand il est long à démarrer ça peut venir de trois facteurs :\n le réseau est mal configuré, à savoir qu\u0026rsquo;il y a une route par défaut mais qu\u0026rsquo;elle ne fonctionne pas le DNS est mal configuré ; le MTA essaye de se connecter à un smarthost inexistant ou inaccessible.  Donc pour régler le problème, allez vérifier que /etc/network/interfaces est bien configuré. Ensuite vérifiez la configuration DNS dans /etc/resolv.conf, ça doit ressembler à ça pour une résolution en local :\ndomain localdomain search localdomain nameserver 127.0.0.1  FIXME Si ça ne fonctionne toujours pas, dommage, je sais pas ce que c\u0026rsquo;est un \u0026ldquo;smarthost inexistant ou inaccessible\u0026rdquo; :p\n", 
    "breadcrumb": " > linux > administration > mta-est-long-a-demarrer.html"
},
{
    
    "uri": "/tags/maven.html",
    "title": "Maven",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > maven.html"
},
{
    
    "uri": "/development/java/maven-release-plugin.html",
    "title": "Maven Release Plugin",
    "tags": ["development", "java", "maven"],
    "description": "",
    "content": " Quelques explications sur le fonctionnement du Maven Release Plugin utilisé pour l’automatisation des releases.\nparent Déjà la première chose est de configurer les différents plugins de release dans le parent.\n\u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-release-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.5.3\u0026lt;/version\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.codehaus.mojo\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;versions-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.5\u0026lt;/version\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-javadoc-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.0.0\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;skip\u0026gt;true\u0026lt;/skip\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt;  Maven Release Plugin Le Maven Release Plugin est le plugin qui va faire le gros du travail, Mettre à jour le pom avec les bons numéros de version, faire les commit et poser les tags.\nVersions Maven Plugin Le Versions Maven Plugin va permettre de mettre à jour les dépendances SNAPSHOT avant la release.\nMaven Javadoc Plugin Le Maven Javadoc Plugin C’est le plugin chargé de générer la JavaDoc, il est utilisé par le Maven Release Plugin. Comme aujourd’hui la Javadoc des composants n’est pas conforme, il est nécessaire de la désactiver.\nDans nos projets Dans les poms de nos projets il est nécessaire d’avoir correctement configuré l’accès Git, sans quoi les tags ne pourront pas être posé correctement.\n\u0026lt;scm\u0026gt; \u0026lt;connection\u0026gt;scm:git:git://github.com/Marthym/hello-osgi-world.git\u0026lt;/connection\u0026gt; \u0026lt;developerConnection\u0026gt;scm:git:git@github.com:Marthym/hello-osgi-world.git\u0026lt;/developerConnection\u0026gt; \u0026lt;url\u0026gt;https://github.com/Marthym/hello-osgi-world\u0026lt;/url\u0026gt; \u0026lt;/scm\u0026gt; Pensez aussi à déclarer les plugins dans la partie \u0026lt;build\u0026gt;.\nRenseignement des versions Pour pouvoir fonctionner en mode silencieux, l’étape de préparation de la release à besoin d’un fichier de configuration, à placer à la racine du projet, contenant les numéros de versions des artefacts du projet, pour la release et pour la prochaine SNAPSHOT.\nLe fichier a cette forme :\nscm.tag=1.5.0 project.rel.fr.ght1pc9kc\\:hello-osgi-world=1.5.0 project.dev.fr.ght1pc9kc\\:hello-osgi-world=1.6.0-SNAPSHOT scm.commentPrefix=rel(main): On peut aussi lui préciser le préfixe de commit, par défaut [maven-release-plugin], selon nos conventions on choisira plutôt rel(main):.\nLes commandes Suppression des snapshots dans les dépendances:\nmvn versions:use-releases -DprocessParent=true -DfailIfNotReplaced=true Le principe du plugin consiste en la suppression des chaînes -SNAPSHOT dans le fichier mais le plugin vérifie quand même que la version à mettre existe bien, ait bien été releasé. Si ce n’est pas le cas, il plante.\nDeux paramètres : * processParent: Précise qu’il faut traiter aussi le bloc parent * failIfNotReplaced: Demande au plugin de sortir en erreur si une version d’une dépendance n’existe pas\nPhase de release:\nmvn release:prepare -DtagNameFormat=\u0026#34;@{version}\u0026#34; release:perform En fait le plugin agit en deux étapes qui peuvent être exécuté en une ligne.\n prepare:  Rassemble les informations Joue les test Pose les tags Modifie les poms avec les bonnes versions  perform:  Compile le code sous le tag Pousse le jar sur le Nexus   Attention, il est nécessaire de push les modif\ngit push \u0026amp;\u0026amp; git push --tags", 
    "breadcrumb": " > development > java > maven-release-plugin.html"
},
{
    
    "uri": "/development/java/maven-exemple-de-settings.xml.html",
    "title": "Maven: Exemple de settings.xml",
    "tags": ["development", "java", "maven"],
    "description": "",
    "content": "A placer dans ~/.m2/settings.xml\n\u0026lt;settings\u0026gt; \u0026lt;mirrors\u0026gt; \u0026lt;!-- Ajout du repos Nexus local --\u0026gt; \u0026lt;mirror\u0026gt; \u0026lt;!--This sends everything else to /public --\u0026gt; \u0026lt;id\u0026gt;nexus\u0026lt;/id\u0026gt; \u0026lt;mirrorOf\u0026gt;*\u0026lt;/mirrorOf\u0026gt; \u0026lt;url\u0026gt;http://nexus.mydomain.org/nexus/content/groups/public/\u0026lt;/url\u0026gt; \u0026lt;/mirror\u0026gt; \u0026lt;/mirrors\u0026gt; \u0026lt;profiles\u0026gt; \u0026lt;profile\u0026gt; \u0026lt;id\u0026gt;nexus\u0026lt;/id\u0026gt; \u0026lt;repositories\u0026gt; \u0026lt;repository\u0026gt; \u0026lt;id\u0026gt;nexus\u0026lt;/id\u0026gt; \u0026lt;url\u0026gt;http://nexus.mydomain.org/nexus/content/groups/public/\u0026lt;/url\u0026gt; \u0026lt;releases\u0026gt; \u0026lt;enabled\u0026gt;true\u0026lt;/enabled\u0026gt; \u0026lt;/releases\u0026gt; \u0026lt;snapshots\u0026gt; \u0026lt;enabled\u0026gt;true\u0026lt;/enabled\u0026gt; \u0026lt;/snapshots\u0026gt; \u0026lt;/repository\u0026gt; \u0026lt;/repositories\u0026gt; \u0026lt;pluginRepositories\u0026gt; \u0026lt;pluginRepository\u0026gt; \u0026lt;id\u0026gt;nexus\u0026lt;/id\u0026gt; \u0026lt;url\u0026gt;http://nexus.mydomain.org/nexus/content/groups/public/\u0026lt;/url\u0026gt; \u0026lt;releases\u0026gt; \u0026lt;enabled\u0026gt;true\u0026lt;/enabled\u0026gt; \u0026lt;/releases\u0026gt; \u0026lt;snapshots\u0026gt; \u0026lt;enabled\u0026gt;true\u0026lt;/enabled\u0026gt; \u0026lt;/snapshots\u0026gt; \u0026lt;/pluginRepository\u0026gt; \u0026lt;/pluginRepositories\u0026gt; \u0026lt;/profile\u0026gt; \u0026lt;profile\u0026gt;\u0026lt;!-- Ajout des infos pour sonar --\u0026gt; \u0026lt;id\u0026gt;sonar\u0026lt;/id\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;!-- SONAR--\u0026gt; \u0026lt;sonar.jdbc.url\u0026gt;jdbc:mysql://mysql.mydomain.org:3306/_sonar?useUnicode=true\u0026amp;amp;characterEncoding=utf8\u0026lt;/sonar.jdbc.url\u0026gt; \u0026lt;sonar.jdbc.username\u0026gt;sonar\u0026lt;/sonar.jdbc.username\u0026gt; \u0026lt;sonar.jdbc.password\u0026gt;sonar\u0026lt;/sonar.jdbc.password\u0026gt; \u0026lt;sonar.host.url\u0026gt;http://mysql.mydomain.org:9090\u0026lt;/sonar.host.url\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;/profile\u0026gt; \u0026lt;/profiles\u0026gt; \u0026lt;servers\u0026gt; \u0026lt;server\u0026gt; \u0026lt;id\u0026gt;nexus\u0026lt;/id\u0026gt; \u0026lt;username\u0026gt;deployment\u0026lt;/username\u0026gt; \u0026lt;password\u0026gt;deploy\u0026lt;/password\u0026gt; \u0026lt;/server\u0026gt; \u0026lt;/servers\u0026gt; \u0026lt;activeProfiles\u0026gt; \u0026lt;!--make the profile active all the time --\u0026gt; \u0026lt;activeProfile\u0026gt;nexus\u0026lt;/activeProfile\u0026gt; \u0026lt;/activeProfiles\u0026gt; \u0026lt;/settings\u0026gt;", 
    "breadcrumb": " > development > java > maven-exemple-de-settings.xml.html"
},
{
    
    "uri": "/outils/intellij/best-jvm-option-for-intellij.html",
    "title": "Meilleures options de JVM pour IntelliJ",
    "tags": ["outils", "intellij"],
    "description": "",
    "content": "Ajouter le fichier ~/.IntelliJIdea\u0026lt;version\u0026gt;/idea64.vmoptions\n-server -Xms2g -Xmx2g -XX:NewRatio=3 -Xss16m -XX:ReservedCodeCacheSize=240m -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:ConcGCThreads=4 -XX:ReservedCodeCacheSize=240m -XX:+AlwaysPreTouch -XX:+TieredCompilation -XX:+UseCompressedOops -XX:SoftRefLRUPolicyMSPerMB=50 -XX:-OmitStackTraceInFastThrow -Dsun.io.useCanonCaches=false -Djava.net.preferIPv4Stack=true -Djsse.enableSNIExtension=false -Dawt.useSystemAAFontSettings=lcd -Dsun.java2d.renderer=sun.java2d.marlin.MarlinRenderingEngine -ea", 
    "breadcrumb": " > outils > intellij > best-jvm-option-for-intellij.html"
},
{
    
    "uri": "/tags/memory.html",
    "title": "Memory",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > memory.html"
},
{
    
    "uri": "/linux/shell/mettre-un-processus-en-pause.html",
    "title": "Mettre un processus en pause",
    "tags": ["linux", "shell", "process"],
    "description": "",
    "content": "On est en train de compresser un film depuis 45mn et merde on a besoin de compresser un gros fichier pour l\u0026rsquo;envoyer rapidos. Pas de bol si on lance juste la compression du fichier, ça va prendre 20mn au lieu de 10s que ça prendrait normalement. On pourrait stopper la compression du film pour zipper le fichier mais on veut pas perdre les 45mn déjà effectuées \u0026hellip;\nkillall -STOP avidemux pour mettre le processus en pause et\nkillall -CONT avidemux pour reprendre.\nSi on a pas le nom du processus on peut faire avec kill -STOP 1234 et kill -CONT 1234\n", 
    "breadcrumb": " > linux > shell > mettre-un-processus-en-pause.html"
},
{
    
    "uri": "/outils/latex/mettre-a-jour-un-package-latex.html",
    "title": "Mettre à jour un package LaTeX",
    "tags": ["outils", "latex", "textlive"],
    "description": "",
    "content": "J’avais un problème avec un package latex apparemment trop vieux (tcolorbox) :\n! Package pgfkeys Error: I do not know the key '/tcb/before skip' and I am goin g to ignore it. Perhaps you misspelled it.  Malgrés une install correcte sous Debian (apt install textlive-full). Mais il est possible de mettre à jour les packages textlive unitairement avec tlmgr.\nCependant j\u0026rsquo;ai quand même pris pas mal d\u0026rsquo;erreur avant de parvenir à la faire fonctionner :\n Déjà il faut commancer par initialiser l\u0026rsquo;arbre des packages. Changer l\u0026rsquo;url du dépôt qui semble\u0026rsquo;t\u0026rsquo;il n\u0026rsquo;était pas bonne. Enfin, faire l\u0026rsquo;install du package désiré.  tlmgr init-usertree tlmgr option repository ftp://tug.org/historic/systems/texlive/2015/tlnet-final tlmgr install tcolorbox Après tout ça et un peu de retry en sudo et tout mon document compile.\n", 
    "breadcrumb": " > outils > latex > mettre-a-jour-un-package-latex.html"
},
{
    
    "uri": "/tags/misc.html",
    "title": "Misc",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > misc.html"
},
{
    
    "uri": "/linux/administration/missing-lsb-tags-and-overrides.html",
    "title": "Missing LSB tags and overrides",
    "tags": ["linux", "sysadmin", "startup", "service", "vmware"],
    "description": "",
    "content": " Symptômes On a ce genre de chose après une installation de paquets et les paquets ne sont pas configuré correctement :\nParamétrage de kerneloops-daemon (0.12+git20090217-3) ... insserv: warning: script 'K01vmware' missing LSB tags and overrides insserv: warning: script 'S50vmware-USBArbitrator' missing LSB tags and overrides insserv: warning: script 'vmware' missing LSB tags and overrides insserv: warning: script 'vmware-USBArbitrator' missing LSB tags and overrides insserv: There is a loop at service minissdpd if started insserv: Starting vmware-USBArbitrator depends on minissdpd and therefore on system facility `$all' which can not be true!  Dans ce cas c\u0026rsquo;est VMWare qui met la grouille !\nSolution Créer le fichier suivant : /etc/insserv/overrides/vmware et y mettre ce qui suis dedans :\n### BEGIN INIT INFO # Provides: vmware # Required-Start: $remote_fs $syslog # Required-Stop: $remote_fs $syslog # Default-Start: 2 3 5 # Default-Stop: 2 3 5 # Short-Description: VMware VMX service for virtual machines # Description: Allows running of VMware virtual machines. ### END INIT INFO Créer /etc/insserv/overrides/vmware-USBArbitrator et y mettre ce qui suis dedans :\n### BEGIN INIT INFO # Provides: vmware-USBArbitrator # Required-Start: $remote_fs $syslog vmware # Required-Stop: $remote_fs $syslog vmware # Default-Start: 2 3 5 # Default-Stop: 2 3 5 # Short-Description: Start daemon when vmware starts # Description: Enable service provided by daemon. ### END INIT INFO Enfin, exécuter :\nchmod +x /etc/insserv/overrides/vmware* Remarque 1 : C\u0026rsquo;est testé sur Debian 6 Remarque 2 : On le fait pour vmware-USBArbitrator mais je suppose que ça doit être valable pour d\u0026rsquo;autre modules de VMWare.\nLiens  http://communities.vmware.com/message/1875412#1875412  ", 
    "breadcrumb": " > linux > administration > missing-lsb-tags-and-overrides.html"
},
{
    
    "uri": "/development/java/mode-debug-tomcat.html",
    "title": "Mode debug sur Tomcat",
    "tags": ["development", "java", "tomcat", "debug", "server", "remote"],
    "description": "",
    "content": "Comme pour [[jboss]], il est possible de démarrer Tomcat en mode debug pour pouvoir s\u0026rsquo;y connecter ensuite via [[Eclipse|eclipse]], il s\u0026rsquo;agit du mode JPDA :\ncatalina.sh jpda run Tomcat ecoute alors sur le port 8000.\n", 
    "breadcrumb": " > development > java > mode-debug-tomcat.html"
},
{
    
    "uri": "/linux/shell/modifier-une-fonction-shell.html",
    "title": "Modifier une fonction shell",
    "tags": ["linux", "shell"],
    "description": "",
    "content": "Dans le cas d\u0026rsquo;une fonction dont on ne possède pas la paternité, on veux pouvoir profiter des mises à jour de la fonction mais sont état ne nous convient pas totalement. Par exemple la fonction suivante est pratique mais elle rejoue des tests qui, pour mon cas à moi, sont vraiment long et que je veux pouvoir skiper.\nmvnwatch () { if [ \u0026#34;$#\u0026#34; -eq 0 ] then WISDOM_DEV_SERVER=/home/lo/workspace/longback/wisdom-dev-server ASSDIR=${WISDOM_DEV_SERVER}/target/wisdom/application/${PWD##*/} else local ASSDIR=$1 fi echo $ASSDIR wisdom-dev-update ${PWD##*/} ~/applications/maven/bin/mvn io.lambdacube.maven:watch-maven-plugin:1.2:watch -DrefreshURL=\u0026#39;http://localhost:9100/osgi/refresh/?b=\u0026#39; -DdestDirectory=\u0026#34;${ASSDIR}\u0026#34; } Pour continuer à bénéficier des modifications, je ne veux pas surcharger la fonction. Je vais donc seulement la modifier. Je met donc la commande suivante qui va rechercher la ligne correspondant à maven et qui va ajouter à la fin l\u0026rsquo;option -DskipTests.\neval $(declare -f mvnwatch | sed \u0026#39;/maven:/ s/$/ -DskipTests/\u0026#39; | sed \u0026#39;s/$/;/\u0026#39;)  declare -f affiche le contenu de la fonction en question sed '/maven:/ s/$/ -DskipTests/' Recherche la ligne contenant maven et ajoute -DskipTests à la fin sed 's/$/;/' Remplace les sauts de ligne par des \u0026lsquo;;\u0026rsquo;, les CR n\u0026rsquo;étant pas interprété par eval.  ", 
    "breadcrumb": " > linux > shell > modifier-une-fonction-shell.html"
},
{
    
    "uri": "/development/gwt/module-may-need-to-be-re-compiled.html",
    "title": "Module may need to be (re)compiled",
    "tags": ["development", "gwt"],
    "description": "",
    "content": "Au lancement de l\u0026rsquo;appli on prend ce message GWT module may need to be (re)compiled.\nEn fait c\u0026rsquo;est parce que j\u0026rsquo;avais lancer auparavant l\u0026rsquo;appli en DevMod. depuis le répertoire src/main/webapp, GWT y a généré un fichier *.nocache.js qui lors du packaging du WAR vient systématiquement écraser celui généré par la compilation GWT précédente.\nCe fichier *.nocache.js de DevMod ne contient que le strict minimum pour s\u0026rsquo;exécuter. Il faut donc le supprimer de src/main/webapp et relancer la compil.\n", 
    "breadcrumb": " > development > gwt > module-may-need-to-be-re-compiled.html"
},
{
    
    "uri": "/linux/administration/monter-une-image-iso.html",
    "title": "Monter une image ISO",
    "tags": ["linux", "sysadmin"],
    "description": "",
    "content": "mount -o loop -t iso9660 fichier.iso /mnt/iso", 
    "breadcrumb": " > linux > administration > monter-une-image-iso.html"
},
{
    
    "uri": "/tags/mpc.html",
    "title": "Mpc",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > mpc.html"
},
{
    
    "uri": "/tags/mta.html",
    "title": "Mta",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > mta.html"
},
{
    
    "uri": "/linux/shell/multi-thread-avec-xargs.html",
    "title": "Multi-thread avec xargs",
    "tags": ["linux", "shell", "xargs"],
    "description": "",
    "content": "Un truc que je ne savais pas c\u0026rsquo;est qu\u0026rsquo;il est possible de faire du multi-thread avec xargs. C\u0026rsquo;est plutôt simple et ça fonctionne très bien.\nPar exemple dans un script qui met à jour tous les repositories git de ma machine je peux faire ça :\nfind -L ~ -maxdepth 5 -path \u0026#34;*.git\u0026#34; -not -path \u0026#34;*zprezto*\u0026#34; -type d 2\u0026gt; /dev/null | \\  xargs --max-proc=4 -n 1 -I {} bash -c \u0026#34;update_git_repo {}\u0026#34; Et xargs me crée un pool de 4 thread pour paralléliser ma mise à jour.\nAutre truc sympa avec xargs on peut nommer et ré-utiliser les arguments :\ndocker ps -aq | xargs -I_id -n1 sh -c \u0026#39;docker stop _id \u0026amp;\u0026amp; docker rm -v _id\u0026#39;", 
    "breadcrumb": " > linux > shell > multi-thread-avec-xargs.html"
},
{
    
    "uri": "/tags/music.html",
    "title": "Music",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > music.html"
},
{
    
    "uri": "/misc/mute-et-cec-sur-openelec.html",
    "title": "Mute et CEC sur OpenELEC",
    "tags": ["misc", "openelec", "cec", "hdmi", "kodi"],
    "description": "",
    "content": " Symptômes  Déjà au démarrage de Kodi, en haut à droit à coté de l\u0026rsquo;heure on voit l\u0026rsquo;icône MUTE. Ensuite la télécommande de la TV qui commende Kodi via CEC ne fonctionne pas. Pourtant le périphérique CEC d\u0026rsquo;OpenELEC s\u0026rsquo;affiche bien comme démarré. Enfin, à la place de Kodi dans la liste des sources de la TV il y a Recorder  Solutions La solution dans ce cas c\u0026rsquo;est de débrancher la TV pendant au moins 10 minutes pour faire un cold reboot. Ensuite on débranche tout ce qui est branché sur la TV on branche le Raspberry et on allume la TV.\nNormalement là la télécommande refonctionne mais il y a toujours l\u0026rsquo;icône MUTE. Pour l\u0026rsquo;enlever, on va sur l\u0026rsquo;interface web de Kodi, onglet Remote et on clique sur monter le son. Et pouf tout refonctionne correctement.\n", 
    "breadcrumb": " > misc > mute-et-cec-sur-openelec.html"
},
{
    
    "uri": "/tags/mysql.html",
    "title": "Mysql",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > mysql.html"
},
{
    
    "uri": "/linux/network/nfs-au-travers-de-ssh.html",
    "title": "NFS au travers de SSH",
    "tags": ["linux", "network", "ssh", "nsf"],
    "description": "",
    "content": " Faire passer du NFS au travers du SSH n\u0026rsquo;est pas si simple que ça. Il faut :\nPré-requis Vérifier que le server SSH autorise l\u0026rsquo;IP Forwarding\nConfiguration du client SSH Il faut faire une redirection de port. Pour ça, soit la ligne de commande :\nssh login@server-ssh -p 2222 -L 3049:server-nfs:2049 -L 3045:server-nfs:627 -N -f Le -N permet de ne pas proposer l\u0026rsquo;invite de commande, le -f lance le process SSH en background.\nsoit on le fait par ssh_config :\nHost SSH-SERVER-MAISON HostName server-ssh User login Port 2222 LocalForward 3049 server-nfs:2049 LocalForward 3045 server-nfs:627  Configuration server SSH Coté du SSH, il faut vérifier que les ports soient correctement ouvert. Pour ouvrir un port coté SSH :\niptables -A OUTPUT -p tcp --dport 627 -j ACCEPT iptables -A INPUT -p tcp --dport 627 -j ACCEPT iptables -A OUTPUT -p tcp --dport 2049 -j ACCEPT iptables -A INPUT -p tcp --dport 2049 -j ACCEPT Ouverture du partage Une fois que tout est fait, il suffit d\u0026rsquo;ouvrir la connexion SSH. Puis de monter le partage NFS avec la commande suivante:\nmount -t nfs -o tcp,mountport=3045,port=3049 localhost:/mnt/videos /mnt/nfs-videos Et normalement un ll /mnt/nfs-videos vous donne le contenu de votre partage.\nLes problèmes channel 5: open failed: connect failed: Connection refused Par ce qu\u0026rsquo;il y en a toujours \u0026hellip; Le cas que j\u0026rsquo;ai eu c\u0026rsquo;est un message d\u0026rsquo;erreur dans la console SSH :\nchannel 5: open failed: connect failed: Connection refused  Le tout se répétant à l\u0026rsquo;infinie. Ce message vient très probalement d\u0026rsquo;un port qui n\u0026rsquo;est pas ouvert sur le firewall. Le ports nécessaire pour le NFS sont 2049 et 627. Mais ça dépend du serveur NFS parfois c\u0026rsquo;est le 802 à la place du 627, \u0026hellip; bref ! Pour connaître le bon port il faut faire un rcpinfo -d mais dans mon cas (sur un serveur FreeNAS, j\u0026rsquo;ai pas trouvé la commande. Du coup un solution est d\u0026rsquo;aller sur le serveur SSH et de faire le montage avec l\u0026rsquo;option -vvv en plus. On voit dessuite le port qui ne parviens pas a franchir firewall :\nroot@server-ssh:/mnt# mount -t nfs -vvv -o nolock,tcp server-nfs:/mnt/videos videos/ mount.nfs: timeout set for Sat Apr 4 22:29:55 2015 mount.nfs: trying text-based options 'nolock,tcp,vers=4,addr=server-nfs,clientaddr=server-ssh' mount.nfs: mount(2): Protocol not supported mount.nfs: trying text-based options 'nolock,tcp,addr=server-nfs' mount.nfs: prog 100003, trying vers=3, prot=6 mount.nfs: trying server-nfs prog 100003 vers 3 prot TCP port 2049 mount.nfs: prog 100005, trying vers=3, prot=6 mount.nfs: trying server-nfs prog 100005 vers 3 prot TCP port 627  Une fois le bon port identifié, il suffit de faire les ajustement sur la redirection et sur le firewall.\nrpc.statd is not running Ici on a le message suivant :\nmount.nfs: rpc.statd is not running but is required for remote locking. mount.nfs: Either use '-o nolock' to keep locks local, or start statd. mount.nfs: an incorrect mount option was specified  Plus facile tout est dans le message il suffit de rajouter l\u0026rsquo;option nolock\n", 
    "breadcrumb": " > linux > network > nfs-au-travers-de-ssh.html"
},
{
    
    "uri": "/tags/naming.html",
    "title": "Naming",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > naming.html"
},
{
    
    "uri": "/tags/neo4j.html",
    "title": "Neo4j",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > neo4j.html"
},
{
    
    "uri": "/linux/administration/nettoyer-les-fichiers-de-configuration.html",
    "title": "Nettoyer les fichiers de configuration",
    "tags": ["linux", "sysadmin", "cleanup", "debian"],
    "description": "",
    "content": " Au fil des mises à jours d\u0026rsquo;une Debian, il arrive que des fichiers de configuration ne soient pas bien nettoyé. Il est parfois difficile dans les deb de supprimer des fichiers. Voilà comment trouver ces fichiers obsolète et les supprimer.\nProcédure standard dpkg-query -W -f=\u0026#39;${Conffiles}\\n\u0026#39; | grep \u0026#39;obsolete$\u0026#39; /etc/apparmor.d/abstractions/evince ae2a1e8cf5a7577239e89435a6ceb469 obsolete /etc/apparmor.d/tunables/ntpd 5519e4c01535818cb26f2ef9e527f191 obsolete /etc/apparmor.d/usr.bin.evince 08a12a7e468e1a70a86555e0070a7167 obsolete /etc/apparmor.d/usr.sbin.ntpd a00aa055d1a5feff414bacc89b8c9f6e obsolete /etc/bash_completion.d/initramfs-tools 7eeb7184772f3658e7cf446945c096b1 obsolete /etc/bash_completion.d/insserv 32975fe14795d6fce1408d5fd22747fd obsolete /etc/dbus-1/system.d/com.redhat.NewPrinterNotification.conf 8df3896101328880517f530c11fff877 obsolete /etc/dbus-1/system.d/com.redhat.PrinterDriversInstaller.conf d81013f5bfeece9858706aed938e16bb obsolete L\u0026rsquo;idée est ensuite de trouver à quel package appatiennent ces fichiers puis de supprimer le fichier et demander une reconfiguration du package en question :\ndpkg -S /etc/bash_completion.d/initramfs-tools initramfs-tools: /etc/bash_completion.d/initramfs-tools dpkg -S /etc/bash_completion.d/insserv initramfs-tools: /etc/bash_completion.d/insserv Donc nos fichiers initramfs-tools et insserv appartiennent à initramfs-tools.\nrm /etc/bash_completion.d/initramfs-tools /etc/bash_completion.d/insserv apt install --reinstall initramfs-tools insserv Et voilà.\nMalheureusement, ça ne fonctionne pas pour tous les types de fichier, ça serait trop facile.\nLes fichiers dbus-1 Pour je ne sais quelle raison, ça ne fonctionne pas avec les fichiers présent dans /etc/dbus-1/system.d/, il faut alors ré-installer les packages correspondant.\ndpkg -S /etc/dbus-1/system.d/com.redhat.NewPrinterNotification.conf system-config-printer-common: /etc/dbus-1/system.d/com.redhat.NewPrinterNotification.conf dpkg -S /etc/dbus-1/system.d/com.redhat.PrinterDriversInstaller.conf system-config-printer-common: /etc/dbus-1/system.d/com.redhat.PrinterDriversInstaller.conf apt purge system-config-printer-common apt install system-config-printer Inutile de les supprimer à la main.\nLes fichiers Apparmor Et biensûr les fichiers apparmor pausent aussi des problèmes. Purger le package qui les installe ne change rien du tout. Il semble qu\u0026rsquo;il faille aussi purger les profils apparmor.\napt purge apparmor-profiles apparmor-profiles-extra evince ntp apt install apparmor-profiles apparmor-profiles-extra evince ntp Rq: Pourquoi evince et ntp sont de la partie. Je sais pas, François Marier suspecte que ces fichiers sont livré pour l\u0026rsquo;un des package apparmor mais qu\u0026rsquo;ils sont finalement migré par evince ou ntp eux même et que du coup dpkg s\u0026rsquo;emmèle les pinceaux.\nLiens  https://feeding.cloud.geek.nz/posts/cleaning-up-obsolete-config-files-debian-ubuntu/  ", 
    "breadcrumb": " > linux > administration > nettoyer-les-fichiers-de-configuration.html"
},
{
    
    "uri": "/linux/administration/nettoyer-sa-debian.html",
    "title": "Nettoyer sa Debian",
    "tags": ["linux", "sysadmin", "cleanup", "debian", "purge"],
    "description": "",
    "content": " Même si ce n\u0026rsquo;est pas dans les proportion de Windows, un Linux a tendance à accumuler des reliquats de vieux paquets et du cache pas vraiment utile qui à la longue pèsent lourd sur l\u0026rsquo;espace disque (ça ne ralenti pas le système pour autant).\nLocalepurge C\u0026rsquo;est la première chose à faire, \u0026ldquo;localepurge\u0026rdquo; est un outil qui à chaque install de paquet ou de mise à jour, va faire le ménage dans les langues installé. Sa première utilisation va potentiellement faire gagner pas mal de place.\napt-get install localepurge localepurge Cette opération ne se fait qu\u0026rsquo;une fois, par la suite localepurge se lance automatiquement avec \u0026ldquo;apt\u0026rdquo;.\nLe nettoyage régulier Les symptomes Si on lance la commande suivante\ndu -hs /var/cache/apt/archives 728M /var/cache/apt/archives On constate que le cache des paquets prend une place significative au sein du système pour une utilité très réduite.\nLa solution Cette suite de commande va permettre d\u0026rsquo;effectuer un nettoyage rapide des caches d\u0026rsquo;apt sans risque d\u0026rsquo;endommager le système :\napt-get autoclean apt-get clean apt-get autoremove La première commande supprimera tous les paquets .deb présent dans le cache dont une version plus récente est installé, la deuxième supprime tous les paquets du cache, et non pas seulement ceux obsolètes comme la commande précédente, enfin la troisième commande supprime les dépendances qui ne sont plus nécessaires.\nSi vous faite un du par la suite vous verrez le gain de place.\nGagner encore de la place Si cela n\u0026rsquo;a pas suffit, voici encore quelques façons de gratter un peu de place.\n Nettoyer /var/tmp ce dernier contient des fichier \u0026hellip; temporaires non effacé Nettoyer /var/log qui contient les log du système  Nettoyer les kernels Lors des mise à jour de kernel, les anciens kernel sont conservé afin de pouvoir y revenir en cas de problème. Il est possible de dés-installer les anciens kernel ainsi que tout ce qui leur est associé (header, src, \u0026hellip;) via apt-get.\nLa commande suivante vous permet de connaître le kernel actuellement utilisé\nuname -r 3.2.0-3-amd64 Celle pour les kernels installé :\ndpkg --list 'linux-image*' Souhait=inconnU/Installé/suppRimé/Purgé/H=à garder | État=Non/Installé/fichier-Config/dépaqUeté/échec-conFig/H=semi-installé/W=attend-traitement-déclenchements |/ Err?=(aucune)/besoin Réinstallation (État,Err: majuscule=mauvais) ||/ Nom Version Architecture Description +++-=======================-===========-=============-==================================================== un linux-image \u0026lt;aucun\u0026gt; (aucune description n’est disponible) un linux-image-2.6-486 \u0026lt;aucun\u0026gt; (aucune description n’est disponible) un linux-image-2.6-686 \u0026lt;aucun\u0026gt; (aucune description n’est disponible) un linux-image-2.6-686-big \u0026lt;aucun\u0026gt; (aucune description n’est disponible) un linux-image-2.6-amd64 \u0026lt;aucun\u0026gt; (aucune description n’est disponible) un linux-image-2.6-k7 \u0026lt;aucun\u0026gt; (aucune description n’est disponible) un linux-image-2.6-openvz- \u0026lt;aucun\u0026gt; (aucune description n’est disponible) un linux-image-2.6-vserver \u0026lt;aucun\u0026gt; (aucune description n’est disponible) un linux-image-2.6-vserver \u0026lt;aucun\u0026gt; (aucune description n’est disponible) un linux-image-2.6-xen-686 \u0026lt;aucun\u0026gt; (aucune description n’est disponible) ii linux-image-3.2.0-3-486 3.2.23-1 i386 Linux 3.2 for older PCs ii linux-image-3.2.0-4-486 3.2.32-1 i386 Linux 3.2 for older PCs ii linux-image-486 3.2+46 i386 Linux for older PCs (meta-package)  Les \u0026ldquo;ii\u0026rdquo; sont les packages installé, pour supprimer ceux qui ne sont plus utilisé, dans l’exemple précédent c’est le 3.2.0-3-486 :\napt-get purge linux-image-3.2.0-3-486 linux-headers-3.2.0-3* ATTENTION Cette commande est a utiliser avec beaucoup de parcimonie, c’est irréversible et ça casse la VM ou le PC définitivement !\nNettoyer les paquets résiduels Afin de faire un véritable ménage sur le système, exécuter la commande suivante :\napt purge $(dpkg --list |grep \u0026#39;^rc\u0026#39; |awk \u0026#39;{print $2}\u0026#39;) Et en prime la liste de paquest qui reste après une montée de version :\ndpkg -l | grep deb8  Explication de la commande :\n dpkg –list : liste tous les paquets présent sur le système grep ‘^rc’ : cherche les paquets désinstallés mais pas purgés awk ‘{print $2}’ : filtre l’affichage de la sortie de la commande précédente  Trouver les gros fichiers [[include:../shell/Trouver les gros fichiers]]\nLiens  http://forum.ovh.com/showthread.php?t=27814 https://memo-linux.com/debian-nettoyer-les-paquets-residuels/  ", 
    "breadcrumb": " > linux > administration > nettoyer-sa-debian.html"
},
{
    
    "uri": "/tags/network.html",
    "title": "Network",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > network.html"
},
{
    
    "uri": "/tags/nexus.html",
    "title": "Nexus",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > nexus.html"
},
{
    
    "uri": "/tags/nginx.html",
    "title": "Nginx",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > nginx.html"
},
{
    
    "uri": "/serveurs/http/nginx-webdav-et-probleme-de-crochets.html",
    "title": "Nginx WebDAV et problème de crochets",
    "tags": ["server", "http", "nginx", "webdav"],
    "description": "",
    "content": "En installant un serveur Webdav sur Nginx j\u0026rsquo;ai eu un problème avec les répertoires contenant des [] dans leur nom. Le répertoire sont toujours vide. Sachant que le Nginx WebDAV n\u0026rsquo;est proxifié par un frontal Nginx lui aussi.\nEn regardant bien la doc de proxy_pass on lit que le comportement du Nginx est différent selon que l\u0026rsquo;ont met ou non l\u0026rsquo;URI dans la commande :\n proxy_pass http://192.168.0.12; proxy_pass http://192.168.0.12/marthym;  Dans le premier cas, nginx transfère la requête telquelle, dans le second, nginx normalise l\u0026rsquo;URI. Ce qui génère des problèmes puisqu\u0026rsquo;il dé-urlencode les noms de répertoire demandés au WebDAV, d\u0026rsquo;où le soucis des [].\n", 
    "breadcrumb": " > serveurs > http > nginx-webdav-et-probleme-de-crochets.html"
},
{
    
    "uri": "/tags/nsf.html",
    "title": "Nsf",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > nsf.html"
},
{
    
    "uri": "/tags/oem.html",
    "title": "Oem",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > oem.html"
},
{
    
    "uri": "/tags/openelec.html",
    "title": "Openelec",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > openelec.html"
},
{
    
    "uri": "/outils/optimisation-de-png.html",
    "title": "Optimisation de PNG",
    "tags": ["outils", "optimise", "png", "graphic"],
    "description": "",
    "content": "Dans un site web, ça peut être utile d\u0026rsquo;optimiser ces images PNG pour les réduire. Le PNG étant un format sans perte, cela permet d\u0026rsquo;avoir une bonne qualité à moindre coût.\nPour cela il y a deux commandes que je ne détaillerais pas :\npngnq -vf -s1 schedulers.png optipng -o7 schedulers-nq8.png  pngnq est un quantizer, il calcule la palette de couleur la plus restreinte pour l\u0026rsquo;image. optpng lui optimise les images suivant tout un tas de règles. Attention, il peut arriver que les images soient plus grosse à la sorti.  ", 
    "breadcrumb": " > outils > optimisation-de-png.html"
},
{
    
    "uri": "/development/java/optimisation-des-statements-batch-mysql.html",
    "title": "Optimisation des statements Batch MySQL",
    "tags": ["development", "java", "sql", "mysql", "database"],
    "description": "",
    "content": "Par défaut le mode batch du driver JDBC de MySQL n\u0026rsquo;est pas correctement optimisé. Il effectue un aller/retour serveur pour chaque requête au lieu de le faire en une seule fois.\nPour le rendre plainement opérationnel il faut ajouté l\u0026rsquo;option rewriteBatchedStatements à la connexion JDBC.\njdbc:mysql://127.0.0.1/?rewriteBatchedStatements=true", 
    "breadcrumb": " > development > java > optimisation-des-statements-batch-mysql.html"
},
{
    
    "uri": "/tags/optimise.html",
    "title": "Optimise",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > optimise.html"
},
{
    
    "uri": "/serveurs/http/optimize-nginx-performances.html",
    "title": "Optimiser les performances",
    "tags": ["server", "http", "nginx"],
    "description": "",
    "content": "  Do not remove {:toc}  Recopie d’un article de Quentin Busuttil sur Optimiser NGINX\nLes workers Concentrons nous d’abord sur worker_processes. Cette directive spécifie le nombre total de workers à créer au démarrage de Nginx. La valeur optimale est d’en avoir un par cpu core. Si vous avez un VPS – lesquels ont souvent un vCPU avec un seul vCore – il arrive souvent que la valeur par défaut soit supérieure au nombre total de cores. Ce n’est pas très grave, néanmoins, les processus supplémentaires vont un peu se tourner les pouces… pas bien utile. Pour définir cette valeur, il nous suffit donc de déterminer le nombre total de cores :\n# grep -c ^processor /proc/cpuinfo 12 Passons maintenant à la directive worker_connections. Elle spécifie combien de connections simultanées chaque worker est en mesure d’établir. Étant donné qu’une connexion nécessite un file descriptor au minimum, une bonne base est d’établir ce nombre en fonction des limites de notre système. Dans le cas de Nginx en reverse proxy, il faut un file descriptor pour la connexion client et un autre vers le serveur proxifié, soit deux par connexion.\n# ulimit -n 1024 Gestion du ulimit.\nEn dernier lieu, on permet aux workers d’accepter plusieurs nouvelles connexions de manière simultané en activant multi_accept. Cela peut être d’une grande utilité lors de pics de trafic.\nmulti_accept on; Les buffers Les buffers permettent à Nginx de travailler en RAM plutôt que sur le disque. Je ne vais pas vous faire un dessin, vous savez bien que les accès disque sont infiniment plus lents que le travail en RAM. On va donc configurer les buffers pour que notre serveur puisse travailler en mémoire autant que faire se peut.\nIl y a quatre types de buffers :\nclient_body_buffer_size Le buffer qui récupère les données clients (typiquement les données POST).\nclient_header_buffer_size Celui-ci s’occupe également des données du client, mais concerne l’en-tête. Généralement, 1k suffit ici amplement (la valeur par défaut). Qui plus est, dans le cas où cette limite est dépassée, alors c’est la directive large_client_header_buffers qui s’applique.\nclient_max_body_size La taille maximum des requêtes envoyées par le client. Si vous autorisez des uploads de fichiers, il s’agit d’y penser ici. Si cette limite est dépassée, Nginx retourne une erreur 413.\nlarge_client_header_buffers Taille et nombre maximum que peuvent atteindre les buffers pour les en-têtes. Au delà, une erreur est retournée.\nclient_body_buffer_size 16K; client_header_buffer_size 1k; client_max_body_size 20m; large_client_header_buffers 2 3k; Proxy buffers Il existe des buffers spécifiquement dédiés aux proxies. Dans le cas où les buffers sont désactivés, Nginx commence l’envoie des données au client aussitôt qu’il les reçoit du backend serveur. Si le client est rapide, tout est pour le mieux. Cependant, si le client est moins véloce, ce fonctionnement oblige à conserver une connexion ouverte entre Nginx – le serveur de proxy – et le backend serveur ; ce qui peut s’avérer dommageable.\nL’activation des buffers permet donc à Nginx de d’abord récupérer l’ensemble des données de la requêtes depuis le backend serveur, de libérer ce dernier, puis de servir les données au client.\nproxy_buffering Contrôle l’activation du buffer pour le proxy (activé par défaut).\nproxy_buffer_size Définie la taille du buffer pour les en-têtes de la réponse. 8k par défaut sur systèmes 64 bits. On peut ici laisser la valeur par défaut car les en-têtes dépassent rarement 8k.\nproxy_buffers Détermine la taille et le nombre des buffers pour le corps de la réponse. Une fois n’est pas coutume, la valeur dépendra grandement de votre application. La valeur par défaut (toujours pour les systèmes 64 bits) est de 8 buffers de 8k. Il s’agit de paramètres s’appliquant par requête. Ainsi, le réglage par défaut permettra de stocker dans les buffers des réponses jusqu’à 64kb. À vous de voir si votre application retourne des résultats plus importants (sachant qu’ensuite les fichiers sont écrits sur le disque).\nproxy_buffering on; proxy_buffer_size 1k; proxy_buffers 12 4k; Les timeouts client_body_timeout Ce timeout s’applique au body. Il définit le temps maximum entre deux opérations d’écriture (pas le temps total de transfert donc). Admettons que je veuille transférer de gros fichiers (plusieurs centaines de MB), je pourrais fixer ce timeout à 30s (défaut 60s). Si le client n’envoie aucune donnée dans ce laps de temps, le serveur émet une erreur 408.\nclient_header_timeout Même logique que la directive précédente mais le timeout s’applique bien ici à la totalité de la transaction. Néanmoins, les en-têtes étant beaucoup plus légères, je me contenterai pour ma part d’établir le timeout à 10s (défaut 60s) pour les headers.\nkeepalive_timeout Cette directive permet à la fois de spécifier le keepalive timeout mais également le header Keep-Alive: timeout=durée.\nAvec des serveurs tels qu’Apache où le serveur conserve un thread par connexion ouverte, de telles connexions impliquent une consommation de mémoire. Cependant, avec des serveurs événementiels comme Nginx, ce coût est relativement faible et il n’est donc pas très impactant en consommation ressources.\nL’intérêt du keepalive est d’autant plus grand si vous êtes en https. En effet, à chaque ouverture de connexion, il vous faudra renégocier un Three-way handshake TLS, ce qui prend du temps et demande de la puissance au serveur.\nsend_timeout C’est en quelque sorte le pendant inverse des body et header timeouts. Ici, le s’agit de définir le temps après lequel le serveur coupe la connexion si le client ne reçoit plus la réponse. Par défaut, 60s.\nkeepalive_requests Cette directive détermine le nombre de requêtes au bout duquel le connexion sera fermée. La valeur par défaut est à 100, ce qui est assez confortable et il n’est souvent pas nécessaire de modifier cette valeur. Néanmoins, si votre application nécessite le chargement de très nombreuses ressources (\u0026gt; 100), il peut être intéressant d’augmenter cette valeur pour qu’elle soit légèrement supérieur au nombre de ressources à charger.\nclient_body_timeout 30; client_header_timeout 10; keepalive_timeout 30; send_timeout 60; keepalive_requests 100; La compression gzip on; gzip_comp_level 5; gzip_min_length 1000; gzip_proxied any; gzip_types application/atom+xml application/javascript application/json application/ld+json application/manifest+json application/rss+xml application/vnd.geo+json application/vnd.ms-fontobject application/x-font-ttf application/x-web-app-manifest+json application/xhtml+xml application/xml font/opentype image/bmp image/svg+xml image/x-icon text/cache-manifest text/css text/plain text/vcard text/vnd.rim.location.xloc text/vtt text/x-component text/x-cross-domain-policy; Sachez également que la directive gzip_buffers peut s’avérer intéressante. Elle définie le nombre et la taille des buffers alloué à la compression. Par défaut, sur les architectures 64 bits, jusqu’à 16 buffers de 8k sont autorisés.\nTout va ici dépendre de la taille de vos fichiers. Il s’agit, comme expliqué dans ce post d’arbitrer entre le nombre de buffers (gérer de nombreux buffers consomme un peu de CPU) ou attribuer plus d’espace aux buffers (utilise plus d’espace mémoire). Si vous n’avez pas assez d’espace dans les buffers pour contenir l’ensemble de la réponse, Nginx attendra qu’une partie de la réponse soit envoyée au client et utilisera ensuite l’espace libéré.\nEnfin, sachez aussi qu’il est possible, avec le module gzip static, de pré-compresser des fichiers pour que Nginx puisse les servir sans avoir à les compresser à la volée.\nLe cache statique location ~* .(jpg|jpeg|png|gif|ico|css|js)$ { expires 60d; } L’open file cache Ce type de cache permet de conserver les métadonnées en mémoire, et donc de limiter l’I/O. Voici un exemple de configuration :\nopen_file_cache max=2000 inactive=5m; open_file_cache_valid 2m; open_file_cache_min_uses 2; open_file_cache_errors on; Cette configuration indique au serveur de mettre en cache 2000 open file descriptors et de les fermer si aucune requête les concernant n’est demandé au bout de 5 minutes. La validité des informations en cache est revérifiée après 2 minutes et un fichier doit être requêté un minimum de deux fois afin d’être valide pour le cache. Enfin, les fichiers d’erreurs sont ici également valables pour le cache.\nLe TLS générateur de configuration ssl de Mozilla\nLe Three-way handshake TLS prend du temps et est couteux en ressources. Activer la réutilisation de connexion SSL permet de limiter les aller/retour (Speeding up SSL).\nssl_session_cache Il permet de définir si on active le cache on non, de spécifier le type de cache (builtin ou shared) ainsi que la taille de ce dernier. Il est à none par défaut. La doc Nginx établit que 1MB permet de stocker 4000 sessions. À vous de juger combien de à sessions vous estimez devoir faire face et la durée de rétention que vous leur accordez. N’ayez crainte, au pire des cas Nginx invalidera les sessions prématurément, il n’y aura pas d’effusion de sang.\nssl_session_timeout Précise la durée au bout de laquelle la session est invalidée.\nssl_buffer_size Permet de déterminer pour l’envoie des données. Gros débat sur la question de la taille de ce dernier, une fois de plus, tout dépend du type de contenus que vous servez.\nssl_session_cache shared:SSL:10m; ssl_session_timeout 24h; ssl_buffer_size 1400; En plus des caches, il y a une dernière optimisation qu’il est possible de réaliser, elle a pour doux nom l’OSCP stapling. Lorsqu’un serveur fourni un certificat, le client en vérifie la validité en interrogeant l’autorité émettrice du certificat. Évidemment, cela demande une requête supplémentaire au client. On peut éviter cela en demandant au serveur de joindre directement l’autorité émettrice et d’ainsi « agraffer » (d’où le stampling) la réponse signée et horodatée de l’autorité afin de prouver la validité du certificat.\nssl_stapling on; ssl_stapling_verify on; resolver 8.8.8.8 8.8.4.4 216.146.35.35 216.146.36.36 valid=60s; resolver_timeout 2s; Optimisations TCP On arrive là dans le domaine de l’optimisation de pointe ! Il est possible de spécifier à Nginx la manière dont quelques options de TCP doivent être gérées. De manière synthétique, ces réglages vont nous permettre de :\n diminuer la latence de 200ms avant l’envoie des données sur le réseau, d’optimiser la copie des données depuis le file descriptors vers le buffer du kernel (ok, c’est compliqué !), et de n’attribuer un thread Nginx qu’au dernier moment de la connexion (économie de ressources).  Pour comprendre tout cela dans les détails, vous pouvez vous référer à l’article Optimisations Nginx : bien comprendre sendfile, tcp_nodelay et tcp_nopush ou à TCP TIME-WAIT \u0026amp; les serveurs Linux à fort trafic\nsendfile on; tcp_nodelay on; tcp_nopush on; En dernier lieu, nous allons nous pencher sur quelques optimisations qui s’effectuent au niveau du block server, elles se placeront donc en général dans un VHOST.\nÀ partir de là, la connexion est ouverte, et théoriquement, un socket est créé et le processus en écoute sur ce socket est réveillé : attribution de ressources Nginx dans le cas présent. Pour autant, tant qu’aucune vraie requête n’est effectuée, Nginx n’a rien à faire. L’option TCP_DEFER_ACCEPT du kernel permet donc de ne réveiller le processus que lors de l’envoie effectif de données. Cette option se traduit dans nginx par deferred.\nDans un autre registre, ce n’est pas tout à fait lié à TCP, mais j’en parle ici quand même, activer le HTTP/2 peut avoir un impact significatif sur les performances.\nserver { # on active deferred  # pour ipv6 sur le port 80 (http)  listen [::]:80 default_server deferred; # pour ipv4 sur le port 80 (http)  listen 80 default_server deferred; # pour ipv6 sur le port 443 (https)  listen [::]:443 ssl http2 deferred; # pour ipv4 sur le port 443 (https)  listen 443 ssl http2 deferred; return 444; } Vous noterez que l’on active HTTP/2 seulement pour le TLS. Il est possible de l’activer en HTTP non sécurisé, mais les navigateurs ne le supportent pas.\nVous remarquez aussi peut-être le default_server. Cela indique à Nginx d’utiliser ce VHOST si l’en-tête Host n’est pas passée avec le requête. Dans le cas d’un accès direct via l’ip par exemple. Et en dernier lieu, return 444 signifie que dans ces cas là, la connexion sera réinitialisée puisque si plusieurs sites sont hébergés sur cette même ip:port, en l’absence de Host, il n’est pas possible de savoir lequel on doit servir.\nIl y a de nombreuses optimisations potentielles au niveau de la pile TCP/IP, non spécifiques à Nginx, nombre d’entre elles sont détaillées dans cet article sur le TCP Tuning.\nLa thread pool Il arrive que certaines opérations bloquantes soient lentes (comme la lecture de fichiers sur le disque). Les requêtes de fichiers de taille importante qui ne tiennent pas en mémoire par exemple, bloqueront un thread de Nginx jusqu’à ce que le disque retourne le fichier en question. Admettons-le, c’est assez dommage. Heureusement, Nginx a une solution pour ça !\nIl s’agit de placer cette requête dans une « file d’attente » et de traiter d’autres requêtes en attendant que le disque ait fini son opération de lecture. Ainsi, on ne bloque pas plusieurs requêtes en attendant une I/O pour l’une d’entre elle, on place cette dernière en attente et on utilise le thread ainsi libéré pour servir d’autres requêtes.\nUn article sur le blog de Nginx détaille le fonctionnement de la Thread Pools et présente un benchmark où les performances en charge sont multipliées par 9 ! Il faut pour cela utiliser l’option aio :\nlocation / { root /var/www; aio threads; } Liens  Optimiser NGINX ulimit multi_accept Three-way handshake TLS gzip_buffers gzip_buffers_explique gzip static générateur de configuration ssl de Mozilla Speeding up SSL OSCP stapling Optimisations Nginx : bien comprendre sendfile, tcp_nodelay et tcp_nopush TCP TIME-WAIT \u0026amp; les serveurs Linux à fort trafic TCP_DEFER_ACCEPT HTTP/2 TCP Tuning Thread Pools  ", 
    "breadcrumb": " > serveurs > http > optimize-nginx-performances.html"
},
{
    
    "uri": "/outils/latex/optimizer-la-taille-dun-pdf.html",
    "title": "Optimizer la taille d’un PDF",
    "tags": ["outils", "latex", "pdf", "gs"],
    "description": "",
    "content": "La génération d\u0026rsquo;un PDF depuis LaTeX c\u0026rsquo;est super mais ça donne des PDF un peu volumineux parfois. Voilà une ligne de commande pour réduire drastiquement la taille des PDF:\n/usr/bin/gs -sDEVICE=pdfwrite -dCompatibilityLevel=1.4 -dNOPAUSE -dQUIET -dBATCH -dPDFSETTINGS=/ebook -sOutputFile=foo-compressed.pdf foo.pdf Attention, gs dans Prezto c\u0026rsquo;est un alias vers git d\u0026rsquo;où l\u0026rsquo;utilisation du chemin complet.\nOn peut ajouter l\u0026rsquo;option -dPDFSETTINGS qui règle la qualité :\n-dPDFSETTINGS=/screen (screen-view-only quality, 72 dpi images) -dPDFSETTINGS=/ebook (low quality, 150 dpi images) -dPDFSETTINGS=/printer (high quality, 300 dpi images) -dPDFSETTINGS=/prepress (high quality, color preserving, 300 dpi imgs) -dPDFSETTINGS=/default (almost identical to /screen)  ", 
    "breadcrumb": " > outils > latex > optimizer-la-taille-dun-pdf.html"
},
{
    
    "uri": "/tags/oracle.html",
    "title": "Oracle",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > oracle.html"
},
{
    
    "uri": "/tags/outils.html",
    "title": "Outils",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > outils.html"
},
{
    
    "uri": "/development/java/ou-telecharger-les-jdk.html",
    "title": "Où télécharger les JDK ?",
    "tags": ["development", "java", "jdk", "download", "oracle"],
    "description": "",
    "content": "http://www.oracle.com/technetwork/java/archive-139210.html\nVous aurez besoin d\u0026rsquo;un login oracle !\n", 
    "breadcrumb": " > development > java > ou-telecharger-les-jdk.html"
},
{
    
    "uri": "/tags/pdf.html",
    "title": "Pdf",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > pdf.html"
},
{
    
    "uri": "/tags/png.html",
    "title": "Png",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > png.html"
},
{
    
    "uri": "/tags/poisson.html",
    "title": "Poisson",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > poisson.html"
},
{
    
    "uri": "/tags/printer.html",
    "title": "Printer",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > printer.html"
},
{
    
    "uri": "/linux/administration/probleme-de-politique-de-securite-x11.html",
    "title": "Probleme de politique de securite X11",
    "tags": ["linux", "sysadmin", "x11", "security", "graphic"],
    "description": "",
    "content": " Symptome Au lancement du vmware-toolbox en tant que root, on prend l\u0026rsquo;erreur :\nPuTTY X11 proxy: wrong authorisation protocol attempted  Solution Créer un fichier /root/.Xauthority\nadministrateur@ubuntu:~$ xauth list ubuntu/unix:10 MIT-MAGIC-COOKIE-1 c76ff33554b56b2a749019c13a1b71c5 administrateur@ubuntu:~$ sudo -i root@ubuntu:~# xauth add ubuntu/unix:10 MIT-MAGIC-COOKIE-1 c76ff33554b56b2a749019c13a1b71c5 xauth: file /root/.Xauthority does not exist Truc \u0026amp; Astuuuuuce Pour automatiser :\nxauth list | while read x ; do sudo xauth add $x ; done", 
    "breadcrumb": " > linux > administration > probleme-de-politique-de-securite-x11.html"
},
{
    
    "uri": "/development/gwt/probleme-de-serialisation-avec-gwt.html",
    "title": "Problème de sérialisation",
    "tags": ["development", "gwt"],
    "description": "",
    "content": " Voilà plusieurs fois que je me fais avoir !\nSymptômes Lors d\u0026rsquo;un appel RPC dans [[GWT]] on se prend un erreur de sérialisation :\ncom.google.gwt.user.client.rpc.SerializationException: Type 'xxx' was not included in the set of types which can be serialized by this SerializationPolicy or its Class object could not be loaded. For security purposes, this type will not be serialized.: instance = xxx@5f39ee05  Alors que la classe en question dérive bin de Serializable ?\nExplication Pour qu\u0026rsquo;une classe soit considérée comme sérialisable par [[GWT]] il faut qu\u0026rsquo;elle remplisse les conditions suivantes :\n La classe doit être assignable à IsSerializable ou java.io.Serializable, directement ou indirectement en dérivant d\u0026rsquo;une classe qui implément une de ces interfaces.. Tous les membres non finaux et non transitoires de la classe doivent être sérialisable. \u0026ldquo;La classe possède une constructeur public par défaut (sans argument)\u0026rdquo;.  Et c\u0026rsquo;est très souvent la dernière raison qui explique le problème \u0026hellip;\nLiens  http://developerlife.com/tutorials/?p=131 http://stackoverflow.com/questions/4202964/serializationpolicy-error-when-performing-rpc-from-within-gwt-application  ", 
    "breadcrumb": " > development > gwt > probleme-de-serialisation-avec-gwt.html"
},
{
    
    "uri": "/tags/process.html",
    "title": "Process",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > process.html"
},
{
    
    "uri": "/tags/proxy.html",
    "title": "Proxy",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > proxy.html"
},
{
    
    "uri": "/tags/purge.html",
    "title": "Purge",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > purge.html"
},
{
    
    "uri": "/outils/git/push-sur-plusieurs-repo-git.html",
    "title": "Push sur plusieurs repo git",
    "tags": ["outils", "git"],
    "description": "",
    "content": "Il est possible de pousser (push) des modifs de repo git dans plusieurs repo origin à la fois. Dans le cas où on veut un repo backup ou je sais pas quoi du genre. Pour cela il suffit d\u0026rsquo;ajouter une pushurl avec la commande suivante :\ngit remote set-url --add --push origin git://original/repo.git git remote set-url --add --push origin git://another/repo.git Attention, il faut vraiment ajouter les deux url de repos, celle par défaut et l\u0026rsquo;autre. La pushurl surcharge l\u0026rsquo;url du depot par défaut dans si on ne met que la nouvelle URL on ne poussera plus vers le repo d\u0026rsquo;origine.\nOn voit le résultat avec\ngit remote -v origin git@git.original.com:original/myproject.git (fetch) origin git@another.org:another/myproject.git (push) origin git@git.original.com:original/myproject.git (push) ## Liens * https://stackoverflow.com/questions/14290113/git-pushing-code-to-two-remotes\n", 
    "breadcrumb": " > outils > git > push-sur-plusieurs-repo-git.html"
},
{
    
    "uri": "/linux/shell/raccourcis-terminal.html",
    "title": "Raccourcis Terminal",
    "tags": ["linux", "shell", "shortcut"],
    "description": "",
    "content": "Quelques commandes/raccourcis pour aller plus vite en mode terminal.\n CTRL + A: Aller au début de la ligne CTRL + E: Aller à la fin de la ligne CTRL + [left arrow]: Avancer d\u0026rsquo;un mot CTRL + [right arrow]: Reculer d\u0026rsquo;un mot CTRL + U: Supprimer toute la ligne CTRL + K; Supprimer tout ce qui est à la droite du curseur ESC + [backspace]: Supprimer le mot qui se trouve à la gauche du curseur CTRL + W: même effet que ESC + [backspace] ALT + D: Delete the word after the cursor CTRL + R: Rechercher dans l’historique des commandes utilisées CTRL + G: Sortir du mode “recherche dans l’historique” CTRL + _: Annuler la dernière commande utilisée CTRL + L: Nettoyer la console CTRL + C: Arrêter/tuer le “foreground process” courant CTRL + Z: Suspendre/arrêter le “foreground process”  ", 
    "breadcrumb": " > linux > shell > raccourcis-terminal.html"
},
{
    
    "uri": "/outils/refresh-redmine-repo.html",
    "title": "Rafraichir un repo git dans Redmine",
    "tags": ["linux", "misc", "redmine", "git"],
    "description": "",
    "content": "En fait, il n\u0026rsquo;y a rien d\u0026rsquo;automatique.\nIl faut commencer par cloner le repo sur le serveur remine. Ensuite on active la fonctionnalité Dépôt pour le projet et on ajoute le repo (en précisant le chemin physique). Attention c’est le chemin du répertoire .git qu’il faut renseigner, Ex /var/lib/git/longback/cosmos/.git sinon ça fonctionne pas.\nCe n’est pas fini, il car le repo ne se pull pas automatiquement. Pour cela, dans /etc/cron.d on ajoute le fichier suivant :\n*/5 * * * * \u0026lt;user\u0026gt; git --git-dir \u0026#39;/var/lib/git/longback/cosmos/.git\u0026#39; pull --all \u0026amp;\u0026amp; curl \u0026#39;http://\u0026lt;serveur-redmine\u0026gt;/sys/fetch_changesets?key=\u0026lt;key\u0026gt;\u0026amp;id=\u0026lt;projet\u0026gt;\u0026#39; Et là pour le coup, ça fonctionne, dès qu’un commit est poussé sur le repo, dans les 5 minutes les tickets redmine sont mis à jour.\n", 
    "breadcrumb": " > outils > refresh-redmine-repo.html"
},
{
    
    "uri": "/linux/shell/rechercher-dans-les-fichiers.html",
    "title": "Rechercher dans les fichiers",
    "tags": ["linux", "shell", "shortcut"],
    "description": "",
    "content": "find . -iname \u0026#39;*.jsp\u0026#39; | xargs grep \u0026#39;string\u0026#39; -sl find . -iname \u0026#39;*.jsp\u0026#39; -mtime -1 | xargs grep \u0026#39;string\u0026#39; -sl La première recherche simplement dans les fichiers, la seconde recherche dans les fichiers récemment modifiés.\n", 
    "breadcrumb": " > linux > shell > rechercher-dans-les-fichiers.html"
},
{
    
    "uri": "/outils/vmware/recompilation-vmplayer-sous-linux.html",
    "title": "Recompilation VMPlayer sous Linux",
    "tags": ["outils", "vmware", "vmplayer", "linux"],
    "description": "",
    "content": "Il est arrivé que l\u0026rsquo;outil de re-compilation automatique de VMPlayer ne fonctionne pas bien pour des raisons diverses. Dans ce cas, la seule solution pour savoir ce qui ne fonctionne pas est de lancer la compilation à la main depuis une console root :\nsudo -i cd /usr/bin vmware-modconfig --console --install-all --icon=vmware-player --appname=VMware", 
    "breadcrumb": " > outils > vmware > recompilation-vmplayer-sous-linux.html"
},
{
    
    "uri": "/tags/recovery.html",
    "title": "Recovery",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > recovery.html"
},
{
    
    "uri": "/serveurs/neo4j/recovery-required-from-position-logposition.html",
    "title": "Recovery required from position LogPosition",
    "tags": ["server", "neo4j", "database", "recovery"],
    "description": "",
    "content": " Après un arrêt intenpestif ou une tentative de backup à l\u0026rsquo;arrache de la base neo4j, on a eu un soucis pour la relancer !\nSymptomes Pas de message nulle part mais base non démarré !\nLe seul truc visible était dans neo4j/data/graph.db/messages.log\n[NeoStoreDataSource] Recovery required from position LogPosition  Un truc comme ça !\nSolution Deux chose :\n Supprimer le store_lock dans graph.db Supprimer les log de transactions dans graph.db : rm neostore.transaction.db.*  Attention tout de même, c\u0026rsquo;est un peu du 80\u0026frasl;20 % de change que ça redémarre. Si la base est vraiment corrompue ça ne fonctionnera pas !\n", 
    "breadcrumb": " > serveurs > neo4j > recovery-required-from-position-logposition.html"
},
{
    
    "uri": "/linux/shell/recuperer-le-chemin-d-un-script-bash.html",
    "title": "Recuperer le chemin d’un script bash",
    "tags": ["linux", "shell", "script"],
    "description": "",
    "content": "getScriptPath () { echo ${0%/*}/ } currentPath=$(getScriptPath) cd $currentPath", 
    "breadcrumb": " > linux > shell > recuperer-le-chemin-d-un-script-bash.html"
},
{
    
    "uri": "/outils/vmware/redirection-de-ports-avec-vmware-server.html",
    "title": "Redirection de ports avec VMWare Server",
    "tags": ["outils", "vmware"],
    "description": "",
    "content": " Il est possible depuis un serveur VMWare de faire de la redirection de port vers les VM invités qui s\u0026rsquo;y trouve pour par exemple rendre une VM accessible en FTP sans pour autant être obligé de la mettre en \u0026ldquo;Bridge\u0026rdquo; et donc de lui donner une adresse réseau.\nPour cela il faut modifier le fichier : /etc/vmware/vmnet8/nat/nat.conf sur le serveur !\nDans la section \u0026ldquo;incomingtcp\u0026rdquo; on a les redirections pour le protocole TCP (celui qui nous intéresse pour le FTP ou le SSH. Il suffit d\u0026rsquo;ajouter une ligne comme suivant :\n# SSH # ssh -p 8889 root@localhost 8889 = 192.168.89.145:22 Ici, on redirige le port 8889 de la machine hôte vers le port SSH (22) de la machine invité. Donc quand on fait un ssh sur le port 8889 sur la machine hôte, on tombe sur la machine invité.\nLiens  http://www.vmware.com/support/ws5/doc/ws_net_nat_advanced.html  ", 
    "breadcrumb": " > outils > vmware > redirection-de-ports-avec-vmware-server.html"
},
{
    
    "uri": "/tags/redmine.html",
    "title": "Redmine",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > redmine.html"
},
{
    
    "uri": "/tags/reflection.html",
    "title": "Reflection",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > reflection.html"
},
{
    
    "uri": "/tags/registry.html",
    "title": "Registry",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > registry.html"
},
{
    
    "uri": "/outils/docker/registry-docker-derriere-un-nginx.html",
    "title": "Registry Docker derrière un Nginx",
    "tags": ["development", "docker"],
    "description": "",
    "content": "L\u0026rsquo;idée est de placer un registry Docker derrière un frontal Nginx afin de faire du HTTPS voire de l\u0026rsquo;authentification.\nLe premier truc c\u0026rsquo;est la conf Nginx :\nserver { listen 80; listen 443 ssl; server_name docker.livingobjects.com; access_log off; server_tokens off; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; # On commence par dire où se situent nos certificats SSL # On aurait pu les mettre séparément dans chaque \u0026#34;server\u0026#34; en ayant besoin. ssl_certificate /etc/nginx/ssl/docker.livingobjects.com.crt; ssl_certificate_key /etc/nginx/ssl/docker.livingobjects.com.key; ssl_session_timeout 5m; ssl_session_cache shared:SSL:10m; ssl_protocols SSLv3 TLSv1 TLSv1.1 TLSv1.2; ssl_ciphers ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:ECDH+3DES:DH+3DES:RSA+AESGCM:RSA+AES:RSA+3DES:!aNULL:!MD5:!DSS; #limit_conn perip 20; #limit_req zone=antiddos burst=10 nodelay; client_max_body_size 0; chunked_transfer_encoding on; location / { proxy_pass http://docker-registry:5000; } } J\u0026rsquo;ai passé un moment à la faire fonctionné \u0026hellip;\nEnsuite, si le certificat est auto-signé, Docker va pas apprécié. Pour lui faire accepter la pilule il faut installé le certificat de l\u0026rsquo;autorité sur le client Docker. Pour ça :\n Copier les fichiers .crt et .key dans /usr/local/share/ca-certificates Lancer la commande update-ca-certificates  Et voilà.\n", 
    "breadcrumb": " > outils > docker > registry-docker-derriere-un-nginx.html"
},
{
    
    "uri": "/outils/git/regrouper-des-repos-git-sans-perdre-l-historique.html",
    "title": "Regrouper des repos git sans perdre l’historique",
    "tags": ["outils", "git"],
    "description": "",
    "content": "J’ai eu le cas pour rassembler plusieurs modules dans un même projet et les mettre en tant que sous-modules :\ngit remote add other /path/to/XXX git fetch other git checkout -b ZZZ other/master mkdir ZZZ git mv stuff ZZZ/stuff # as necessary git commit -m \u0026#34;Moved stuff to ZZZ\u0026#34; git checkout master git merge ZZZ # should add ZZZ/ to master git commit git remote rm other git branch -d ZZZ # to get rid of the extra branch before pushing git push # if you have a remote, that is", 
    "breadcrumb": " > outils > git > regrouper-des-repos-git-sans-perdre-l-historique.html"
},
{
    
    "uri": "/tags/remote.html",
    "title": "Remote",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > remote.html"
},
{
    
    "uri": "/development/java/remote-jmx-console.html",
    "title": "Remote JMX Console",
    "tags": ["development", "java", "jmx", "console", "remote"],
    "description": "",
    "content": "Pour passer une application Java en JMX Remote Console, il faut ajouter des paramètres :\n-Djava.rmi.server.hostname=172.17.10.19 -Dcom.sun.management.jmxremote.port=1088 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false  ", 
    "breadcrumb": " > development > java > remote-jmx-console.html"
},
{
    
    "uri": "/outils/docker/remove-ophans-from-docker-registry.html",
    "title": "Remove ophans from docker registry",
    "tags": ["development", "docker", "cleanup", "registry"],
    "description": "",
    "content": "Il est compliqué de nettoyer un registry Docker (avant sa version 2.0 en tout cas). Voici un script qui supprime les images orpheline dans le registry.\n#!/bin/bash  JQPATH=$(which jq) if [ \u0026#34;x$JQPATH\u0026#34; == \u0026#34;x\u0026#34; ]; then echo \u0026#34;Couldn\u0026#39;t find jq executable.\u0026#34; 1\u0026gt;\u0026amp;2 exit 2 fi set -eu shopt -s nullglob readonly base_dir=/data/docker_registry_backup/registry/ readonly output_dir=$(mktemp -d -t trace-images-XXXX) readonly jq=$JQPATH readonly repository_dir=$base_dir/repositories readonly image_dir=$base_dir/images readonly all_images=$output_dir/all readonly used_images=$output_dir/used readonly unused_images=$output_dir/unused function info() { echo -e \u0026#34;\\nArtifacts available in $output_dir\u0026#34; } trap info EXIT ERR INT function image_history() { local readonly image_hash=$1 $jq \u0026#39;.[]\u0026#39; $image_dir/$image_hash/ancestry | tr -d \u0026#39;\u0026#34;\u0026#39; } echo \u0026#34;Collecting orphan images\u0026#34; for library in $repository_dir/*; do echo \u0026#34;Library $(basename $library)\u0026#34; \u0026gt;\u0026amp;2 for repo in $library/*; do echo \u0026#34; Repo $(basename $repo)\u0026#34; \u0026gt;\u0026amp;2 for tag in $repo/tag_*; do echo \u0026#34; Tag $(basename $tag)\u0026#34; \u0026gt;\u0026amp;2 tagged_image=$(cat $tag) image_history $tagged_image done done done | sort | uniq \u0026gt; $used_images ls $image_dir \u0026gt; $all_images grep -v -F -f $used_images $all_images \u0026gt; $unused_images readonly all_image_count=$(wc -l $all_images | awk \u0026#39;{print $1}\u0026#39;) readonly used_image_count=$(wc -l $used_images | awk \u0026#39;{print $1}\u0026#39;) readonly unused_image_count=$(wc -l $unused_images | awk \u0026#39;{print $1}\u0026#39;) readonly unused_image_size=$(cd $image_dir; du -hc $(cat $unused_images) | tail -n1 | cut -f1) echo \u0026#34;${all_image_count}images, ${used_image_count}used, ${unused_image_count}unused\u0026#34; echo \u0026#34;Unused images consume ${unused_image_size}\u0026#34; echo -e \u0026#34;\\nTrimming _index_images...\u0026#34; readonly unused_images_flatten=$output_dir/unused.flatten cat $unused_images | sed -e \u0026#39;s/\\(.*\\)/\\\u0026#34;\\1\\\u0026#34; /\u0026#39; | tr -d \u0026#34;\\n\u0026#34; \u0026gt; $unused_images_flatten for library in $repository_dir/*; do echo \u0026#34;Library $(basename $library)\u0026#34; \u0026gt;\u0026amp;2 for repo in $library/*; do echo \u0026#34; Repo $(basename $repo)\u0026#34; \u0026gt;\u0026amp;2 mkdir -p \u0026#34;$output_dir/$(basename $repo)\u0026#34; jq \u0026#39;.\u0026#39; \u0026#34;$repo/_index_images\u0026#34; \u0026gt; \u0026#34;$output_dir/$(basename $repo)/_index_images.old\u0026#34; jq -s \u0026#39;.[0] - [ .[1:][] | {id: .} ]\u0026#39; \u0026#34;$repo/_index_images\u0026#34; $unused_images_flatten \u0026gt; \u0026#34;$output_dir/$(basename $repo)/_index_images\u0026#34; sudo cp \u0026#34;$output_dir/$(basename $repo)/_index_images\u0026#34; \u0026#34;$repo/_index_images\u0026#34; done done echo -e \u0026#34;\\nRemoving images\u0026#34; cat $unused_images | xargs -I{} rm -rf $image_dir/{}", 
    "breadcrumb": " > outils > docker > remove-ophans-from-docker-registry.html"
},
{
    
    "uri": "/linux/shell/rendre-executable-ce-qui-peut-l-etre-dans-une-arborescence.html",
    "title": "Rendre exécutable ce qui peut l’être dans une arborescence",
    "tags": ["linux", "shell"],
    "description": "",
    "content": "Dans le cas où par exemple on décompresse un fichier .zip qui contient une appli (sqldevelopper, soapUI, \u0026hellip;) on se retrouve avec un répertoire en mode 700. Du coup, seul le user qui a décompressé le zip peut lancer l\u0026rsquo;appli. Il faut donc passer les répertoire et les fichiers exécutable en 644 et 744 avec chmod. Le plus rapide pour ça est la commande suivante :\nchmod a+rX -R repertoire_appli Le X permet de dire que tout les répertoire passe en exécutable et que les fichiers qui sont déjà exécutable le reste les autres ne changent pas.\n", 
    "breadcrumb": " > linux > shell > rendre-executable-ce-qui-peut-l-etre-dans-une-arborescence.html"
},
{
    
    "uri": "/linux/shell/renommer-des-fichiers-sous-linux.html",
    "title": "Renommer des fichiers sous Linux",
    "tags": ["linux", "shell"],
    "description": "",
    "content": "La commande est pas compliqué mais les expression régulières c\u0026rsquo;est jamais la fête.\nrename s/\u0026#34;SEARCH\u0026#34;/\u0026#34;REPLACE\u0026#34;/g *", 
    "breadcrumb": " > linux > shell > renommer-des-fichiers-sous-linux.html"
},
{
    
    "uri": "/development/java/requete-oracle-avec-variables-liees.html",
    "title": "Requête Oracle avec Variables Liées",
    "tags": ["development", "java", "oracle", "sql", "database", "mysql"],
    "description": "",
    "content": " C\u0026rsquo;est un info qui date mais c\u0026rsquo;est toujours juste.\nC\u0026rsquo;est quoi des Bind Variable déjà ? Dans le processus d\u0026rsquo;exécution d\u0026rsquo;une requête, Oracle effectue plusieurs étapes. Parmi ces étapes, on retrouve le parsing. Durant cette étape, oracle décortique l\u0026rsquo;ordre SQL et choisit quel est le plan d\u0026rsquo;exécution le plus court pour récupérer les données ramenées par la requête.\nSi une requête est exécuté 20 fois de suite, Oracle fait le parsing pour la première requête seulement. Il se ressert des infos de parsing obtenu pour la première requête pour exécuter les autres.\nLa où les BV interviennent c\u0026rsquo;est pour déterminer si une requête et une autre sont identique ou non et si du coup elle partage ou non les même infos de parsing. Par exemple :\nselect colonne from table_toto where autre_colonne = \u0026#39;valeur\u0026#39;; select colonne from table_toto where autre_colonne = \u0026#39;autre valeur\u0026#39;; Ces deux requête, si elles sont envoyées à Oracle de cette façon sont considérées comme des requêtes différente et le parsing sera refait pour chaque requête alors qu\u0026rsquo;au final, le plan d\u0026rsquo;exécution est bien le même.\nSi par contre, on avant la requête avec des BV au lieux des valeurs en dur :\nselect colonne from table_toto where autre_colonne = :b1; Là on peut exécuter la requête pour\n b1 = valeur b1 = autre valeur   Oracle considère les deux requêtes comme identique et ne refais pas l\u0026rsquo;étape de parsing.\nLe gain de temps représente l\u0026rsquo;utilisation va de 5% à 25%. Le gain peut monter jusqu\u0026rsquo;à 90% en utilisant le profilage de requête qui est impossible sans les BV.\nConcrètement dans le code Concrètement dans le code Java, ça veut dire quoi ? Ca veut dire qu\u0026rsquo;on évite les concaténations du genre :\nString requete = \u0026#34;select colonne from table_toto where autre_colonne = \u0026#39;\u0026#34;+valeur+\u0026#34;\u0026#39;\u0026#34; On utilisera plutôt les PreparedStatement :\nString requete = \u0026#34;select colonne from table_toto where autre_colonne = ?\u0026#34;; PreparedStatement statement = conn.prepareStatement(requete); statement.setString(1, \u0026#34;valeur\u0026#34;); ResultSet result = statement.executeQuery(); // ... statement.setString(1, \u0026#34;autre valeur\u0026#34;); ResultSet result = statement.executeQuery();", 
    "breadcrumb": " > development > java > requete-oracle-avec-variables-liees.html"
},
{
    
    "uri": "/misc/resynchroniser-des-cpl-devolo.html",
    "title": "Resynchroniser des CPL Devolo",
    "tags": ["misc", "cpl"],
    "description": "",
    "content": " Symptômes Premier vrai sympte, les chaines HD de la box sont noire, impossible de les afficher. Seules les chaines LD fonctionnent. A force d\u0026rsquo;insiter la box me dit qu\u0026rsquo;il n\u0026rsquo;y a pas assez de débit pour afficher en HD.\nDeuxième symptôme en testant les débits entre les différents PC :\niperf -c 192.168.1.xx -p 80  Je me retrouve avec des débits de l\u0026rsquo;ordre de 700b/s \u0026hellip; ouahouh !\nSolution La solution c\u0026rsquo;est de resynchroniser les CPL pour qu\u0026rsquo;ils se remettent à discuter à fond. Pour ça :\n Tout débrancher Débrancher les cables réseaux des CPL. Brancher les deux premiers CPL Sur chaque CPL, appuyez pendant 20s sur le petit bouton sur le coté du boitier. Jsuqu\u0026rsquo;à ce que les loupiotes passent au rouge. Les deux boitiers se retrouve On redébranche On branche le premier On appuis une fois sur le bouton On branche le second dans la foulé et on appuit sur le bouton On rappuit sur le bouton du premier On rebranche le troisième On appuit sur le bouton On rebranche tout les cable  Le débit passe maintenant à +50M/s\n", 
    "breadcrumb": " > misc > resynchroniser-des-cpl-devolo.html"
},
{
    
    "uri": "/linux/divers/retrouver-la-cle-oem-windows.html",
    "title": "Retrouver la cle OEM Windows",
    "tags": ["linux", "misc", "bash", "windows", "license", "oem"],
    "description": "",
    "content": "Les PC vendu avec Windows OEM ont un numéro de license associé au matériel dirrectement. La plus part du temps il est collé sur le PC mais pas toujours. Du coup si on enlève Windows pour installer Linux sans prendre le temps de noter la clé, il peut être compliqué de le retrouver plustard si on veut réinstaller Windows, pour revendre par exemple.\nMais la clé OEM est souvent inscrite directement dans la carte mère et est récupérable depuis Linux via la commande suivante :\nsudo tail -c+57 /sys/firmware/acpi/tables/MSDM", 
    "breadcrumb": " > linux > divers > retrouver-la-cle-oem-windows.html"
},
{
    
    "uri": "/outils/vmware/retrouver-la-configuration-reseau-vmware.html",
    "title": "Retrouver la configuration réseau VMWare",
    "tags": ["outils", "vmware", "network"],
    "description": "",
    "content": " Il est possible avec VMPlayer de configurer les interfaces réseaux qu\u0026rsquo;il crée pour le NAT ou le Bridge (ou le Host-Only). Il existe même une interface graphique mais pour une raison inconnu, l\u0026rsquo;accès a cette interface a disparu entre deux versions. Il existe tout de même un moyen de la retrouver.\nSous Windows Depuis Windows, vous devez ouvrir une fenêtre ligne de commande (cmd) en tant qu\u0026rsquo;administrateur (click droit sur cmd.exe \u0026gt; Run as Administrator). Puis dans la fenêtre tapez :\nc:\\\u0026gt; rundll32.exe vmnetui.dll VMNetUI_ShowStandalone A tester \u0026hellip;\nSous Linux Sous Linux c\u0026rsquo;est plus simple :\ncd /usr/lib/vmware/bin ln -s /usr/lib/vmware/bin/appLoader vmware-netcfg ln -s /usr/lib/vmware/bin/vmware-netcfg /usr/bin/vmware-netcfg On obtient à peu de chose prêt le même écran que sous Windows.\nLiens  http://communities.vmware.com/message/2155960#2155960 http://www.windows7home.net/how-to-enable-or-disable-administrator-account-in-windows-7/ http://wiki.debian.org/VMware#Running_vmware-netcfg_.28Virtual_Network_Editor.29_with_VMware_Player  ", 
    "breadcrumb": " > outils > vmware > retrouver-la-configuration-reseau-vmware.html"
},
{
    
    "uri": "/tags/rsync.html",
    "title": "Rsync",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > rsync.html"
},
{
    
    "uri": "/development/java/repeat-test-with-junit.html",
    "title": "Répéter un test x fois avec jUnit",
    "tags": ["development", "java", "junit", "test"],
    "description": "",
    "content": "Ajouter la Rule RepeatRule :\n@Rule public RepeatRule repeatRule = new RepeatRule(); Ajouter l\u0026rsquo;annotation @Repeat au test :\n@Repeat(times = 100) Dans IntelliJ, les X exécutions n\u0026rsquo;apparaissent pas, il n\u0026rsquo;y a qu\u0026rsquo;une ligne mais qui a pris plus de temps.\n", 
    "breadcrumb": " > development > java > repeat-test-with-junit.html"
},
{
    
    "uri": "/search.html",
    "title": "Résultats de recherche",
    "tags": [],
    "description": "",
    "content": "Loading...  ", 
    "breadcrumb": " > search.html"
},
{
    
    "uri": "/linux/network/ssh--verification-de-la-cle-host-echoue.html",
    "title": "SSH Vérification de la clé hôte échoué",
    "tags": ["linux", "network", "ssh", "security"],
    "description": "",
    "content": " Symptome login@debian:~$ ssh server-ssh @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY! Someone could be eavesdropping on you right now (man-in-the-middle attack)! It is also possible that the RSA host key has just been changed. The fingerprint for the RSA key sent by the remote host is d7:16:94:8f:b9:e3:b0:16:3e:fc:e3:65:ba:d3:b4:1d. Please contact your system administrator. Add correct host key in /home/login/.ssh/known_hosts to get rid of this message. Offending key in /home/login/.ssh/known_hosts:3 RSA host key for server-ssh has changed and you have requested strict checking. Host key verification failed.  On peut prendre cette erreur quand on se connecte a une nouvelle VM qui a pris l\u0026rsquo;IP d\u0026rsquo;une ancienne VM à laquelle on c\u0026rsquo;était connecté il y a longtemps. En gros, SSH dit que le serveur se présente plus avec la même clé et que c\u0026rsquo;est pas normal.\n## Résolution La ligne intéressante c\u0026rsquo;est ça :\nOffending key in /home/login/.ssh/known_hosts:3  Il faut supprimer l\u0026rsquo;ancienne référence à l’hôte 3 dans le fichier .ssh/known_hosts\nsed -i \u0026#34;3 d\u0026#34; .ssh/known_hosts", 
    "breadcrumb": " > linux > network > ssh--verification-de-la-cle-host-echoue.html"
},
{
    
    "uri": "/linux/network/ssh-reprendre-la-main-sur-un-session-perdue.html",
    "title": "SSH: Reprendre la main sur un session perdue",
    "tags": ["linux", "network", "ssh"],
    "description": "",
    "content": "Il arrive de perdre la main sur un session SSH pour différentes raisons. Dans ce cas il n\u0026rsquo;est plus possible de faire quoi que ce soir dans la console. Une manière simple de tuer la session sans tuer le terminal est de faire la suite de touche suivante :\nENTER , '~' , '.'\n", 
    "breadcrumb": " > linux > network > ssh-reprendre-la-main-sur-un-session-perdue.html"
},
{
    
    "uri": "/linux/network/scanner-les-ports-sur-une-ip.html",
    "title": "Scanner les ports sur une IP",
    "tags": ["linux", "network", "security"],
    "description": "",
    "content": "Une commande toute conne permet de scanner les ports ouvert sur une machine et donne des tas d\u0026rsquo;information sur ce qui se trouve de l\u0026rsquo;autre coté :\nnmap -sV ip-a-scanner -vvv Un truc qui peut servir pour lutter contre ça : http://portspoof.org/ ca va générer n\u0026rsquo;importe quoi sur tout les ports qu\u0026rsquo;ils soient ouvert ou fermé.\n", 
    "breadcrumb": " > linux > network > scanner-les-ports-sur-une-ip.html"
},
{
    
    "uri": "/tags/scp.html",
    "title": "Scp",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > scp.html"
},
{
    
    "uri": "/linux/gnomeshell/screencast-avec-gnome-shell.html",
    "title": "Screencast avec gnome-shell",
    "tags": ["linux", "gnome", "shell", "debian"],
    "description": "",
    "content": "Un truc très sympa, Gnome-Shell intégre un outils de screencast. En effet, il suffit de faire Ctrl + Shift + Alt + R pour démarrer et arréter l\u0026rsquo;enregistrement. A la fin de l\u0026rsquo;enregistrement, le fichier résultat est déposé dans le répertoire Vidéos de l\u0026rsquo;utilisateur. Le format d\u0026rsquo;enregistrement est WebM.\nPar défaut, la durée maximum d\u0026rsquo;enregistrement est de 30s, au dela l\u0026rsquo;enregistrement s\u0026rsquo;arrête de lui même. Pour changer cette durée max, il est possible de mettre à jour les settings :\ngsettings set org.gnome.settings-daemon.plugins.media-keys max-screencast-length 120 La nouvelle durée sera donc de 2mn. Pour vérifier la durée max :\ngsettings get org.gnome.settings-daemon.plugins.media-keys max-screencast-length", 
    "breadcrumb": " > linux > gnomeshell > screencast-avec-gnome-shell.html"
},
{
    
    "uri": "/tags/script.html",
    "title": "Script",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > script.html"
},
{
    
    "uri": "/tags/security.html",
    "title": "Security",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > security.html"
},
{
    
    "uri": "/tags/secutity.html",
    "title": "Secutity",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > secutity.html"
},
{
    
    "uri": "/tags/segfault.html",
    "title": "Segfault",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > segfault.html"
},
{
    
    "uri": "/tags/server.html",
    "title": "Server",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > server.html"
},
{
    
    "uri": "/tags/service.html",
    "title": "Service",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > service.html"
},
{
    
    "uri": "/tags/shell.html",
    "title": "Shell",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > shell.html"
},
{
    
    "uri": "/tags/shortcut.html",
    "title": "Shortcut",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > shortcut.html"
},
{
    
    "uri": "/tags/soapui.html",
    "title": "Soapui",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > soapui.html"
},
{
    
    "uri": "/tags/soja.html",
    "title": "Soja",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > soja.html"
},
{
    
    "uri": "/tags/spring.html",
    "title": "Spring",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > spring.html"
},
{
    
    "uri": "/tags/sql.html",
    "title": "Sql",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > sql.html"
},
{
    
    "uri": "/tags/squid.html",
    "title": "Squid",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > squid.html"
},
{
    
    "uri": "/linux/divers/squid-storeid-rewrite.html",
    "title": "Squid StoreId rewrite",
    "tags": ["linux", "misc", "squid", "tools", "proxy"],
    "description": "",
    "content": "Un truc que j\u0026rsquo;ai cherché à faire avec Squid est de mettre en cache les téléchargements que docker fait pour construire ses images afin qu\u0026rsquo;il ne re-télécharge pas systématiquement tout.\nPas compliqué, il suffit d\u0026rsquo;augmenter la taille max des fichiers a cacher et du répertoire de cache :\ncache_dir ufs /var/spool/squid3 5000 16 256 maximum_object_size 400 MB Mais j\u0026rsquo;ai surtout eu un problème avec le téléchargement des JRE \u0026amp; JDK Oracle. En effet, le lien demande une authentification et redirige alors vers la même URL mais avec une query-string :\nhttp://download.oracle.com/otn-pub/java/jdk/7u75-b13/jre-7u75-linux-x64.tar.gz  devient\nhttp://download.oracle.com/otn-pub/java/jdk/7u75-b13/jre-7u75-linux-x64.tar.gz?AuthParam=jkhefuihzefglkjhazfligezkfg  Et biensur le AuthParam change à chaque fois. Du coup Squid considère le fichier comme un nouveau fichier et le remet en cache a chaque fois. C\u0026rsquo;est que qu\u0026rsquo;on appelle la duplication. Pas gênant pour les petits fichiers, beaucoup plus pour les gros.\nPour palier ce problème dans Squid 3.4 (pas avant) il est possible de re-écrire le StoreId. Pour cela, il faut commence par récupérer le programme perl store-id.pl que je rajoute ici au cas où :\n{% include_relative .doc/store-id.pl %} Attention, ce fichier doit être exécutable ! Ensuite dans le fichier squid.conf on active la mise en cache des query-string en remplaçant :\nrefresh_pattern -i (/cgi-bin/|\\?) 0 0% 0 par\nrefresh_pattern -i cgi-bin 0 0% 0 Puis on ajoute les lignes suivantes :\nstore_id_program /usr/local/squid/store-id.pl /usr/local/squid/store_id_db store_id_children 5 startup=1  store_id_program est le chemin vers le programme perl avec en argument le fichier de mapping des urls store_id_children permet de paramétrer les sous-process, 5 max, 1 au départ.  Reste enfin le fichier de mappin d\u0026rsquo;URL a ajouter. Il est sous la forme : * Regex de l\u0026rsquo;url * tabulation * URL re-ecrite\nExemple :\n^http:\\/\\/download\\.oracle\\.com\\/otn\\-pub\\/java\\/([a-zA-Z0-9\\/\\.\\-\\_]+\\.(tar\\.gz))\thttp://download.oracle.com/otn-pub/java/$1  Attention que la tabulation ne soit pas remplacer par défaut par votre éditeur de texte. Pour être sûr, on peut utiliser cette commande :\ncat dbfile | sed -r -e \u0026#39;s/\\s+/\\t/g\u0026#39; |sed \u0026#39;/^\\#/d\u0026#39; \u0026gt;cleaned_db_file Cela va nettoyer le fichier de base pour être certain qu\u0026rsquo;il soit bon. Tout un listing d\u0026rsquo;url est donné en exemple dans la doc Squid : http://wiki.squid-cache.org/Features/StoreID/DB\n", 
    "breadcrumb": " > linux > divers > squid-storeid-rewrite.html"
},
{
    
    "uri": "/linux/divers/squid-refresh-pattern.html",
    "title": "Squid refresh pattern",
    "tags": ["linux", "misc", "squid", "tools", "proxy"],
    "description": "",
    "content": " Dans le cadre de mon serveur proxy pour fichier téléchargés, je suis tombé sur un problème avec neo4j dont les fichiers tar.gz n\u0026rsquo;étaient jamais mis en cache.\nSymptômes Le premier symptôme est que le fichier est systématiquement re-téléchargé.\nDeuxième symptôme, les logs store affichent que le fichier est immédiatement RELEASE et que sa date de péremption est passé.\nExplication En fait, dans l\u0026rsquo;entête de la requête HTTP, il y a une directive\nCache-Control \u0026quot;must-revalidate\u0026quot;  Ce qui implique de récupérer le fichier à chaque fois. Pour régler ça, il est possible de demander à Squid d\u0026rsquo;ignorer certaine entêtes. C\u0026rsquo;est pas conseillé mais dans le cadre un proxy de téléchargement, c\u0026rsquo;est pas interdit.\nPour faire ça on met à jour les refresh pattern :\nrefresh_pattern ^ftp: 1440 20% 10080 refresh_pattern ^gopher: 1440 0% 1440 refresh_pattern Packages\\.bz2$ 0 20% 4320 refresh-ims refresh_pattern Sources\\.bz2$ 0 20% 4320 refresh-ims refresh_pattern Release\\.gpg$ 0 20% 4320 refresh-ims refresh_pattern Release$ 0 20% 4320 refresh-ims refresh_pattern -i cgi-bin 0 0% 0 refresh_pattern . 1440 20% 10080 override-expire override-lastmod ignore-no-store ignore-must-revalidate ignore-private ignore-auth  Ici pour toutes les URL on ignore les directives de cache.\nLiens  http://www.squid-cache.org/Doc/config/refresh_pattern/  ", 
    "breadcrumb": " > linux > divers > squid-refresh-pattern.html"
},
{
    
    "uri": "/tags/ssh.html",
    "title": "Ssh",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > ssh.html"
},
{
    
    "uri": "/tags/ssl.html",
    "title": "Ssl",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > ssl.html"
},
{
    
    "uri": "/tags/startup.html",
    "title": "Startup",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > startup.html"
},
{
    
    "uri": "/outils/vmware/stopper-une-vm-sur-le-serveur-en-ligne-de-commande.html",
    "title": "Stopper une VM sur le serveur en ligne de commande",
    "tags": ["outils", "vmware"],
    "description": "",
    "content": "En tant que root :\nvmrun -T server -h https://localhost:8333/sdk -u root -p \u0026lt;passroot\u0026gt; stop \u0026#34;[standard] Win2003Srv/CameleonEdge.vmx\u0026#34; Note : vmrun permet de faire pas mal de truc sur les VMs d\u0026rsquo;un serveur, vmrun -? donne des exemples.\n", 
    "breadcrumb": " > outils > vmware > stopper-une-vm-sur-le-serveur-en-ligne-de-commande.html"
},
{
    
    "uri": "/tags/string.html",
    "title": "String",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > string.html"
},
{
    
    "uri": "/development/java/stringindexoutofboundsexception-dans-ivy.html",
    "title": "StringIndexOutOfBoundsException dans Ivy",
    "tags": ["development", "java", "ivy"],
    "description": "",
    "content": "Quand on utilise un serveur Ivy perso en plus d\u0026rsquo;un repo local. Qu\u0026rsquo;une des dépendances est en \u0026ldquo;latest.integration\u0026rdquo; et que le repo distant est accédé en SSH, on peut prendre une erreur ~StringIndexOutOfBoundsException disant qu\u0026rsquo;il y a un problème lors du listing du répertoire.\nCela vient du SSH. J\u0026rsquo;ai pas le détail mais on changeant dans le ivysettings.xml, au niveau du repo distant, \u0026ldquo;ssh\u0026rdquo; par \u0026ldquo;sftp\u0026rdquo;, ca se met à fonctionner correctement.\n", 
    "breadcrumb": " > development > java > stringindexoutofboundsexception-dans-ivy.html"
},
{
    
    "uri": "/tags/sudo.html",
    "title": "Sudo",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > sudo.html"
},
{
    
    "uri": "/linux/shell/massive-file-deletion.html",
    "title": "Suppression de fichiers en masse",
    "tags": ["linux", "shell", "bash"],
    "description": "",
    "content": "Supprimer des milliers de fichiers en même temps est assez compliqué. Plusieurs solutions :\n rm -rf /mon/repertoire find /mon/repertoire/* -type f -mtime +3 -delete rsync -a --delete /tmp/empty /mon/repertoire/  le plus rapide est clairement le rsync !\n", 
    "breadcrumb": " > linux > shell > massive-file-deletion.html"
},
{
    
    "uri": "/linux/shell/remove-file-by-inode.html",
    "title": "Suppression de fichiers par Inode",
    "tags": ["linux", "shell", "inode"],
    "description": "",
    "content": "Il m’est arrivé l’autre jour de créer par accident un fichier \u0026lsquo;-I\u0026rsquo; et là, c’est le drame !\nImpossible de supprimer ce fichier par les rm conventionnel.\nLa solution c’est de supprimer le fichier par son inode :\nls -il find . -inum 782263 -exec rm -i {} \\; Et voilà.\n", 
    "breadcrumb": " > linux > shell > remove-file-by-inode.html"
},
{
    
    "uri": "/outils/docker/supprimer-les-images-non-tagge.html",
    "title": "Supprimer les images non taggé",
    "tags": ["development", "docker", "cleanup"],
    "description": "",
    "content": "Pour virer toutes les images non taggé :\ndocker rmi $(docker images | grep \u0026#39;^\u0026lt;none\u0026gt;\u0026#39; | awk \u0026#39;{print $3}\u0026#39;) Attention si l\u0026rsquo;image est utilisé par un conteneur ça marchera pas.\nA partir de la 1.3.1 :\ndocker rmi $(docker images -f \u0026#34;dangling=true\u0026#34; -q)", 
    "breadcrumb": " > outils > docker > supprimer-les-images-non-tagge.html"
},
{
    
    "uri": "/outils/docker/supprimer-les-vieux-conteneurs.html",
    "title": "Supprimer les vieux conteneurs",
    "tags": ["development", "docker", "cleanup"],
    "description": "",
    "content": "Pour supprimer les vieux conteneur de plus d\u0026rsquo;une semaine par exemple :\ndocker ps -a | grep \u0026#39;weeks ago\u0026#39; | awk \u0026#39;{print $1}\u0026#39; | xargs docker rm -v  Les conteneurs issue d\u0026rsquo;images non taggé :\ndocker ps -a | awk \u0026#39;$2 ~ \u0026#34;[0-9a-f]{12}\u0026#34; {print $\u0026#34;$1\u0026#34;}\u0026#39; docker ps -a | awk \u0026#39;$2 ~ /^[0-9a-f]+$/ {print $1}\u0026#39; | xargs docker rm -v  Et pour virer tous les conteneurs arrété :\ndocker rm -v $(docker ps -a -q) Remarque: le -v permet de supprimer aussi les volumes déclaré mais non monté qui se trouvent alors dans /var/lib/docker sans quoi ils restent orphelin et sont alors compliqué a nettoyer !\n", 
    "breadcrumb": " > outils > docker > supprimer-les-vieux-conteneurs.html"
},
{
    
    "uri": "/linux/shell/supprimer-un-type-de-fichier-dans-une-arborescence.html",
    "title": "Supprimer un type de fichier dans une arborescence",
    "tags": ["linux", "shell", "cleanup"],
    "description": "",
    "content": "Par exemple les fichiers générés par VSS :\nrm $(find . -name *.scc)", 
    "breadcrumb": " > linux > shell > supprimer-un-type-de-fichier-dans-une-arborescence.html"
},
{
    
    "uri": "/tags/swap.html",
    "title": "Swap",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > swap.html"
},
{
    
    "uri": "/linux/shell/synchronisation-rsync.html",
    "title": "Synchronisation Rsync",
    "tags": ["linux", "shell", "rsync"],
    "description": "",
    "content": " Très pratique quand on change de PC par exemple et que l\u0026rsquo;on veux copier sa centaine de gigs du vieux PV au nouveau.\nAu travers d\u0026rsquo;une connexion directe RJ45 :\nrsync -az --size-only --delete /home/kevin/source/* kevin@server.example.com:/home/kevin/destination/  -a archive, conserve tout les attribut des fichiers en l\u0026rsquo;état (date, owner, \u0026hellip;) -z active la compression --size-only ne teste que la taille du fichier pour savoir s\u0026rsquo;il doit être mis à jour. C\u0026rsquo;est plus rapide que le Hash. --delete supprime de la destination les fichiers qui l\u0026rsquo;ont été de la source  Autres options intéressantes  --delay-updates Copie tout les fichiers à transférer dans un répertoire temporaire et les transfère à la fin. Pratique sur des environnements de prod. --exclude-from=/root/sync_exclude liste des patterns de fichiers à exclure de la copie.  Autre point intéressant, il est facile de mettre cette ligne de commande dans un cron. Pour se dispenser du mot de passe on peut alors utiliser l\u0026rsquo;identification par clé de ssh.\n", 
    "breadcrumb": " > linux > shell > synchronisation-rsync.html"
},
{
    
    "uri": "/tags/sysadmin.html",
    "title": "Sysadmin",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > sysadmin.html"
},
{
    
    "uri": "/tags/sysadmin-ipp.html",
    "title": "Sysadmin Ipp",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > sysadmin-ipp.html"
},
{
    
    "uri": "/outils/docker/system-error-on-docker-run.html",
    "title": "System error on docker run",
    "tags": ["development", "docker", "error"],
    "description": "",
    "content": " Après une mise à jour du système, au démarrage de mon container docker je prend l\u0026rsquo;erreur suivante :\nError response from daemon: Cannot start container {id}: [8] System error: open /sys/fs/cgroup/cpu,cpuacct/init.scope/system.slice/docker-{id}.scope/cpu.shares: no such file or directory \nC\u0026rsquo;est apparement lié a un bug Debian : https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=798778\nIl est possible de palier en ajoutant --exec-opt native.cgroupdriver=cgroupfs dans /etc/default/docker.\nLiens  https://stackoverflow.com/questions/32845917/system-error-on-docker-run  ", 
    "breadcrumb": " > outils > docker > system-error-on-docker-run.html"
},
{
    
    "uri": "/tags/systemd.html",
    "title": "Systemd",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > systemd.html"
},
{
    
    "uri": "/tags.html",
    "title": "Tags",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags.html"
},
{
    
    "uri": "/cuisine/tataki-de-thon.html",
    "title": "Tataki de thon",
    "tags": ["cuisine", "cook", "thon", "soja", "poisson"],
    "description": "",
    "content": " Marinade  Sauce soja Demi citron pressé Gingembre Filet d’huile de sésame  Faire mariner au moins une heure.\nPréparation Dans la poele, à feu vif, faire griller les graine de sésame.\nPuis une fois roussie, ajouter un filet d’huile d’olive et mettre le thon.\nLe faire griller le thon sur la première face.\nLe retourner pour ajouter un peu de la marinade pour faire de la sauce.\nLaisser roussir 2mn et c’est prêt.\n", 
    "breadcrumb": " > cuisine > tataki-de-thon.html"
},
{
    
    "uri": "/tags/tcpdump.html",
    "title": "Tcpdump",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > tcpdump.html"
},
{
    
    "uri": "/outils/ansible/template-error-while-templating-string.html",
    "title": "Template error while templating string: Missing end of comment tag",
    "tags": ["ansible", "jinja"],
    "description": "",
    "content": "C\u0026rsquo;est une erreur qui se produit lorsque l\u0026rsquo;on essaye de templatizer un script bash contenant un calcul de taille de table genre ${#modules[@]}. Jinja le prend pour un commentaire non terminé {# comment #} et ça marche pas.\nPour corriger : {% raw %}{{ '${#modules[@]}' }}{% endraw %}.\n", 
    "breadcrumb": " > outils > ansible > template-error-while-templating-string.html"
},
{
    
    "uri": "/tags/teradata.html",
    "title": "Teradata",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > teradata.html"
},
{
    
    "uri": "/tags/test.html",
    "title": "Test",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > test.html"
},
{
    
    "uri": "/development/java/tester-des-membres-private.html",
    "title": "Tester des membres private",
    "tags": ["development", "java", "test", "reflection"],
    "description": "",
    "content": "Un des truc compliqué quand on fait des test unitaire c\u0026rsquo;est de tester les membres private. Un solution pour faire ça :\n/** * Gets the field value from an instance. The field we wish to retrieve is * specified by passing the name. The value will be returned, even if the * field would have private or protected access. */ private Object getField( Object instance, String name ) throws Exception { Class c = instance.getClass(); // Retrieve the field with the specified name \tField f = c.getDeclaredField( name ); // *MAGIC* make sure the field is accessible, even if it \t// would be private or protected \tf.setAccessible( true ); // Return the value of the field for the instance \treturn f.get( instance ); } Ensuite on mets dans le test :\npublic void testLengthAndCalled() throws Exception { Demo demo\t= new Demo(); // Retrieve the wasCalled field \tBoolean wasCalled\t= (Boolean) getField( demo, \u0026#34;wasCalled\u0026#34; ); // Should be false before calling \tassertFalse( wasCalled.booleanValue() ); // Call the private stringLength method \tInteger strlen\t= (Integer) executeMethod( demo, \u0026#34;stringLength\u0026#34;, new Object[] { \u0026#34;four\u0026#34; } ); // The value returned should be \u0026#39;4\u0026#39; (length of the string \u0026#39;four\u0026#39;) \tassertEquals( 4, strlen.intValue() ); // Even though Boolean is a non-primitive and uses a reference, \t// it was a one-time object created with the actual primitive boolean \t// value of demo, so we must fetch it again \twasCalled\t= (Boolean) getField( demo, \u0026#34;wasCalled\u0026#34; ); // THe method has now been called \tassertTrue( wasCalled.booleanValue() ); }", 
    "breadcrumb": " > development > java > tester-des-membres-private.html"
},
{
    
    "uri": "/development/javascript/tester-un-numerique-en-javascript.html",
    "title": "Tester un numérique en JavaScript",
    "tags": ["development", "javascript", "type"],
    "description": "",
    "content": " Après des tas et des tas de tests, voici le façon de tester un numérique en [[JavaScript|javascript]] que passe le plus de cas :\n/** * Check if value is numerical or not * @param strString\tValue to check * @returns\tTrue if value is numerical, False otherwise */ mgpRemainsDependant.isNumeric = function (strString) { return !isNaN(parseFloat(strString)) \u0026amp;\u0026amp; isFinite(strString); };  Attention tout de même, les nombres en hexa du type 0xFF passent \u0026hellip;\nLiens  http://dl.dropbox.com/u/35146/js/tests/isNumber.html  ", 
    "breadcrumb": " > development > javascript > tester-un-numerique-en-javascript.html"
},
{
    
    "uri": "/tags/textlive.html",
    "title": "Textlive",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > textlive.html"
},
{
    
    "uri": "/tags/thon.html",
    "title": "Thon",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > thon.html"
},
{
    
    "uri": "/tags/timezone.html",
    "title": "Timezone",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > timezone.html"
},
{
    
    "uri": "/serveurs/mysql/timezoner-mysql.html",
    "title": "Timezoner MySQL",
    "tags": ["server", "mysql", "database", "jdbc", "java"],
    "description": "",
    "content": "Dans le cas d\u0026rsquo;un MySQL sous docker, le server MySQL n\u0026rsquo;est pas Timezoné correctement. Par exemple, quand on lance la requête suivante :\nmysql\u0026gt; SELECT @@global.time_zone, @@session.time_zone; +--------------------+---------------------+ | @@global.time_zone | @@session.time_zone | +--------------------+---------------------+ | SYSTEM | SYSTEM | +--------------------+---------------------+ 1 row in set (0.00 sec) Ce qui signifie que c\u0026rsquo;est la Timezone du système qui est utilisé. Un\ndate +%Z CEST nous confirme que nous sommes bien sur Europe/Paris.\nPourtant quand on insère des dates en base via un champs Timestamp, on se retrouve avec un décallage de 2 heures soit une timezone UTC ?\nL\u0026rsquo;explication dans le cas qui nous intéresse, c\u0026rsquo;est que la table de Timezone de MySQL est vide. De ce fait, elle ne comprend pas la timezone que le système lui fourni et passe par défaut en UTC. La solution pour être certain de sa Timezone est ce la setter correctement :\n Dans un terminal:  mysql_tzinfo_to_sql /usr/share/zoneinfo | mysql -u root mysql qui va mettre à jour la table de zonage de MySQL * Dans MySQL:\nSET GLOBAL time_zone = \u0026#39;Europe/Paris\u0026#39;; SET time_zone = \u0026#39;Europe/Paris\u0026#39;;", 
    "breadcrumb": " > serveurs > mysql > timezoner-mysql.html"
},
{
    
    "uri": "/tags/tmux.html",
    "title": "Tmux",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > tmux.html"
},
{
    
    "uri": "/linux/shell/tmux-cheatcheet.html",
    "title": "Tmux cheatcheet",
    "tags": ["linux", "shell", "tmux"],
    "description": "",
    "content": "  Do not remove {:toc}  Sessions New Session  tmux new [-s name] [cmd] (:new) - new session  Switch Session  tmux ls (:ls) - list sessions tmux switch [-t name] (:switch) - switches to an existing session tmux as [id] [-t name] (:attach) - attaches to an existing session \u0026lt;C-a\u0026gt;c (:detach) - detach the currently attached session  Session Management  \u0026lt;C-a\u0026gt;s - list sessions \u0026lt;C-a\u0026gt;$ - name session  Close Session  tmux kill-session [-t name] (:kill-session)  Windows New Window  \u0026lt;C-a\u0026gt;c (:neww [-n name] [cmd]) - new window  Cursor Movement  \u0026lt;C-a\u0026gt;[i] (:selectw -t [i]) - go to window [i] \u0026lt;C-a\u0026gt;l - go to last window \u0026lt;C-a\u0026gt;p - go to previous window \u0026lt;C-a\u0026gt;n - go to next window  Window Management  \u0026lt;C-a\u0026gt;T - rename window \u0026lt;C-a\u0026gt;, - rename window \u0026lt;C-a\u0026gt;w - list all windows \u0026lt;C-a\u0026gt;f - find window by name \u0026lt;C-a\u0026gt;. - move window to another session (promt) :movew - move window to next unused number  Close Window  \u0026lt;C-a\u0026gt;\u0026amp; (:kill-window) - kill window  Panes New Pane  (%) \u0026lt;C-a\u0026gt;| (:splitw [-v] [-p width] [-t focus] [cmd]) - split current pane vertically (\u0026ldquo;) \u0026lt;C-a\u0026gt;s (:splitw -h [-p width] [-t focus] [cmd]) - split current pane horizontally  Cursor Movement  (o) \u0026lt;C-a\u0026gt;\u0026lt;Tab\u0026gt; (:selectp -t :.+) - move cursor to the next pane \u0026lt;C-a\u0026gt;\u0026lt;Up\u0026gt; (:selectp -U) - move cursor to the pane above \u0026lt;C-a\u0026gt;\u0026lt;Down\u0026gt; (:selectp -D) - move cursor to the pane below \u0026lt;C-a\u0026gt;\u0026lt;Left\u0026gt; (:selectp -L) - move cursor to the pane to the left \u0026lt;C-a\u0026gt;\u0026lt;Right\u0026gt; (:selectp -R) - move cursor to the pane to the right :selectp [i] - move cursor to the pane [i]  Panes Management  (:swap-pane -U) - move current pane up (:swap-pane -D) - move current pane down \u0026lt;C-a\u0026gt;{ (:swap-pane -L) - move current pane to the left \u0026lt;C-a\u0026gt;} (:swap-pane -R) - move current pane to the right \u0026lt;C-a\u0026gt;q - show pane numbers (type number to move cursor) \u0026lt;C-a\u0026gt;\u0026lt;Space\u0026gt; - toggle pane arrangements  Resize Pane  :resize-pane -U [i] - move horizontal divider up by [i] lines :resize-pane -D [i] - move horizontal divider down by [i] lines :resize-pane -L [i] - move vertical divider left by [i] columns :resize-pane -R [i] - move vertical divider right by [i] columns  resize-pane [-DLRUZ] [-x width] [-y height] [-t target-pane] [adjustment]\nClose Pane  \u0026lt;C-a\u0026gt;x (:kill-pane) - kill current pane  Misc  \u0026lt;C-a\u0026gt;t - show time \u0026lt;C-a\u0026gt;r - reload config  Sources  http://robots.thoughtbot.com/post/2641409235/a-tmux-crash-course https://wiki.archlinux.org/index.php/Tmux https://gist.github.com/henrik/1967800 http://blog.hawkhost.com/2010/06/28/tmux-the-terminal-multiplexer/ https://gist.github.com/Starefossen/5955406  ", 
    "breadcrumb": " > linux > shell > tmux-cheatcheet.html"
},
{
    
    "uri": "/tags/tomcat.html",
    "title": "Tomcat",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > tomcat.html"
},
{
    
    "uri": "/tags/tools.html",
    "title": "Tools",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > tools.html"
},
{
    
    "uri": "/serveurs/oracle/tracer-une-requete-oracle.html",
    "title": "Tracer une requête Oracle",
    "tags": ["server", "oracle", "database", "tunning", "sql"],
    "description": "",
    "content": " Il arrive qu\u0026rsquo;une partie de l\u0026rsquo;appli (voire toute l\u0026rsquo;appli) rame particulièrement. Selon la version d\u0026rsquo;Oracle, il est plus ou moins facile de tracer une requête. Dans tout les cas, il peut arrivé qu\u0026rsquo;on est besoin d\u0026rsquo;un maximum d\u0026rsquo;info.\nOracle 10g et Entreprise Manager Sur un serveur 10g, le plus simple est d\u0026rsquo;ouvrir l\u0026rsquo;entreprise manager et de cliquer sur l\u0026rsquo;onglet performances. De là, sous le graphique, un lien \u0026ldquo;Sessions les plus consommatrices\u0026hellip;\u0026rdquo; qui permet de visualiser les requêtes et les sessions les plus gourmandes.\nOracle 8i et 9i Dans ces versions, on n\u0026rsquo;a pas les outils de la 10g et la recherche s\u0026rsquo;avère plus compliqué. La solution la plus simple est de tracer les sessions (voire toute la base). Pour cela, voilà la marche à suivre. L\u0026rsquo;intérêt de cette procédure un peu lourde est qu\u0026rsquo;il n\u0026rsquo;est pas nécessaire de redémarrer l\u0026rsquo;instance et la trace ne s\u0026rsquo;applique que sur une seule session.\nDéjà, faut trouver la session qui va bien :\nSQL\u0026gt; select sid, serial# from v$session where username = \u0026#39;CCS43\u0026#39;; S\u0026rsquo;il y en a plusieurs, on peut filtrer sur des champs comme PROGRAM ou MACHINE.\nEnsuite on vide le répertoire de trace du serveur, en général ORACLE_HOME/admin/ORCL/udump\nPuis on démarre la trace sur la session :\nSQL\u0026gt; exec DBMS_SYSTEM.SET_SQL_TRACE_IN_SESSION (sid=\u0026gt;507, serial#=\u0026gt;4957,sql_trace=\u0026gt;TRUE); On lance les requêtes que l\u0026rsquo;on veut tracer ou on va dans la partie de l\u0026rsquo;appli qui rame.\nOn récupère le fichier .trc généré dans le répertoire de trace\nOn arrête la trace :\nSQL\u0026gt; exec DBMS_SYSTEM.SET_SQL_TRACE_IN_SESSION (sid=\u0026gt;507, serial#=\u0026gt;4957,sql_trace=\u0026gt;FALSE);\u0026lt;/code\u0026gt; On tkprof le fichier TRC pour avoir un truc lisible.\nPS : On peut aussi exécuter un\nSQL\u0026gt; alter system set timed_statistics = TRUE; Cela va permettre de connaitre les temps de réponse d\u0026rsquo;oracle, le temps de parsing ou d\u0026rsquo;exécution de chaque requête. Ca permet lors du tkprof de trier par temps d\u0026rsquo;exécution pour avoir les plus grosse requêtes en haut du fichier.\nLiens  http://alexzeng.wordpress.com/2008/08/01/how-to-trace-oracle-sessions/ http://www.oradev.com/create_statistics.jsp  ", 
    "breadcrumb": " > serveurs > oracle > tracer-une-requete-oracle.html"
},
{
    
    "uri": "/development/java/transformer-une-url-java-en-chemin-complet.html",
    "title": "Transformer une URL java en chemin complet",
    "tags": ["development", "java", "url"],
    "description": "",
    "content": "// w_fileURL est l\u0026#39;URL d\u0026#39;un fichier sur le serveur String w_realPath = request.getSession().getServletContext().getRealPath(\u0026#34;/\u0026#34;); String w_filePath = w_fileURL.replaceFirst(getSettings(request).getCodeBase(), \u0026#34;\u0026#34;); w_filePath = w_realPath+w_filePath.replaceAll(\u0026#34;[/\\\\\\\\]+\u0026#34;, \u0026#34;\\\\\u0026#34; + File.separator); Bien-sur ça marche si l\u0026rsquo;URL est l\u0026rsquo;URL d\u0026rsquo;un fichier sur le serveur ou la servlet s\u0026rsquo;exécute. Dans le cas présent, c\u0026rsquo;est le fichier d\u0026rsquo;un OMM.\n", 
    "breadcrumb": " > development > java > transformer-une-url-java-en-chemin-complet.html"
},
{
    
    "uri": "/linux/shell/trouver-les-uuid-de-mes-partitions.html",
    "title": "Trouver les UUID de mes partitions",
    "tags": ["linux", "shell", "uuid", "hdd"],
    "description": "",
    "content": "ls -l /dev/disk/by-uuid total 0 lrwxrwxrwx 1 root root 10 2009-11-05 14:10 4d7f0ae6-3945-41e2-9123-76a3858bc68a -\u0026gt; ../../sda1 lrwxrwxrwx 1 root root 10 2009-11-05 14:10 d9d24356-9c59-4888-9b00-6a316deb8aba -\u0026gt; ../../sda5", 
    "breadcrumb": " > linux > shell > trouver-les-uuid-de-mes-partitions.html"
},
{
    
    "uri": "/linux/shell/trouver-les-gros-fichiers.html",
    "title": "Trouver les gros fichiers",
    "tags": ["linux", "shell", "cleanup"],
    "description": "",
    "content": "Un ligne de commande bien pratique pour ça :\ndu -hms /* | sort -nr | head Ca ne donne que le premier niveau de hiérarchie, il faudra relancer la commande pour affiner le recherche par sous-dossier.\n", 
    "breadcrumb": " > linux > shell > trouver-les-gros-fichiers.html"
},
{
    
    "uri": "/outils/trucs-et-astuce-soapui.html",
    "title": "Trucs et Astuce SoapUI",
    "tags": ["linux", "misc", "soapui", "tools", "ws"],
    "description": "",
    "content": " SoapUI à 100% de CPU Sous Linux il peut arriver (souvent) que SoapUI monte à 100% de CPU et y reste \u0026hellip;\nEn fouillant un peu sur internet, on trouve que ça vient de de JxBrowser et par chance il est possible de le désactiver en modifiant le fichier de lancement .sh :\n#uncomment to disable browser component  JAVA_OPTS=\u0026#34;$JAVA_OPTS-Dsoapui.jxbrowser.disable=true\u0026#34;", 
    "breadcrumb": " > outils > trucs-et-astuce-soapui.html"
},
{
    
    "uri": "/tags/try.html",
    "title": "Try",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > try.html"
},
{
    
    "uri": "/tags/tunning.html",
    "title": "Tunning",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > tunning.html"
},
{
    
    "uri": "/tags/type.html",
    "title": "Type",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > type.html"
},
{
    
    "uri": "/development/java/telecharger-java-en-une-ligne.html",
    "title": "Télécharger Java en une ligne",
    "tags": ["development", "java", "download"],
    "description": "",
    "content": "Pour télécharger java en une ligne de commande avec wget :\nwget --no-check-certificate --no-cookies - --header \u0026#34;Cookie: oraclelicense=accept-securebackup-cookie\u0026#34; http://download.oracle.com/otn-pub/java/jdk/7u51-b13/jdk-7u51-linux-x64.tar.gz Changer pour le JDK que l\u0026rsquo;on veut.\n", 
    "breadcrumb": " > development > java > telecharger-java-en-une-ligne.html"
},
{
    
    "uri": "/linux/shell/tele-charger-un-repertoire-entier-via-ftp.html",
    "title": "Télécharger un répertoire entier via FTP",
    "tags": ["linux", "shell", "wget", "network"],
    "description": "",
    "content": "wget -r --restrict-file-names=nocontrol ftp://username:password4@www.monserveur.com/monrepertoire", 
    "breadcrumb": " > linux > shell > tele-charger-un-repertoire-entier-via-ftp.html"
},
{
    
    "uri": "/serveurs/mysql/uuid-most-significant-bits.html",
    "title": "UUID Most Significant Bits",
    "tags": ["server", "mysql", "database", "uuid"],
    "description": "",
    "content": "Comment dans une requête MySQL peut on extraire les bits les plus significatif d\u0026rsquo;un UUID. En gros ça correspond en Java à UUID.randomUUID().getMostSignificantBits().\nSELECT -conv(substring_index(uuid(), \u0026#39;-\u0026#39;, 1), 16, 10) Ce qui donne un long !\n", 
    "breadcrumb": " > serveurs > mysql > uuid-most-significant-bits.html"
},
{
    
    "uri": "/linux/shell/unifier-des-pdfs.html",
    "title": "Unifier des PDFs",
    "tags": ["linux", "shell", "pdf"],
    "description": "",
    "content": "pdfunite in-1.pdf in-2.pdf in-n.pdf out.pdf", 
    "breadcrumb": " > linux > shell > unifier-des-pdfs.html"
},
{
    
    "uri": "/outils/docker/update-toutes-les-images.html",
    "title": "Update toutes les images",
    "tags": ["development", "docker", "error"],
    "description": "",
    "content": "Pour mettre à jour toutes les images d\u0026rsquo;un docker en une commande:\ndocker images | awk \u0026#39;{print $1}\u0026#39; | xargs -L1 docker pull", 
    "breadcrumb": " > outils > docker > update-toutes-les-images.html"
},
{
    
    "uri": "/misc/upload-file-to-nexus-repository.html",
    "title": "Upload file to Nexus Repository",
    "tags": ["misc", "cpl", "nexus", "java", "proxy"],
    "description": "",
    "content": "Il y a une feature intéressante sur les serveurs Nexus qui semble pas forcément très utilisé. Il est possible de créer une Repository de type site. Ca donne un repo qui héberge des fichiers statique et c\u0026rsquo;est pratique pour stocker par exemeple des applications ou des fichiers que l\u0026rsquo;on recupère souvent mais que l\u0026rsquo;on veut pas télécharger chaque fois depuis internet.\nPour uploader un fichier on peut utiliser la commande suivante :\ncurl -v -u admin:admin123 --upload-file jprofiler_linux_8_1_4.tar.gz http://\u0026lt;nexus_ip\u0026gt;/nexus/content/sites/\u0026lt;site_repo\u0026gt;/jprofiler_linux_8_1_4.tar.gz", 
    "breadcrumb": " > misc > upload-file-to-nexus-repository.html"
},
{
    
    "uri": "/tags/url.html",
    "title": "Url",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > url.html"
},
{
    
    "uri": "/tags/usb.html",
    "title": "Usb",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > usb.html"
},
{
    
    "uri": "/linux/shell/using-mpc.html",
    "title": "Using MPC",
    "tags": ["linux", "shell", "mpc", "music"],
    "description": "",
    "content": " Je cherchais un lecteur de zik hyper léger avec un minimum de fonctionnalités :\n Shuffle Lecture en boucle Intégré à Gnome-shell Commandable du clavier  Résultat de la recherche, Music Player Daemon. Bon ya de la config à faire pour l’afficher dans gnome et pour le clavier mais au final ça fonctionne super. Il existe un panel de logicels pour le commander mais dans un soucis de légèreté, MPC est ce qu\u0026rsquo;il se fait de mieux.\nVoici quelques commandes pratique avec MPC pour utiliser Music Player Daemon à son plein potentiel :\nAjouter toutes les chansons dans la liste courante mpc clear \u0026amp;\u0026amp; mpc update \u0026amp;\u0026amp; mpc listall | mpc add  clear vide la playlist courante update re-scanne le répertoire de zik pour mettre à jour les fichiers dans la base listall liste tous les fichiers dans la base add ajoute chaque fichier listé dans la playlist  Enregistrer la playlist courante mpc save pl-soft.m3u Lister les playlists mpc lsplaylists Charger une playlist mpc load pl-soft.m3u Interface ASCII Il existe aussi un interface ASCII qui donne un peu de visualisation à ce qu’il se passe dans MPC mais le nom de l’outil est pas évident à retenir :\nncmpcpp Streaming Il est possible de streamer des fichiers ou podcast :\nmpc add http://traffic.libsyn.com/lescastcodeurs/LesCastCodeurs-Episode-182.mp3", 
    "breadcrumb": " > linux > shell > using-mpc.html"
},
{
    
    "uri": "/serveurs/oracle/utiliser-datapump-en-ligne-de-commande.html",
    "title": "Utiliser DataPump en ligne de commande",
    "tags": ["server", "oracle", "database", "datapump", "dump", "backup"],
    "description": "",
    "content": " Depuis la 10g, il y a deux méthodes pour exporter/importer un schéma :\n Import / Export normaux (exp) DataPump (\u0026gt;=10g uniquement) (expdp) La méthode ~DataPump est plus rapide et plus souple. Préférez donc celle là !  DataPump c:\\\u0026gt; expdp system/manager directory=DATA_PUMP_DIR dumpfile=EDGE_SP3_FIX016_AXA.dmp schemas=edgec:\\\u0026gt; impdp system/manager directory=DATA_PUMP_DIR dumpfile=EDGE_SP3_FIX016_AXA.dmp schemas=edge L\u0026rsquo;intérêt du ~DataPump est qu\u0026rsquo;il crée les schémas s\u0026rsquo;ils n\u0026rsquo;existent pas, qu\u0026rsquo;il permet de re-mapper les schémas et les tablespaces très facilement et qu\u0026rsquo;il va 3x plus vite que l\u0026rsquo;export normal.\nLocaliser le DIRECTORY L\u0026rsquo;une des particularité du Datapump est qu\u0026rsquo;à l\u0026rsquo;inverse de l\u0026rsquo;export standard, les fichiers sont lut et écrit, non pas dans le répertoire courant mais dans un DIRECTORY Oracle, genre de lien base de données vers un répertoire physique. Il ne faut donc pas mettre le fichier n\u0026rsquo;importe où !\nPar défaut, un DIRECTORY est toujours présent dans Oracle, le directory ~DATA_PUMP_DIR. Il se trouve généralement dans $ORACLE_HOME/admin/INSTANCE/dpdump. Mais cela diffère selon les installation. Pour le localiser à coup sur, sous sql*plus :\nSQL\u0026gt; select * from dba_directories; OWNER DIRECTORY_NAME DIRECTORY_PATH ------- ---------------- -------------------------------------------- SYS DATA_PUMP_DIR D:\\oracle\\product\\10.2.0\\admin\\campj\\dpdump\\  Il est aussi possible d\u0026rsquo;en créer un avec la commande :\nSQL\u0026gt; create directory TEMP_DATAPUMP_DIR as \u0026#39;c:\\\\temp\\\\\u0026#39;; Attention ! Oracle ne vérifie l\u0026rsquo;existence ni les permissions du répertoire ! S\u0026rsquo;il y a un soucis de ce genre c\u0026rsquo;est au moment du dump qu\u0026rsquo;une erreur sera levé !\nRemarque 1 le paramètre SQLFILE de la commande impdp permet d\u0026rsquo;obtenir un fichier SQL contenant les requêtes de l\u0026rsquo;import. Ca permet entre autre d\u0026rsquo;avoir la liste des tablespaces de l\u0026rsquo;export.\nRemarque 2 Il est possible de re-mapper les tablespaces ou les users pour, par exemple, déverser un user edge_test dans un user edge_prod. Les paramètres ~REMAP_SCHEMA et ~REMAP_TABLESPACE s\u0026rsquo;utilisent de la façon suivante :\nc:\\\u0026gt; impdp system/manager directory=DATA_PUMP_DIR dumpfile=EDGE_SP3_FIX016_AXA.dmp remap_schema=edge:cameleon remap_tablespace=users:tbs_cameleon Page d\u0026rsquo;aide Pour obtenir la liste des options :\nexpdp help=y Copyright (c) 2003, 2005, Oracle. All rights reserved. The Data Pump export utility provides a mechanism for transferring data objects between Oracle databases. The utility is invoked with the following command: Example: expdp scott/tiger DIRECTORY=dmpdir DUMPFILE=scott.dmp You can control how Export runs by entering the 'expdp' command followed by various parameters. To specify parameters, you use keywords: Format: expdp KEYWORD=value or KEYWORD=(value1,value2,...,valueN) Example: expdp scott/tiger DUMPFILE=scott.dmp DIRECTORY=dmpdir SCHEMAS=scott or TABLES=(T1:P1,T1:P2), if T1 is partitioned table USERID must be the first parameter on the command line. Keyword Description (Default) ------------------------------------------------------------------------------ ATTACH Attach to existing job, e.g. ATTACH [=job name]. COMPRESSION Reduce size of dumpfile contents where valid keyword values are: (METADATA_ONLY) and NONE. CONTENT Specifies data to unload where the valid keywords are: (ALL), DATA_ONLY, and METADATA_ONLY. DIRECTORY Directory object to be used for dumpfiles and logfiles. DUMPFILE List of destination dump files (expdat.dmp), e.g. DUMPFILE=scott1.dmp, scott2.dmp, dmpdir:scott3.dmp. ENCRYPTION_PASSWORD Password key for creating encrypted column data. ESTIMATE Calculate job estimates where the valid keywords are: (BLOCKS) and STATISTICS. ESTIMATE_ONLY Calculate job estimates without performing the export. EXCLUDE Exclude specific object types, e.g. EXCLUDE=TABLE:EMP. FILESIZE Specify the size of each dumpfile in units of bytes. FLASHBACK_SCN SCN used to set session snapshot back to. FLASHBACK_TIME Time used to get the SCN closest to the specified time. FULL Export entire database (N). HELP Display Help messages (N). INCLUDE Include specific object types, e.g. INCLUDE=TABLE_DATA. JOB_NAME Name of export job to create. LOGFILE Log file name (export.log). NETWORK_LINK Name of remote database link to the source system. NOLOGFILE Do not write logfile (N). PARALLEL Change the number of active workers for current job. PARFILE Specify parameter file. QUERY Predicate clause used to export a subset of a table. SAMPLE Percentage of data to be exported; SCHEMAS List of schemas to export (login schema). STATUS Frequency (secs) job status is to be monitored where the default (0) will show new status when available. TABLES Identifies a list of tables to export - one schema only. TABLESPACES Identifies a list of tablespaces to export. TRANSPORT_FULL_CHECK Verify storage segments of all tables (N). TRANSPORT_TABLESPACES List of tablespaces from which metadata will be unloaded. VERSION Version of objects to export where valid keywords are: (COMPATIBLE), LATEST, or any valid database version. The following commands are valid while in interactive mode. Note: abbreviations are allowed Command Description ------------------------------------------------------------------------------ ADD_FILE Add dumpfile to dumpfile set. CONTINUE_CLIENT Return to logging mode. Job will be re-started if idle. EXIT_CLIENT Quit client session and leave job running. FILESIZE Default filesize (bytes) for subsequent ADD_FILE commands. HELP Summarize interactive commands. KILL_JOB Detach and delete job. PARALLEL Change the number of active workers for current job. PARALLEL=\u0026lt;number of workers\u0026gt;. START_JOB Start/resume current job. STATUS Frequency (secs) job status is to be monitored where the default (0) will show new status when available. STATUS[=interval] STOP_JOB Orderly shutdown of job execution and exits the client. STOP_JOB=IMMEDIATE performs an immediate shutdown of the Data Pump job.  ", 
    "breadcrumb": " > serveurs > oracle > utiliser-datapump-en-ligne-de-commande.html"
},
{
    
    "uri": "/development/javascript/utiliser-firebug-lite-avec-ie.html",
    "title": "Utiliser Firebug (Lite) avec IE",
    "tags": ["development", "javascript", "ie", "firebug"],
    "description": "",
    "content": "Autant avec Firefox le débogage de CSS ne fait pas vraiment peur, autant dés qu\u0026rsquo;on parle de problèmes IE et d\u0026rsquo;autant plus de problème IE6 ou 7, là c\u0026rsquo;est un peu plus flippant.\nAlors il va exister exister une solution : https://getfirebug.com/firebuglite\nC\u0026rsquo;est pas trop compliqué à utiliser, il suffit d\u0026rsquo;inclure le JS présent dans l\u0026rsquo;archive https://getfirebug.com/releases/lite/latest/firebug-lite.tar.tgz à l\u0026rsquo;intérieur de vos composants puis de créer un marque page avec le code suivant à l\u0026rsquo;intérieur :\njavascript:(function(F,i,r,e,b,u,g,L,I,T,E){if(F.getElementById(b))return;E=F[i+\u0026#39;NS\u0026#39;]\u0026amp;\u0026amp;F.documentElement.namespaceURI;E=E?F[i+\u0026#39;NS\u0026#39;](E,\u0026#39;script\u0026#39;):F[i](\u0026#39;script\u0026#39;);E[r](\u0026#39;id\u0026#39;,b);E[r](\u0026#39;src\u0026#39;,I+g+T);E[r](b,u);(F[e](\u0026#39;head\u0026#39;)[0]||F[e](\u0026#39;body\u0026#39;)[0]).appendChild(E);E=new%20Image;E[r](\u0026#39;src\u0026#39;,I+L);})(document,\u0026#39;createElement\u0026#39;,\u0026#39;setAttribute\u0026#39;,\u0026#39;getElementsByTagName\u0026#39;,\u0026#39;FirebugLite\u0026#39;,\u0026#39;4\u0026#39;,\u0026#39;firebug-lite-debug.js\u0026#39;,\u0026#39;releases/lite/debug/skin/xp/sprite.png\u0026#39;,\u0026#39;https://getfirebug.com/\u0026#39;,\u0026#39;#startOpened\u0026#39;);  Après, sous IE vous chargez le composant, puis quand vous voulez ouvrir Firebug, vous clickez sur le marque page. Et hop firefox apparaît.\nBon, c\u0026rsquo;est une version \u0026ldquo;Lite\u0026rdquo; donc faut pas s\u0026rsquo;attendre à des trucs magiques mais bon \u0026hellip; c\u0026rsquo;est quand même pratique.\n", 
    "breadcrumb": " > development > javascript > utiliser-firebug-lite-avec-ie.html"
},
{
    
    "uri": "/development/javascript/utiliser-firebug-sans-planter-ie7-8.html",
    "title": "Utiliser Firebug sans planter IE7&amp;8",
    "tags": ["development", "javascript", "ie", "firebug"],
    "description": "",
    "content": "Firebug fournit justement un bout de code pour éviter de planter IE :\nif (!window.console || !console.log || !console.debug) { var names = [\u0026#34;log\u0026#34;, \u0026#34;debug\u0026#34;, \u0026#34;info\u0026#34;, \u0026#34;warn\u0026#34;, \u0026#34;error\u0026#34;, \u0026#34;assert\u0026#34;, \u0026#34;dir\u0026#34;, \u0026#34;dirxml\u0026#34;, \u0026#34;group\u0026#34;, \u0026#34;groupEnd\u0026#34;, \u0026#34;time\u0026#34;, \u0026#34;timeEnd\u0026#34;, \u0026#34;count\u0026#34;, \u0026#34;trace\u0026#34;, \u0026#34;profile\u0026#34;, \u0026#34;profileEnd\u0026#34;]; if (!window.console) window.console = {}; for (var i = 0; i \u0026lt; names.length; ++i) { if (!window.console[names[i]]) window.console[names[i]] = function() {}; } }   http://code.google.com/p/fbug/source/browse/branches/firebug1.5/lite/firebugx.js  ", 
    "breadcrumb": " > development > javascript > utiliser-firebug-sans-planter-ie7-8.html"
},
{
    
    "uri": "/tags/uuid.html",
    "title": "Uuid",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > uuid.html"
},
{
    
    "uri": "/outils/vmware/vmtools-en-ligne-de-commande.html",
    "title": "VMTools en ligne de commande",
    "tags": ["outils", "vmware"],
    "description": "",
    "content": "Pour lister les disques, en tant que root :\nvmware-toolbox-cmd disk list Pour faire un shrink sur un disque :\nvmware-toolbox-cmd disk shrink /", 
    "breadcrumb": " > outils > vmware > vmtools-en-ligne-de-commande.html"
},
{
    
    "uri": "/tags/visitor.html",
    "title": "Visitor",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > visitor.html"
},
{
    
    "uri": "/development/java/visitor-vs-instanceof.html",
    "title": "Visitor vs. InstanceOf",
    "tags": ["development", "java", "code", "visitor"],
    "description": "",
    "content": "public abstract class PartialKPIResolution { public final String aggregation; public PartialKPIResolution(String aggregation) { this.aggregation = aggregation; } public static PartialFlatKPIResolution flat(String aggregation, ImmutableMap\u0026lt;RealmAttribute, PartialFlatRealmKPIResolution\u0026gt; byRealm) { return new PartialFlatKPIResolution(aggregation, byRealm); } public static PartialSyntheticKPIResolution synthetic(Formula syntheticFormula, ApplicationPhase applicationPhase, String aggregation, ImmutableMap\u0026lt;String, String\u0026gt; columnAggregations, ImmutableMap\u0026lt;String, PartialFlatRealmKPIResolution\u0026gt; columns) { return new PartialSyntheticKPIResolution(syntheticFormula, applicationPhase, aggregation, columnAggregations, columns); } public abstract \u0026lt;R\u0026gt; R visit(Visitor\u0026lt;R\u0026gt; visitor); public interface Visitor\u0026lt;R\u0026gt; { R visitFlat(PartialFlatKPIResolution resolution); R visitSynthetic(PartialSyntheticKPIResolution resolution); } }public class PartialFlatKPIResolution extends PartialKPIResolution { public final ImmutableMap\u0026lt;RealmAttribute, PartialFlatRealmKPIResolution\u0026gt; byRealm; public PartialFlatKPIResolution(String aggregation, ImmutableMap\u0026lt;RealmAttribute, PartialFlatRealmKPIResolution\u0026gt; byRealm) { super(aggregation); this.byRealm = byRealm; } @Override public \u0026lt;R\u0026gt; R visit(Visitor\u0026lt;R\u0026gt; visitor) { return visitor.visitFlat(this); } }public final class PartialSyntheticKPIResolution extends PartialKPIResolution { public final Formula syntheticFormula; public final ApplicationPhase applicationPhase; public final ImmutableMap\u0026lt;String, String\u0026gt; columnAggregations; public final ImmutableMap\u0026lt;String, PartialFlatRealmKPIResolution\u0026gt; columns; public PartialSyntheticKPIResolution(Formula syntheticFormula, ApplicationPhase applicationPhase, String aggregation, ImmutableMap\u0026lt;String, String\u0026gt; columnAggregations, ImmutableMap\u0026lt;String, PartialFlatRealmKPIResolution\u0026gt; columns) { super(aggregation); this.syntheticFormula = syntheticFormula; this.applicationPhase = applicationPhase; this.columnAggregations = columnAggregations; this.columns = columns; } @Override public \u0026lt;R\u0026gt; R visit(Visitor\u0026lt;R\u0026gt; visitor) { return visitor.visitSynthetic(this); } } A la place de faire\nif (partialKPIResolution instanceof PartialFlatRealmKPIResolution) { PartialFlatKPIResolution resolution = (PartialFlatRealmKPIResolution)partialKPIResolution; return resolution.byRealm.get(realmAttribute); } else { LOGGER.warn(\u0026#34;Currently unsupported operation: TODO\u0026#34;); throw new UnsupportedOperationException(\u0026#34;synthetic in realm child!\u0026#34;); } on peut faire :\nPartialFlatRealmKPIResolution partialFlatRealmKPIResolution = partialKPIResolution .visit(new Visitor\u0026lt;PartialFlatRealmKPIResolution\u0026gt;() { @Override public PartialFlatRealmKPIResolution visitFlat(PartialFlatKPIResolution resolution) { return resolution.byRealm.get(realmAttribute); } @Override public PartialFlatRealmKPIResolution visitSynthetic( PartialSyntheticKPIResolution resolution) { LOGGER.warn(\u0026#34;Currently unsupported operation: TODO\u0026#34;); throw new UnsupportedOperationException(\u0026#34;synthetic in realm child!\u0026#34;); } });PartialFlatRealmKPIResolution partialFlatRealmKPIResolution = partialKPIResolution .visit(new Visitor\u0026lt;PartialFlatRealmKPIResolution\u0026gt;() { @Override public PartialFlatRealmKPIResolution visitFlat(PartialFlatKPIResolution resolution) { return resolution.byRealm.get(realmAttribute); } @Override public PartialFlatRealmKPIResolution visitSynthetic( PartialSyntheticKPIResolution resolution) { LOGGER.warn(\u0026#34;Currently unsupported operation: TODO\u0026#34;); throw new UnsupportedOperationException(\u0026#34;synthetic in realm child!\u0026#34;); } });", 
    "breadcrumb": " > development > java > visitor-vs-instanceof.html"
},
{
    
    "uri": "/outils/latex/visualisation-instantane-de-modifications-latex.html",
    "title": "Visualisation instantané de modifications Latex",
    "tags": ["outils", "latex", "pdf", "ghostview", "tmux"],
    "description": "",
    "content": "Jusque là j\u0026rsquo;utilisais Gummy comme éditeur LaTeX. Il me permet de voir en direct les modifications que je fais sur le(s) fichier(s). Mais ça rame un peu et surtout quand j\u0026rsquo;utilise XelateX ça fonctionne pas bien, il me faut faire plusieurs modification pour qu\u0026rsquo;il se mette à jour. En plus sur des PDF avec un peu plus d\u0026rsquo;une page c\u0026rsquo;est pas pratique, son éditeur n\u0026rsquo;est pas configurable, \u0026hellip; bref.\nVoilà une solution qui fonctionne bien. Dans un onglet tmux j\u0026rsquo;ouvre 3 panels :\n Commandes Git latexmk --pvc qui compile et génère le PDF en boucle tant qu\u0026rsquo;il n\u0026rsquo;y a pas d\u0026rsquo;erreur gv --watch build/mon-pdf.pdf qui affiche et rafraichis mon PDf  ", 
    "breadcrumb": " > outils > latex > visualisation-instantane-de-modifications-latex.html"
},
{
    
    "uri": "/tags/vmplayer.html",
    "title": "Vmplayer",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > vmplayer.html"
},
{
    
    "uri": "/tags/vmware.html",
    "title": "Vmware",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > vmware.html"
},
{
    
    "uri": "/tags/vpn.html",
    "title": "Vpn",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > vpn.html"
},
{
    
    "uri": "/linux/network/wget-avec-un-proxy.html",
    "title": "WGET avec un proxy",
    "tags": ["linux", "network", "proxy", "wget"],
    "description": "",
    "content": " WGET est un programme en ligne de commande qui permet de télécharger des fichiers depuis le web. Pour une utilisation en entreprise, il se peut qu’un proxy filtre les accès au web.\n## Utiliser WGET avec un proxy simple\nCréer un fichier .wgetrc (n\u0026rsquo;oubliez pas le point devant le fichier) a la racine de votre répertoire personnel avec le contenu suivant :\nhttp_proxy = http://votre_proxy:port_proxy/ use_proxy = on wait = 15  Vérifiez que ca fonctionne en rapatriant un fichier test :\nwget http://www.debian.org/Pics/debian.png Si vous avez un message d\u0026rsquo;erreur comme ci-dessous, passez au paragraphe suivant :\nrequête Proxy transmise, en attente de la réponse...407 Proxy Authentication Required ERREUR 407: Proxy Authentication Required.  Utiliser WGET avec un proxy et authentification Créer un fichier .wgetrc (n\u0026rsquo;oubliez pas le point devant le fichier) a la racine de votre répertoire personnel avec le contenu suivant :\nhttp_proxy = http://votre_proxy:port_proxy/ proxy_user = votre_user_proxy proxy_password = votre_mot_de_passe use_proxy = on wait = 15  Vérifiez que ca fonctionne en rapatriant un fichier test :\nwget http://www.debian.org/Pics/debian.png", 
    "breadcrumb": " > linux > network > wget-avec-un-proxy.html"
},
{
    
    "uri": "/development/dotnet/web-service-erreur-cs0029-cs0030.html",
    "title": "Web service erreur CS0029, CS0030",
    "tags": ["development", "dotnet", ".net", "error", "ws"],
    "description": "",
    "content": "Lors de la création de client ou de serveur WSCart en .Net, on rencontre l\u0026rsquo;erreur CS0029, CS0030. C\u0026rsquo;est du à un problème d\u0026rsquo;interprétation du WSDL par .Net qui gère mal est attributs \u0026ldquo;unbounded\u0026rdquo; sur les type complex. Pour corriger, il faut, dans le fichier .cs généré avec le wsdl.exe de Microsoft, rechercher la chaine [][] et remplacer par []. On va normalement trouver 2 occurrences.\n http://www.developpez.net/forums/d1101500/dotnet/langages/csharp/web-service-erreur-cs0029-cs0030/  ", 
    "breadcrumb": " > development > dotnet > web-service-erreur-cs0029-cs0030.html"
},
{
    
    "uri": "/linux/shell/web-server-en-une-ligne-de-commande.html",
    "title": "Web-server en une ligne de commande",
    "tags": ["linux", "shell", "server"],
    "description": "",
    "content": " Comment lancer un serveur web en une ligne de commande bash :\nwhile true; do { echo -e \u0026#39;HTTP/1.1 200 OK\\r\\n\u0026#39;; cat index.html; } | nc -l 8080; done L\u0026rsquo;intéret c\u0026rsquo;est que l\u0026rsquo;on peut voir facilement ce qui est envoyé comme entête HTTP, c\u0026rsquo;est simple et rapide !\nliens  https://razvantudorica.com/08/web-server-in-one-line--bash/  ", 
    "breadcrumb": " > linux > shell > web-server-en-une-ligne-de-commande.html"
},
{
    
    "uri": "/tags/webdav.html",
    "title": "Webdav",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > webdav.html"
},
{
    
    "uri": "/outils/docker/weekly-cleaner.html",
    "title": "Weekly cleaner",
    "tags": ["development", "docker", "cleanup"],
    "description": "",
    "content": "A placer dans /etc/cron.weekly/clean-docker + chmod +x ...\ndocker rm -v $(docker ps -a -q) docker rmi $(docker images | grep \u0026#39;^\u0026lt;none\u0026gt;\u0026#39; | awk \u0026#39;{print $3}\u0026#39;) docker rmi $(docker images | grep \u0026#39;months ago\u0026#39; | awk \u0026#39;{print $3}\u0026#39;) Ca ve supprimer les images vieilles de plusieurs mois et celle non tagés ainsi que les conteneur associés.\n", 
    "breadcrumb": " > outils > docker > weekly-cleaner.html"
},
{
    
    "uri": "/tags/wget.html",
    "title": "Wget",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > wget.html"
},
{
    
    "uri": "/tags/windows.html",
    "title": "Windows",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > windows.html"
},
{
    
    "uri": "/tags/ws.html",
    "title": "Ws",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > ws.html"
},
{
    
    "uri": "/tags/x11.html",
    "title": "X11",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > x11.html"
},
{
    
    "uri": "/tags/xargs.html",
    "title": "Xargs",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > xargs.html"
},
{
    
    "uri": "/tags/xml.html",
    "title": "Xml",
    "tags": [],
    "description": "",
    "content": "", 
    "breadcrumb": " > tags > xml.html"
},
{
    
    "uri": "/development/java/valeur-memoire-java-par-defaut.html",
    "title": "Xmx &amp; Xms par defaut",
    "tags": ["development", "java", "configuration", "memory"],
    "description": "",
    "content": " Linux java -XX:+PrintFlagsFinal -version 2\u0026gt;\u0026amp;1 | grep -i -E \u0026#39;heapsize|permsize|version\u0026#39; Windows java -XX:+PrintFlagsFinal -version 2\u0026gt;\u0026amp;1 | findstr /I \u0026#34;heapsize permsize version\u0026#34; Selon le système, les valeurs par défaut peuvent être différente avec les options -client et -server. Pour connaitre les valeurs par defaut il faut dans ce cas ajouter l\u0026rsquo;option pour avoir les valeurs correspondante.\n", 
    "breadcrumb": " > development > java > valeur-memoire-java-par-defaut.html"
},
{
    
    "uri": "/serveurs/teradata/error-1178--sqlstate-hy000--the-teradata-database-did-not-provide-the-expected-response.html",
    "title": "[Error 1178] [SQLState HY000] The Teradata Database did not provide the expected response",
    "tags": ["server", "teradata", "Error"],
    "description": "",
    "content": " Symptômes [Teradata JDBC Driver] [TeraJDBC 15.00.00.15] [Error 1178] [SQLState HY000] The Teradata Database did not provide the expected response - unable to read 8224 byte(s) from the database. Only 209814 byte(s) were received from the database and 202217 byte(s) have already been read.  Le problème se pose à l\u0026rsquo;exécution d\u0026rsquo;une requête bête :\nselect * from table_truc where the_date between ? and ? sachant que ça fonctionne très bien sur la plus-part des table mais pas sur l\u0026rsquo;une d\u0026rsquo;elles ??\nSolution En fait, la table contenait un champ de type Structured UDT formé comme suit :\nCREATE TYPE SYSUDTLIB.AddAttribute_Type AS CHAR(24) CHARACTER SET LATIN ARRAY [10] DEFAULT NULL ; C\u0026rsquo;est ça qui causait le souci. En supprimant ce champ de la close SELECT, la requête re-fonctionne très bien.\nReste à savoir comment on sélectionne ce type de champ.\nEdit 2015-04-08: J\u0026rsquo;avais oublié de mettre à jour, en fait un bug dans les driver JDBC interdit de faire un select de champ tableau si ce dernier est pas à la fin du select. Donc lors d\u0026rsquo;un select il faut trier les champs UDT et les mettre à la fin du select. Cela fonctionne aussi avec plusieurs champs UDT.\n", 
    "breadcrumb": " > serveurs > teradata > error-1178--sqlstate-hy000--the-teradata-database-did-not-provide-the-expected-response.html"
},
{
    
    "uri": "/linux/administration/apt-get--encountered-a-section-with-no-package--header.html",
    "title": "apt-get Encountered a section with no Package header",
    "tags": ["linux", "sysadmin", "apt", "debian"],
    "description": "",
    "content": "Un beau matin alors que je veux mettre a jour ma debian, je me retrouve avec le message d\u0026rsquo;erreur suivant :\nW: Impossible de récupérer copy:/var/lib/apt/lists/partial/... Encountered a section with no Package: header  C\u0026rsquo;est un problème bien foireux dont j\u0026rsquo;ai eu du mal à me sortir ! En fait ya eu un bug un moment donné et du coup sur les machines qui n\u0026rsquo;ont pas été mise à jour régulièrement on trouve ce message. En fait sur internet on trouve que des solutions bidons ! La vrai solution c\u0026rsquo;est qu\u0026rsquo;il faut installer bzip2 ! Pour faire ça :\ncd /var/lib/apt/lists rm -Rf * apt-get clean apt-get update Là ça vous met toujours l\u0026rsquo;erreur !\napt-get install bzip2 Ya de bonne chance que ça marche pas, il va vous sortir un truc genre :\nLecture des listes de paquets... Erreur ! E: Encountered a section with no Package: header E: Problem with MergeList /var/lib/apt/lists/ftp.fr.debian.org_debian_dists_testing_main_i18n_Translation-en E: Les listes de paquets ou le fichier « status » ne peuvent être analysés ou lus.  Du coup :\nrm /var/lib/apt/lists/ftp.fr.debian.org_debian_dists_testing_main_i18n_Translation-en apt-get install bzip2 Et là ça devrait fonctionner\nrm -Rf /var/lib/apt/lists/* apt-get clean apt-get update Et ça fonctionne ! Reste plus qu\u0026rsquo;à faire le dist-upgrade.\n", 
    "breadcrumb": " > linux > administration > apt-get--encountered-a-section-with-no-package--header.html"
},
{
    
    "uri": "/outils/git/gitflow-breakdown.html",
    "title": "gitflow-breakdown",
    "tags": ["outils", "git"],
    "description": "",
    "content": " Initialize    gitflow git     git flow init git init    git commit --allow-empty -m \u0026quot;Initial commit\u0026quot;    git checkout -b develop master    Connect to the remote repository    gitflow git     N/A git remote add origin git@github.com:MYACCOUNT/MYREPO    Features Create a feature branch    gitflow git     git flow feature start MYFEATURE git checkout -b feature/MYFEATURE develop    Share a feature branch    gitflow git     git flow feature publish MYFEATURE git checkout feature/MYFEATURE    git push origin feature/MYFEATURE    Get latest for a feature branch    gitflow git     git flow feature pull origin MYFEATURE git checkout feature/MYFEATURE    git pull --rebase origin feature/MYFEATURE    Finalize a feature branch    gitflow git     git flow feature finish MYFEATURE git checkout develop    git merge --no-ff feature/MYFEATURE    git branch -d feature/MYFEATURE    Push the merged feature branch    gitflow git     N/A git push origin develop    git push origin :feature/MYFEATURE (if pushed)    Releases Create a release branch    gitflow git     git flow release start 1.2.0 git checkout -b release/1.2.0 develop    Share a release branch    gitflow git     git flow release publish 1.2.0 git checkout release/1.2.0    git push origin release/1.2.0    Get latest for a release branch    gitflow git     N/A git checkout release/1.2.0    git pull --rebase origin release/1.2.0    Finalize a release branch    gitflow git     git flow release finish 1.2.0 git checkout master    git merge --no-ff release/1.2.0    git tag -a 1.2.0    git checkout develop    git merge --no-ff release/1.2.0    git branch -d release/1.2.0    Push the merged feature branch    gitflow git     N/A git push origin master    git push origin develop    git push origin --tags    git push origin :release/1.2.0 (if pushed)    Hotfixes Create a hotfix branch    gitflow git     git flow hotfix start 1.2.1 [commit] git checkout -b hotfix/1.2.1 [commit]    Finalize a hotfix branch    gitflow git     git flow hotfix finish 1.2.1 git checkout master    git merge --no-ff hotfix/1.2.1    git tag -a 1.2.1    git checkout develop    git merge --no-ff hotfix/1.2.1    git branch -d hotfix/1.2.1    Push the merged hotfix branch    gitflow git     N/A git push origin master    git push origin develop    git push origin --tags    git push origin :hotfix/1.2.1 (if pushed)    References  http://nvie.com/posts/a-successful-git-branching-model/ https://help.github.com/articles/using-pull-requests#shared-repository-model  ", 
    "breadcrumb": " > outils > git > gitflow-breakdown.html"
},
{
    
    "uri": "/linux/gnomeshell/gnome-shell-segfault-at-84-ip-libcogl.so.12.1.1.html",
    "title": "gnome-shell segfault at 84 ip libcogl.so.12.1.1",
    "tags": ["linux", "gnome", "shell", "debian", "error", "segfault", "libcogl"],
    "description": "",
    "content": " Symptome GnomeShell ne se lance pas, un message \u0026ldquo;Oh no! Something has gone wrong\u0026rdquo; à la place. Et dans les logs :\nhp kernel: [34946.429046] gnome-shell[10202]: segfault at 84 ip 00007ffa2ae877b9 sp 00007fff2bc5c7d0 error 4 in libcogl.so.12.1.1[7ffa2ae3e000+97000]  Remède sudo apt-get install --reinstall libgdk-pixbuf2.0-0 libgdk-pixbuf2.0-common Liens  http://forums.debian.net/viewtopic.php?f=5\u0026amp;t=78716\u0026amp;p=523535\u0026amp;hilit=libcogl#p523535  ", 
    "breadcrumb": " > linux > gnomeshell > gnome-shell-segfault-at-84-ip-libcogl.so.12.1.1.html"
},
{
    
    "uri": "/outils/git/non-fast-forward-updates-were-rejected.html",
    "title": "non fast forward updates were rejected",
    "tags": ["outils", "git", "eclipse"],
    "description": "",
    "content": "C’est qu’il y a un problème de synchro, s’il est réglé immédiatement c’est pas compliqué. En général c’est provoqué par le plugin EGit qui a du mal à gérer les modification de commit.\ngit pull origin develop git push", 
    "breadcrumb": " > outils > git > non-fast-forward-updates-were-rejected.html"
},
{
    
    "uri": "/linux/network/ssh-control-de-tunnel-via-socket.html",
    "title": "ssh, control de tunnel via socket",
    "tags": ["linux", "network", "ssh"],
    "description": "",
    "content": "Il est possible de commander une session SSH via un socket.\nssh -M -S my-ctrl-socket -fnNT -L 50000:localhost:3306 jm@sampledomain.com ssh -S my-ctrl-socket -O check jm@sampledomain.com Master running (pid=3517) ssh -S my-ctrl-socket -O exit jm@sampledomain.com Exit request sent. ", 
    "breadcrumb": " > linux > network > ssh-control-de-tunnel-via-socket.html"
},
{
    
    "uri": "/",
    "title": "wiki.ght1pc9kc.fr",
    "tags": [],
    "description": "",
    "content": "  Cuisine Quelques recettes de cuisine\n  Divers Un peu tout et n’importe quoi\n  Développement Tout pour le dev, Java, javascript, ...\n  Linux Le seul le vrai l’unique système\n  Outils Les outils pour le dev, Git, Ansible, Docker, ...\n  Serveur Les serveurs dans l’ensemble, HTTP, Base de données, ...\n  ", 
    "breadcrumb": ""
}]